---
title: "Home Work Assignment - 04"
author: "Critical Thinking Group 5"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

\newpage

# Overview 
The data set contains approximately 8161 records. Each record represents a customer profile at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A "1" means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero
if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.
\

\
We will be exploring, analyzing, and modeling the training data to build many binary logistic regression models (to predict if a person will crash the car) and also some linear regression models (to predict the amount of money it will take to fix the car after crashing). Out of the many models for each task, we will go ahead and shortlist one model that works the best. We will then use these models (one for each task) on the test / evaluation data.

\

To attain our objective, we will be following the below best practice steps and guidelines: \

1 -Data Exploration \
2 -Data Preparation \
3 -Build Models \
4 -Select Models \

\
\

As a strategy, we will split the train dataset into 2 parts - TRAIN and VALID. In the VALID dataset, we will hold out some values to validate how well the model is trained using the TRAIN dataset.
\
\
We will do this once all the data transformations are complete and we are ready to build the models.
\
\

While building and selecting models, We will deal with the problem in 2 parts: \

- Part 1 - Here we build and select Binary Logistic Regression models using the training data set. 
\

- Part 2 - Here we build and select Linear Regression models using only the "Crashed" data from the training data set.

\
\


#1 Data Exploration Analysis

In section we will explore and gain some insights into the dataset by pursuing the below high level steps and inquiries: \
-Variable identification \
-Variable Relationships \
-Data summary analysis \
-Outliers and Missing Values Identification


##1.1	Variable identification

First let's display and examine the data dictionary or the data columns as shown in table 1

```{r, echo = FALSE, warning=FALSE, message=FALSE}

if (!require("ggplot2",character.only = TRUE)) (install.packages("ggplot2",dep=TRUE))
if (!require("MASS",character.only = TRUE)) (install.packages("MASS",dep=TRUE))
if (!require("knitr",character.only = TRUE)) (install.packages("knitr",dep=TRUE))
if (!require("xtable",character.only = TRUE)) (install.packages("xtable",dep=TRUE))
if (!require("dplyr",character.only = TRUE)) (install.packages("dplyr",dep=TRUE))
if (!require("psych",character.only = TRUE)) (install.packages("psych",dep=TRUE))
if (!require("stringr",character.only = TRUE)) (install.packages("stringr",dep=TRUE))
if (!require("car",character.only = TRUE)) (install.packages("car",dep=TRUE))
if (!require("faraway",character.only = TRUE)) (install.packages("faraway",dep=TRUE))
if (!require("dummy",character.only = TRUE)) (install.packages("dummy",dep=TRUE))
if (!require("reshape2",character.only = TRUE)) (install.packages("reshape2",dep=TRUE))
if (!require("popbio",character.only = TRUE)) (install.packages("popbio",dep=TRUE))


library(ggplot2)
library(MASS)
library(knitr)
library(xtable)
library(dplyr)
library(psych)
library(stringr)
library(car)
library(faraway)
library(dummy)
library(reshape2)
library(popbio)

insure_train_full <- read.csv("https://raw.githubusercontent.com/kishkp/data621-ctg5/master/HW4/insurance_training_data.csv")

insurevars <- read.csv("https://raw.githubusercontent.com/kishkp/data621-ctg5/master/HW4/insurevars.csv")
kable(insurevars, caption = "Variable Description")

```

We notice that there are 2 dependent variables - TARGET_FLAG and TARGET_AMT. Apart from these 2 dependent variables, we have 23 independent or predictor variables.\
\

```{r}
str(insure_train_full)

levels(insure_train_full$MSTATUS)
levels(insure_train_full$SEX)
levels(insure_train_full$EDUCATION)
levels(insure_train_full$JOB)
levels(insure_train_full$CAR_TYPE)
levels(insure_train_full$URBANICITY)
levels(insure_train_full$REVOKED)
```
\
\

From the output above we can make the following observations:\


- some numeric variables like INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM have been converted to Factor variables. This needs to be set right.

- Some of the variables like MSTATUS, SEX, EDUCATION, JOB, CAR_TYPE, URBANICITY have some of the values encoded with "z_". Not that this will impact the analysis, but it will look a bit odd. So we will be fixing this.

- EDUCATION has 2 "High School" values - one starting with "<" and another starting with "z_". It is assumed that both these values are to be converted to "HIGH School".

- JOB has a "" value. This needs to be replaced with NA.

- We will also create dummy variables for all the factors. 

- Please note that we will not be using INDEX variable as it serves as just an identifier for each row. And has no relationships to other variables.   

\
\

Making the above fixes to the data, we now have a "clean" dataset which can be explored further.
\

```{r}

#- some numeric variables like INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM have been converted to Factor variables. This needs to be set right.

insure_train_full$INCOME <- as.numeric(insure_train_full$INCOME)
insure_train_full$HOME_VAL <- as.numeric(insure_train_full$HOME_VAL)
insure_train_full$BLUEBOOK <- as.numeric(insure_train_full$BLUEBOOK)
insure_train_full$OLDCLAIM <- as.numeric(insure_train_full$OLDCLAIM)

#- Some of the variables like MSTATUS, SEX, EDUCATION, JOB, CAR_TYPE, URBANICITY have some of the values encoded with "z_". Not that this will impact the analysis, but it will look a bit odd. So we will be fixing this.

#- EDUCATION has 2 "High School" values - one starting with "<" and another starting with "z_". It is assumed that both these values are to be converted to "HIGH School".

#- JOB has a "" value. This needs to be replaced with NA.

insure_train_full$MSTATUS <- as.factor(str_replace_all(insure_train_full$MSTATUS, "z_", ""))
insure_train_full$SEX <- as.factor(str_replace_all(insure_train_full$SEX, "z_", ""))
insure_train_full$EDUCATION <- as.factor(str_replace_all(insure_train_full$EDUCATION, "z_", ""))
insure_train_full$EDUCATION <- as.factor(str_replace_all(insure_train_full$EDUCATION, "<", ""))
insure_train_full$CAR_TYPE <- as.factor(str_replace_all(insure_train_full$CAR_TYPE, "z_", ""))
insure_train_full$URBANICITY <- as.factor(str_replace_all(insure_train_full$URBANICITY, "z_", ""))

insure_train_full$JOB[insure_train_full$JOB==""] <- NA
insure_train_full$JOB <- as.factor(str_replace_all(insure_train_full$JOB, "z_", ""))

#- We will also create dummy variables for all the factors and drop the original variables. 

dummy_vars<-as.data.frame(sapply(dummy(insure_train_full), FUN = as.numeric))
dummy_vars <- dummy_vars-1

# EDU_ = factor(insure_train_full$EDUCATION)
# dummies = model.matrix(~EDU_)

insure_train_full <- cbind(select(insure_train_full, -PARENT1, -MSTATUS, -SEX, -EDUCATION, -JOB, -CAR_USE, -CAR_TYPE, -RED_CAR, -REVOKED, -URBANICITY), dummy_vars)

# insure_train_full <- cbind(insure_train_full, dummy_vars)

# - Please note that we will not be using INDEX variable as it serves as just an identifier for each row. And has no relationships to other variables.   

insure_train_full <- select(insure_train_full, -INDEX)

```



##1.2 Variable Relationships \

Since we have 2 models to build, we have 2 sets of assumptions to be checked:


- Logistic Regression for TARGET_FLAG: \newline
-- The dependent variable need not to be normally distributed \
-- Errors need to be independent but not normally distributed. \
-- We will be using GLM and GLM does not assume a linear relationship between dependent and independent variables. However, it assumes a linear relationship between link function and independent variables in logit model. \
-- Also does not use OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE) 

\
\


**NEED TO ADD SOME POINTS**

- Linear Regression for TARGET_AMT: \newline
-- The dependent variable is normally distributed \
-- Errors are independent and normally distributed. \


\
\



In next step below relationship between the target variable and dependent variables is shown in three charts.


##1.3 Data Summary Analysis


In this section, we will create summary data to better understand the relationship each of the variables have with our dependent variables using correlation, central tendency, and dispersion As shown in table 2.  

```{r, echo = FALSE, warning=FALSE, message=FALSE, results='hide'}

ds_stats <- psych::describe(insure_train_full, skew = TRUE, na.rm = TRUE)
#ds_stats
kable(ds_stats[1:7], caption= "Data Summary")
kable(ds_stats[8:13], caption= "Data Summary (Cont)")

fun1 <- function(a, y) cor(y, a , use = 'na.or.complete')
x<-insure_train_full[,]
Correlation_TARGET_FLAG <- sapply(x, FUN = fun1, y=insure_train_full$TARGET_FLAG) 

```


Now we will produce the correlation table between the independent variables and the dependent variables - TARGET_FLAG and TARGET_AMT  

\
\
First lets see the correlation for TARGET_FLAG:\

```{r, echo = FALSE, warning=FALSE, message=FALSE}
Correlation_TARGET_FLAG <- sort(Correlation_TARGET_FLAG, decreasing = TRUE)
kable(data.frame(Correlation_TARGET_FLAG), caption = "Correlation between TARGET_FLAG and predictor variables")

```
\

The above table suggests that none of the variables seem to have a very strong correlation with TARGET_FLAG. However, CAR_TYPE_Van, RED_CAR_no, JOB_Home.Maker, SEX_F, JOB_Clerical, CAR_TYPE_SUV, TRAVTIME, BLUEBOOK, CAR_TYPE_Pickup, CAR_TYPE_Sports.Car, JOB_Student, KIDSDRIV, JOB_Blue.Collar, HOMEKIDS, MSTATUS_No, EDUCATION_High.School, CAR_USE_Commercial, REVOKED_Yes, PARENT1_Yes, OLDCLAIM, CLM_FREQ, MVR_PTS and URBANICITY_Highly.Urban..Urban have a positive correlation. 
\
Similarly, URBANICITY_Highly.Rural..Rural, PARENT1_No, REVOKED_No, HOME_VAL, CAR_USE_Private, CAR_TYPE_Minivan, MSTATUS_Yes, JOB_Manager, AGE, CAR_AGE, TIF, EDUCATION_Masters, YOJ, EDUCATION_PhD, JOB_Lawyer, JOB_Doctor, EDUCATION_Bachelors, JOB_Professional, INCOME, SEX_M, RED_CAR_yes, CAR_TYPE_Panel.Truck have a negative correlation. 

\
\

Lets now see how values in some of the variable affects the correlation:

\

CAR_TYPE - If you drive Minivans and Panel Trucks you have lesser chance of being in a crash as against Pickups, Sports, SUVs and Vans. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation.

\
\

EDUCATION - If you have only a high school education then you are more likely to crash than if you have a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation.
\
\

JOB - If you are a Student, Homemaker, or in a Blue Collar or Clerical job, you are more likely to be in a crash against Doctor, Lawyer, Manager or professional.  Again binning this variable will strengthen the correlation.

\
\

We will carry out the above transformations in the Data Preparation phase.
\
\

Next lets look at the correlation for TARGET_AMT. 
\

Prior to this, we need to filter for only those records where there has been a crash. The amount incurred is relevant only when there is a crash. We then look at the correlations.
\

```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_crash <- insure_train_full[insure_train_full$TARGET_FLAG==1,]
insure_train_crash <- select(insure_train_crash, -TARGET_FLAG)


#select(insure_train_full, -PARENT1, -MSTATUS, -SEX, -EDUCATION, -JOB, -CAR_USE, -CAR_TYPE, -RED_CAR, -REVOKED, -URBANICITY)
```

\
\

We now use the TARGET_AMT to check the correlations.
\

```{r, echo = FALSE, warning=FALSE, message=FALSE}
Correlation_TARGET_AMT <- sapply(insure_train_crash, FUN = fun1, y=insure_train_crash$TARGET_AMT) 
Correlation_TARGET_AMT <- sort(Correlation_TARGET_AMT, decreasing = TRUE)
kable(data.frame(Correlation_TARGET_AMT), caption = "Correlation between TARGET_AMT and predictor variables")

```
\
\

The above table suggests that none of the variables seem to have a very strong correlation with TARGET_AMT. KIDSDRIV, HOMEKIDS, CLM_FREQ, URBANICITY_Highly.Urban..Urban, TRAVTIME, HOME_VAL, EDUCATION_Bachelors, JOB_Lawyer, EDUCATION_Masters, INCOME, PARENT1_Yes, JOB_Blue.Collar, RED_CAR_yes, AGE, EDUCATION_PhD, YOJ, MSTATUS_No, REVOKED_No, MVR_PTS, JOB_Professional, CAR_USE_Commercial, CAR_TYPE_Van, SEX_M, CAR_TYPE_Panel.Truck have a positive correlation. 
\
Similarly, SEX_F, CAR_USE_Private, CAR_TYPE_SUV, REVOKED_Yes, EDUCATION_High.School, MSTATUS_Yes, JOB_Student, JOB_Home.Maker, RED_CAR_no, JOB_Manager, PARENT1_No, CAR_TYPE_Pickup, CAR_TYPE_Sports.Car, CAR_AGE, JOB_Doctor, JOB_Clerical, OLDCLAIM, BLUEBOOK, TIF, CAR_TYPE_Minivan, URBANICITY_Highly.Rural..Rural have a negative correlation. 

\
\

Lets now see how values in some of the variable affects the correlation:

\

CAR_TYPE - If you drive Vans or Panel Trucks your cost of repair seems to increase as against Minivan, Pickup, Sports.Car, SUV. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation.

\
\

EDUCATION - If you have only a high school education then your cost of repair is less compared to a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation.
\
\

JOB - If you are a Lawyer, Professional or in a Blue Collar job, you spend more on repairs as compared to a Doctor, Manager, Home Maker, Student, or Clerical job.  Again binning this variable will strengthen the correlation.

\
\

We will carry out the above transformations in the Data Preparation phase.
\
\


##1.4	Missing Values and Outliers Identification


###1.4.1 Missing Values 

\
\
Based on the missing data from the below table, we can see that there are a few missing values for AGE, CAR_AGE, YOJ and JOB variables. 


```{r, echo = FALSE, warning=FALSE, message=FALSE}
missings<- sapply(insure_train_full,function(x) sum(is.na(x)))
kable(data.frame(missings), caption = "Missing Values")

```

\
\

We can try and impute values to AGE, YOJ, CAR_AGE. However, we will not be able to impute values for JOB since this is a categorical variable. Though there are a few methods to do this imputation, it may not be worth it.  
\
\
Lets see the impact if we have to exclude these missing records.
\
```{r}

sum(!complete.cases(insure_train_full))

# Percentage of records

sum(!complete.cases(insure_train_full))/nrow(insure_train_full) *100

```

\

We see from the above that excluding the missing rows will remove about 17.24% of the records. This would not seem to have too much of an impact. 
\
\
We will exclude the rows with the missing values when we do the data preparation / tranformations for the TARGET_FLAG dataset.

\
\

Similarly, based on the below analysis, we see that we are losing about 17.6% of the data for the TARGET_AMT dataset.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
missings<- sapply(insure_train_crash,function(x) sum(is.na(x)))
kable(data.frame(missings), caption = "Missing Values")
sum(!complete.cases(insure_train_crash))
# Percentage of records
sum(!complete.cases(insure_train_crash))/nrow(insure_train_crash) *100

```

\
\
We will exclude the rows with the missing values when we do the data preparation / tranformations for the TARGET_AMT dataset.

\
\
\



###1.4.2 Outliers identification 

In this section univariate analysis is being carried out and boxplots diagrams are being used to determine the outliers in variables and decide on whether to act on the outliers.\
\

We will do the outliers only on the numeric variables.\
\
Below are the plots:
\


```{r, echo = FALSE, warning=FALSE, message=FALSE}

#
mdata<- select(insure_train_full, KIDSDRIV, AGE, BLUEBOOK, CAR_AGE, CLM_FREQ, HOME_VAL, HOMEKIDS, INCOME, MVR_PTS, OLDCLAIM, TIF, TRAVTIME, YOJ)
mdata2 <- melt(mdata)
# Output the boxplot
p <- ggplot(data = mdata2, aes(x=variable, y=value)) + 
  geom_boxplot() + ggtitle("Outliers identification")
p + facet_wrap( ~ variable, scales="free", ncol=5)

```

From the "Outliers identification" plot above, we see that we have few outliers that we need to treat. \

We see that: KIDSDRIV, AGE, HOMEKIDS, MVR_PTS, OLDCLAIM, TIF, TRAVTIME, YOJ  need to be treated when we do the data preparation for modeling the TARGET_FLAG. 

\
\

We carry out the same exercise for TARGET_AMT as well:

\
\

We will do the outliers only on the numeric variables.\
\
Below are the plots:
\


```{r, echo = FALSE, warning=FALSE, message=FALSE}

#
mdata<- select(insure_train_crash, KIDSDRIV, AGE, BLUEBOOK, CAR_AGE, CLM_FREQ, HOME_VAL, HOMEKIDS, INCOME, MVR_PTS, OLDCLAIM, TIF, TRAVTIME, YOJ)
mdata2 <- melt(mdata)
# Output the boxplot
p <- ggplot(data = mdata2, aes(x=variable, y=value)) + 
  geom_boxplot() + ggtitle("Outliers identification")
p + facet_wrap( ~ variable, scales="free", ncol=5)

```

From the "Outliers identification" plot above, we see that we have few outliers that we need to treat. \

We see that: KIDSDRIV, AGE, MVR_PTS, TIF, TRAVTIME, YOJ  need to be treated when we do the data preparation for modeling the TARGET_AMT. 


\newpage

##1.5 Analysis the link function \

In this section, we will investigate how our initial data aligns with a typical logistic model plot. 

Recall the Logistic Regression is part of a larger class of algorithms known as Generalized Linear Model (glm).  The fundamental equation of generalized linear model is:

$g(E(y)) = a+ Bx_1+B_2x_2+ B_3x_3+...$   

where, g() is the link function, E(y) is the expectation of target variable and $B_0 + B_1x_1 + B_2x_2+B_3x_3$ is the linear predictor ( $B_0,B_1,B_2, B_3$ to be predicted). The role of link function is to 'link' the expectation of y to linear predictor.

In logistic regression, we are only concerned about the probability of outcome dependent variable ( success or failure). As described above, g() is the link function. This function is established using two things: Probability of Success (p) and Probability of Failure (1-p).  p should meet following criteria:
It must always be positive (since p >= 0)
It must always be less than equals to 1 (since p <= 1).

Now let's investigate how our initial data model aligns with the above criteria. In other words, we will plot regression model plots for each variable and compare it to a typical logistic model plot:


```{r}
par(mfrow=c(2,3))

# #fun1 <- function(a, y) cor(y, a , use = 'na.or.complete')
# #Correlation_TARGET_FLAG <- sapply(x, FUN = fun1, y=insure_train_full$TARGET_FLAG) 
# 
# show_chart_logi.hist <- function(a, y, ...) {
# #    xlabel <- unlist(str_split(deparse(substitute(a)), pattern = "\\$"))[2]
#     xlabel <- deparse(substitute(a))
#     message(xlabel)
#     logi.hist.plot(a,y,logi.mod = 1, type="hist", boxp=FALSE,col="gray", mainlabel = xlabel)
# }

x <- insure_train_full[,-2]
x <- x[complete.cases(x),]

# sapply(x, FUN = show_chart_logi.hist, y=x$TARGET_FLAG) 

logi.hist.plot(x$REVOKED_Yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'REVOKED_Yes')
logi.hist.plot(x$CAR_USE_Private,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_USE_Private')
logi.hist.plot(x$CAR_TYPE_SUV,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_SUV')
logi.hist.plot(x$PARENT1_Yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'PARENT1_Yes')
logi.hist.plot(x$KIDSDRIV,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'KIDSDRIV')
logi.hist.plot(x$CAR_AGE,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_AGE')
logi.hist.plot(x$JOB_Clerical,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Clerical')
logi.hist.plot(x$HOMEKIDS,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'HOMEKIDS')
logi.hist.plot(x$JOB_Doctor,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Doctor')
logi.hist.plot(x$CLM_FREQ,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CLM_FREQ')
logi.hist.plot(x$SEX_F,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'SEX_F')
logi.hist.plot(x$URBANICITY_Highly.Rural..Rural,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'URBANICITY_Highly.Rural..Rural')
logi.hist.plot(x$MVR_PTS,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'MVR_PTS')
logi.hist.plot(x$EDUCATION_Masters,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_Masters')
logi.hist.plot(x$CAR_TYPE_Van,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Van')
logi.hist.plot(x$CAR_TYPE_Minivan,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Minivan')
logi.hist.plot(x$YOJ,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'YOJ')
logi.hist.plot(x$TIF,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'TIF')
logi.hist.plot(x$MSTATUS_Yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'MSTATUS_Yes')
logi.hist.plot(x$RED_CAR_no,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'RED_CAR_no')
logi.hist.plot(x$JOB_Lawyer,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Lawyer')
logi.hist.plot(x$CAR_TYPE_Pickup,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Pickup')
logi.hist.plot(x$JOB_Student,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Student')
logi.hist.plot(x$OLDCLAIM,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'OLDCLAIM')
logi.hist.plot(x$INCOME,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'INCOME')
logi.hist.plot(x$EDUCATION_Bachelors,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_Bachelors')
logi.hist.plot(x$JOB_Manager,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Manager')
logi.hist.plot(x$EDUCATION_High.School,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_High.School')
logi.hist.plot(x$RED_CAR_yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'RED_CAR_yes')
logi.hist.plot(x$MSTATUS_No,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'MSTATUS_No')
logi.hist.plot(x$JOB_Home.Maker,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Home.Maker')
logi.hist.plot(x$EDUCATION_PhD,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_PhD')
logi.hist.plot(x$TRAVTIME,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'TRAVTIME')
logi.hist.plot(x$JOB_Professional,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Professional')
logi.hist.plot(x$URBANICITY_Highly.Urban..Urban,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'URBANICITY_Highly.Urban..Urban')
logi.hist.plot(x$SEX_M,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'SEX_M')
logi.hist.plot(x$JOB_Blue.Collar,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Blue.Collar')
logi.hist.plot(x$AGE,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'AGE')
logi.hist.plot(x$CAR_TYPE_Sports.Car,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Sports.Car')
logi.hist.plot(x$HOME_VAL,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'HOME_VAL')
logi.hist.plot(x$BLUEBOOK,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'BLUEBOOK')
logi.hist.plot(x$PARENT1_No,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'PARENT1_No')
logi.hist.plot(x$CAR_USE_Commercial,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_USE_Commercial')
logi.hist.plot(x$REVOKED_No,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'REVOKED_No')
logi.hist.plot(x$CAR_TYPE_Panel.Truck,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Panel.Truck')

```


####Interpretation \


You can see that the probability of crashing increases as we get closer to the "1" classification for the CAR_TYPE_Van, RED_CAR_no, JOB_Home.Maker, SEX_F, JOB_Clerical, CAR_TYPE_SUV, TRAVTIME, BLUEBOOK, CAR_TYPE_Pickup, CAR_TYPE_Sports.Car, JOB_Student, KIDSDRIV, JOB_Blue.Collar, HOMEKIDS, MSTATUS_No, EDUCATION_High.School, CAR_USE_Commercial, REVOKED_Yes, PARENT1_Yes, OLDCLAIM, CLM_FREQ, MVR_PTS, URBANICITY_Highly.Urban..Urban variables.  \

You can see that the probability of crashing decreases as we get closer to the "1" classification for the URBANICITY_Highly.Rural..Rural, PARENT1_No, REVOKED_No, HOME_VAL, CAR_USE_Private, CAR_TYPE_Minivan, MSTATUS_Yes, JOB_Manager, AGE, CAR_AGE, TIF, EDUCATION_Masters, YOJ, EDUCATION_PhD, JOB_Lawyer, JOB_Doctor, EDUCATION_Bachelors, JOB_Professional, INCOME, SEX_M, RED_CAR_yes, CAR_TYPE_Panel.Truck variables.  \

\
\
\


#2. Data Preparation 

Now that we have completed the data exploration / analysis, we will be cleaning and consolidating data into two datasets for use in analysis and modeling. \

\
One dataset will be used for building and selecting models for TARGET_FLAG and the other dataset with only the "crash" records will be used for building and selecting models for TARGET_AMT.\
\


We will be following the below steps as guidelines: \
- Outliers treatment \
- Missing values treatment \
- Adding New Variables \



##2.1 Outliers treatment

As the outliers have no impact on the outcome of TARGET_FLAG, we will not be carrying out any transformations for this dataset to handle the outlier.\
\

However, the "crashed" dataset where we predict the "TARGET_AMT" needs to have its outliers handled. 
\
\
In the sections below, we will check different transformations for each of the variables - KIDSDRIV, AGE, HOMEKIDS, MVR_PTS, OLDCLAIM, TIF, TRAVTIME and YOJ - and create the appropriate outlier-handled / transformed variables.  

\

```{r, echo = FALSE, warning=FALSE, message=FALSE}
show_charts <- function(x, ...) {
    xlabel <- unlist(str_split(deparse(substitute(x)), pattern = "\\$"))[2]
    xlab_log <- paste0(xlabel, "_log")
    xlab_sqrt <- paste0(xlabel, "_sqrt")
    xlab_sin <- paste0(xlabel, "_sin")
    xlab_inv <- paste0(xlabel, "_inv")
    
    mdata <- cbind(xlabel=x, xlab_log=log(x), xlab_sqrt=sqrt(x), xlab_sin=sin(x), xlab_inv=1/x)
    mdata2 <- melt(mdata)
    mdata2 <- mdata2[, c(2:3)]
    names(mdata2) <- c("variable", "value")
    
    # Output the boxplot
    p <- ggplot(data = mdata2, aes(x=variable, y=value)) + geom_boxplot() + ggtitle("Outliers identification")
    p + facet_wrap( ~ variable, scales="free", ncol=5)
}

```
\
\

** Transformations for KIDSDRV**
```{r}

show_charts(insure_train_crash$KIDSDRIV)

insure_train_crash$KIDSDRIV_log <- log(insure_train_crash$KIDSDRIV)
insure_train_crash$KIDSDRIV_inv <- 1 / insure_train_crash$KIDSDRIV

```

\

From the above charts we can see that a log or an inverse transformation works well for KIDSDRV. Hence, We create both these variables.
\
\

** Transformations for AGE**
```{r}

show_charts(insure_train_crash$AGE)

insure_train_crash$AGE_sin <- sin(insure_train_crash$AGE)


```

\

From the above charts we can see that a sin transformation works well for AGE. We will create this variable.
\
\

** Transformations for MVR_PTS**
```{r}

show_charts(insure_train_crash$MVR_PTS)

insure_train_crash$MVR_PTS_log <- log(insure_train_crash$MVR_PTS)
insure_train_crash$MVR_PTS_sqrt <- sqrt(insure_train_crash$MVR_PTS)


```

\

From the above charts we can see that a log, sqrt or an inverse transformation works well for MVR_PTS. Hence, We will create these variables.
\
\

** Transformations for TIF**
```{r}
#TIF, TRAVTIME and YOJ

show_charts(insure_train_crash$TIF)

insure_train_crash$TIF_log <- log(insure_train_crash$TIF)
insure_train_crash$TIF_sqrt <- sqrt(insure_train_crash$TIF)
insure_train_crash$TIF_sin <- sin(insure_train_crash$TIF)
insure_train_crash$TIF_inv <- 1 / insure_train_crash$TIF


```

\

From the above charts we can see that a log, sqrt, sin or an inverse transformation works well for TIF. Hence, We will create these variables.
\
\


** Transformations for TRAVTIME**
```{r}
#TRAVTIME and YOJ

show_charts(insure_train_crash$TRAVTIME)

insure_train_crash$TRAVTIME_sin <- sin(insure_train_crash$TRAVTIME)

```

\

From the above charts we can see that a sin transformation works well for TRAVTIME. Hence, We will create this variable.
\
\

** Transformations for YOJ**
```{r}
#YOJ

show_charts(insure_train_crash$YOJ)

insure_train_crash$YOJ_sin <- sin(insure_train_crash$YOJ)

```

\

From the above charts we can see that a sin transformation works well for YOJ. Hence, We will create this variable.
\
\


##2.2 Missing Values treatment

\
\
As we have seen in the data exploration phase, we can do with removing the rows that contain missing values. We now do this for both the datasets:

\
\

```{r}

insure_train_full <- insure_train_full[complete.cases(insure_train_full),]
insure_train_crash <- insure_train_crash[complete.cases(insure_train_crash),]

```
\
\



##2.3 Adding New Variables

In this section, we generate some additional variables that we feel will help the correlations. As before, we do it for both the datasets.

###2.3.1 New Variables for Full Dataset (TARGET_FLAG)

\

The following were some of the observations we made during the data exploration phase for TARGET_FLAG

\
\

CAR_TYPE - If you drive Minivans and Panel Trucks you have lesser chance of being in a crash as against Pickups, Sports, SUVs and Vans. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation. \
\
Accordingly, we will bin these variables as below:
\
CAR_TYPE_FLAG_BIN : \
\
- 1 : if CAR_TYPE is Minivans or Panel Trucks \
- 0 : if CAR_TYPE is Pickups, Sports, SUVs or Vans
\
\


```{r}

insure_train_full$CAR_TYPE_FLAG_BIN <- ifelse(insure_train_full$CAR_TYPE_Minivan | insure_train_full$CAR_TYPE_Panel.Truck, 1, 0)

```

\
\

EDUCATION - If you have only a high school education then you are more likely to crash than if you have a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation.
\
\
Accordingly, we will bin these variables as below:
\
EDUCATION_FLAG_BIN : \
\
- 1 : if EDUCATION is High School \
- 0 : if EDUCATION is Bachelors, Masters or Phd
\
\


```{r}

insure_train_full$EDUCATION_FLAG_BIN <- ifelse(insure_train_full$EDUCATION_High.School, 1, 0)

```



JOB - If you are a Student, Homemaker, or in a Blue Collar or Clerical job, you are more likely to be in a crash against Doctor, Lawyer, Manager or professional.  Again binning this variable will strengthen the correlation.

\
\
Accordingly, we will bin these variables as below:
\
JOB_TYPE_FLAG_BIN : \
\
- 1 : if JOB_TYPE is Student, Homemaker, or in a Blue Collar or Clerical \
- 0 : if JOB_TYPE is Doctor, Lawyer, Manager or professional
\
\

```{r}

insure_train_full$JOB_TYPE_FLAG_BIN <- ifelse(insure_train_full$JOB_Student |  insure_train_full$JOB_Home.Maker | insure_train_full$JOB_Clerical | insure_train_full$JOB_Blue.Collar, 1, 0)

```


###2.3.2 New Variables for Crashed Dataset (TARGET_AMT)

\

The following were some of the observations we made during the data exploration phase for TARGET_AMT

\
\

CAR_TYPE - If you drive Vans or Panel Trucks your cost of repair seems to increase as against Minivan, Pickup, Sports.Car, SUV. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation.

\
\
Accordingly, we will bin these variables as below:
\
CAR_TYPE_AMT_BIN : \
\
- 1 : if CAR_TYPE is Vans or Panel Trucks \
- 0 : if CAR_TYPE is Pickups, Sports, SUVs or Minivans
\
\

```{r}

insure_train_crash$CAR_TYPE_AMT_BIN <- ifelse(insure_train_crash$CAR_TYPE_Van | insure_train_crash$CAR_TYPE_Panel.Truck, 1, 0)

```

EDUCATION - If you have only a high school education then your cost of repair is less compared to a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation.
\
\

Accordingly, we will bin these variables as below:
\
EDUCATION_AMT_BIN : \
\
- 1 : if EDUCATION is High School \
- 0 : if EDUCATION is Bachelors, Masters or Phd
\
\


```{r}

insure_train_crash$EDUCATION_AMT_BIN <- ifelse(insure_train_crash$EDUCATION_High.School, 1, 0)

```

JOB - If you are a Lawyer, Professional or in a Blue Collar job, you spend more on repairs as compared to a Doctor, Manager, Home Maker, Student, or Clerical job.  Again binning this variable will strengthen the correlation.

\
\

Accordingly, we will bin these variables as below:
\
JOB_TYPE_AMT_BIN : \
\
- 1 : if JOB_TYPE is Lawyer, Professional or in a Blue Collar \
- 0 : if JOB_TYPE is Doctor, Manager, Home Maker, Student, or Clerical
\
\

```{r}

insure_train_crash$JOB_TYPE_AMT_BIN <- ifelse(insure_train_crash$JOB_Lawyer |  insure_train_crash$JOB_Professional | insure_train_crash$JOB_Blue.Collar, 1, 0)

```



