---
title: "Home Work Assignment - 04"
author: "Critical Thinking Group 5"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

\newpage

#1 Overview 
The data set contains approximately 8161 records. Each record represents a customer profile at an auto insurance company. Each record has two response variables. 
\
\
The first response variable, TARGET_FLAG, is a 1 or a 0. A "1" means that the person was in a car crash. A zero means that the person was not in a car crash. 
\
\
The second response variable is TARGET_AMT. This is the amount spent on repairs if there was a crash. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.
\
\
We will be exploring, analyzing, and modeling the training data. Since there are 2 different predictions we have to work with, we will deal with each prediction independently. The following are the 2 predictions we will be modeling for:\
\
1. TARGET_FLAG - This dependent variable tells whether there was a crash or not. This is a binary variable and as such we will be using a Logistic Regression Model to predict this.\
2. TARGET_AMT - This dependent variable gives the amount / cost of repairs if there was a crash. This is a continuous variable and we will ve using a Linear Regression Model to predict this.
\
\
Each of the above models will be built and evaluated separately. In the first section of this document we will deal with the Logistic Model for TARGET_FLAG and in the second section we will deal with Linear Model for the TARGET_AMT
\
\
Out of the many models for each task, we will go ahead and shortlist one model that works the best. We will then use these models (one for each task) on the test / evaluation data.
\
\
To attain our objective, we will be follow the below steps for each modeling exercise:\

1 -Data Exploration \
2 -Data Preparation \
3 -Build Models \
4 -Select Models \
\
**Model Selection Strategy:** As a strategy, we will split the train dataset into 2 parts - TRAIN and VALID. In the VALID dataset, we will hold out some values to validate how well the model is trained using the TRAIN dataset. We will then use the Model that performs the best on the EVALUATION data to give the required output. We will split the TRAIN / VALID data after the **Data Exploration / Preparation** before the **Build Models**.
\
\
**Please Note:** 
\
\
- While working on the Linear Models for the TARGET_AMT, we will be using only a subset of the data where the TARGET_FLAG = 1. This will give us all the records where there was a crash and subsequently a repair amount.\
\
- While Predicting the TARGET_AMT with the given Evaluation dataset, We will do 2 evaluations:\
1. Independent Prediction - Here we use the Evaluation dataset variables to predict the TARGET_AMT without the need to predict the TARGET_FLAG since the model has been developed independently.\
2. Dependent Prediction - Here we will take the output of the TARGET_FLAG predictions on the Evaluation dataset and use only those rows that were classified as a "Crash" and use it as the input to the TARGET_AMT prediction. So this is a two step prediction, one for the TARGET_FLAG and using the output to predict TARGET_AMT.
\
\


\newpage

#2 Logistic Regression for TARGET_FLAG\



In this section we will use Logistic regression to model the TARGET_FLAG. We will first start with the Data Exploration.\
\


##2.1 Data Exploration Analysis\

In this sub-section we will explore and gain some insights into the dataset by pursuing the below high level steps and inquiries: \
-Variable Identification / Relationships\
-Data Summary Analysis\
-Missing Values\
-Outliers identification\ 
-Analysis the link function \


###2.1.1 Variable Identification\


First let's display and examine the data dictionary or the data columns as shown in table 1

```{r, echo = FALSE, warning=FALSE, message=FALSE}
if (!require("ggplot2",character.only = TRUE)) (install.packages("ggplot2",dep=TRUE))
if (!require("MASS",character.only = TRUE)) (install.packages("MASS",dep=TRUE))
if (!require("knitr",character.only = TRUE)) (install.packages("knitr",dep=TRUE))
if (!require("xtable",character.only = TRUE)) (install.packages("xtable",dep=TRUE))
if (!require("dplyr",character.only = TRUE)) (install.packages("dplyr",dep=TRUE))
if (!require("psych",character.only = TRUE)) (install.packages("psych",dep=TRUE))
if (!require("stringr",character.only = TRUE)) (install.packages("stringr",dep=TRUE))
if (!require("car",character.only = TRUE)) (install.packages("car",dep=TRUE))
if (!require("faraway",character.only = TRUE)) (install.packages("faraway",dep=TRUE))
if (!require("dummy",character.only = TRUE)) (install.packages("dummy",dep=TRUE))
if (!require("reshape2",character.only = TRUE)) (install.packages("reshape2",dep=TRUE))
if (!require("popbio",character.only = TRUE)) (install.packages("popbio",dep=TRUE))


library(ggplot2)
library(MASS)
library(knitr)
library(xtable)
library(dplyr)
library(psych)
library(stringr)
library(car)
library(faraway)
library(dummy)
library(reshape2)
library(popbio)

insure_train_full <- read.csv("https://raw.githubusercontent.com/kishkp/data621-ctg5/master/HW4/insurance_training_data.csv")

insurevars <- read.csv("https://raw.githubusercontent.com/kishkp/data621-ctg5/master/HW4/insurevars.csv")
kable(insurevars, caption = "Variable Description")

```
\
We notice that there are 2 dependent variables - TARGET_FLAG and TARGET_AMT. Apart from these 2 dependent variables, we have 23 independent or predictor variables.\
\

```{r, echo = FALSE, warning=FALSE, message=FALSE}
str(insure_train_full)

levels(insure_train_full$MSTATUS)
levels(insure_train_full$SEX)
levels(insure_train_full$EDUCATION)
levels(insure_train_full$JOB)
levels(insure_train_full$CAR_TYPE)
levels(insure_train_full$URBANICITY)
levels(insure_train_full$REVOKED)


summary(insure_train_full)

```
\
\
From the output above we can make the following observations:\
\
- some numeric variables like INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM have been converted to Factor variables. This needs to be set right.\
\
- Some of the variables like MSTATUS, SEX, EDUCATION, JOB, CAR_TYPE, URBANICITY have some of the values encoded with "z_". Not that this will impact the analysis, but it will look a bit odd. So we will be fixing this.
\
\
- EDUCATION has 2 "High School" values - one starting with "<" and another starting with "z_". It is assumed that both these values are to be converted to "HIGH School".
\
\
- JOB has a "" value. This would indicate that the job is unknown or is not coded. Hence, we will replace this with "Unknown".
\
\
- There are records where CAR_AGE is negative which is not possible. Upon investigation, we find that this is a single record that have the negative value. We will remove this record.
\
\
- We will also create dummy variables for all the factors. 
\
\
- Please note that we will not be using INDEX variable as it serves as just an identifier for each row. And has no relationships to other variables. 
\
\
- Similarly, we will drop the TARGET_AMT since we will not be using this as a predictor in this exercise.
\
\
Making the above fixes to the data, we now have a "clean" dataset which can be explored further.
\
\


```{r, echo = FALSE, warning=FALSE, message=FALSE}
#- some numeric variables like INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM have been converted to Factor variables. This needs to be set right.

insure_train_full$INCOME <- as.numeric(str_replace_all(insure_train_full$INCOME, pattern =  "[\\$*,]", replacement = ""))
insure_train_full$HOME_VAL <- as.numeric(str_replace_all(insure_train_full$HOME_VAL, pattern =  "[\\$*,]", replacement = ""))
insure_train_full$BLUEBOOK <- as.numeric(str_replace_all(insure_train_full$BLUEBOOK, pattern =  "[\\$*,]", replacement = ""))
insure_train_full$OLDCLAIM <- as.numeric(str_replace_all(insure_train_full$OLDCLAIM, pattern =  "[\\$*,]", replacement = ""))

#head(insure_train_full[, c(8,27,10,28, 17,29, 21, 30)], 20)


#- Some of the variables like MSTATUS, SEX, EDUCATION, JOB, CAR_TYPE, URBANICITY have some of the values encoded with "z_". Not that this will impact the analysis, but it will look a bit odd. So we will be fixing this.

#- EDUCATION has 2 "High School" values - one starting with "<" and another starting with "z_". It is assumed that both these values are to be converted to "HIGH School".

#- JOB has a "" value. This needs to be replaced with Unknown.

#- There are records where CAR_AGE is negative which is not possible. Upon investigation, we find that this is a single record that have the negative value. We will remove this record.

insure_train_full$MSTATUS <- as.factor(str_replace_all(insure_train_full$MSTATUS, "z_", ""))
insure_train_full$SEX <- as.factor(str_replace_all(insure_train_full$SEX, "z_", ""))
insure_train_full$EDUCATION <- as.factor(str_replace_all(insure_train_full$EDUCATION, "z_", ""))
insure_train_full$EDUCATION <- as.factor(str_replace_all(insure_train_full$EDUCATION, "<", ""))
insure_train_full$CAR_TYPE <- as.factor(str_replace_all(insure_train_full$CAR_TYPE, "z_", ""))
insure_train_full$URBANICITY <- as.factor(str_replace_all(insure_train_full$URBANICITY, "z_", ""))


insure_train_full$JOB <- as.character(insure_train_full$JOB)
insure_train_full$JOB[insure_train_full$JOB==""] <- "Unknown"
insure_train_full$JOB <- as.factor(str_replace_all(insure_train_full$JOB, "z_", ""))

insure_train_full <- filter(insure_train_full, INDEX !=as.integer(filter(insure_train_full, insure_train_full$CAR_AGE<0)[1]))

#- We will also create dummy variables for all the factors and drop the original variables. 

dummy_vars<-as.data.frame(sapply(dummy(insure_train_full), FUN = as.numeric))
dummy_vars <- dummy_vars-1

# EDU_ = factor(insure_train_full$EDUCATION)
# dummies = model.matrix(~EDU_)

insure_train_full <- cbind(select(insure_train_full, -PARENT1, -MSTATUS, -SEX, -EDUCATION, -JOB, -CAR_USE, -CAR_TYPE, -RED_CAR, -REVOKED, -URBANICITY), dummy_vars)

# insure_train_full <- cbind(insure_train_full, dummy_vars)

# - Please note that we will not be using INDEX variable as it serves as just an identifier for each row. And has no relationships to other variables.   

insure_train_full <- select(insure_train_full, -INDEX, -TARGET_AMT)


```
\
\


###2.1.2 Data Summary and Correlation Analysis\

\
\
In this section, we will create summary data to better understand the relationship each of the variables have with our dependent variables using correlation, central tendency, and dispersion as shown below:  
\
\

```{r, echo = FALSE, warning=FALSE, message=FALSE, results='hide'}
ds_stats <- psych::describe(insure_train_full, skew = TRUE, na.rm = TRUE)
#ds_stats
kable(ds_stats[1:7], caption= "Data Summary")
kable(ds_stats[8:13], caption= "Data Summary (Cont)")

fun1 <- function(a, y) cor(y, a , use = 'na.or.complete')
x<-insure_train_full[,]
Correlation_TARGET_FLAG <- sapply(x, FUN = fun1, y=insure_train_full$TARGET_FLAG) 

```
\
\

Now we will produce the correlation table between the independent variables and the dependent variable - TARGET_FLAG  
\
\

```{r, echo = FALSE, warning=FALSE, message=FALSE}
Correlation_TARGET_FLAG <- sort(Correlation_TARGET_FLAG, decreasing = TRUE)
kable(data.frame(Correlation_TARGET_FLAG), caption = "Correlation between TARGET_FLAG and predictor variables")

```
\
\

The above table suggests that none of the variables seem to have a very strong correlation with TARGET_FLAG. However, CAR_TYPE_Van, RED_CAR_no, JOB_Home.Maker, SEX_F, JOB_Clerical, CAR_TYPE_SUV, TRAVTIME, CAR_TYPE_Pickup, CAR_TYPE_Sports.Car, JOB_Student, JOB_Blue.Collar, KIDSDRIV, HOMEKIDS, MSTATUS_No, OLDCLAIM, EDUCATION_High.School, CAR_USE_Commercial, REVOKED_Yes, PARENT1_Yes, CLM_FREQ, MVR_PTS, URBANICITY_Highly.Urban..Urban have a positive correlation. 
\
\
Similarly, URBANICITY_Highly.Rural..Rural, HOME_VAL, PARENT1_No, REVOKED_No, CAR_USE_Private, INCOME, CAR_TYPE_Minivan, MSTATUS_Yes, JOB_Manager, BLUEBOOK, AGE, CAR_AGE, TIF, EDUCATION_Masters, YOJ, EDUCATION_PhD, JOB_Lawyer, JOB_Doctor, EDUCATION_Bachelors, JOB_Professional, SEX_M, RED_CAR_yes, CAR_TYPE_Panel.Truck have a negative correlation. 
\
\
Lets now see how values in some of the variable affects the correlation:
\
\
CAR_TYPE - If you drive Minivans and Panel Trucks you have lesser chance of being in a crash as against Pickups, Sports, SUVs and Vans. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation.
\
\
EDUCATION - If you have only a high school education then you are more likely to crash than if you have a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation.
\
\
JOB - If you are a Student, Homemaker, or in a Blue Collar or Clerical job, you are more likely to be in a crash against Doctor, Lawyer, Manager, professional or Unknown job.  Again binning this variable will strengthen the correlation.
\
\

Lets explore the KIDSDRIV variable.
\
\

```{r}

table(insure_train_full$KIDSDRIV)

```
\
\


We can see from the above table that there are many insurers that do not have driving kids. So we can bin this variable as well.
\
\

Similarly, having a look at a few more variables for binning, we get the below output:

```{r}

table(insure_train_full$HOMEKIDS)

HOMEKIDS_0 <- insure_train_full[insure_train_full$TARGET_FLAG==0,'HOMEKIDS' ]
HOMEKIDS_1 <- insure_train_full[insure_train_full$TARGET_FLAG==1,'HOMEKIDS' ]


income_0 <- insure_train_full[insure_train_full$TARGET_FLAG==0,6 ]








```



We will carry out the above transformations in the Data Preparation phase.
\
\




###2.1.3 Missing Values\


\
\
Based on the missing data from the below table, we can see that there are a few missing values for AGE, YOJ, INCOME, HOME_VAL, CAR_AGE variables. 
\
\
```{r, echo = FALSE, warning=FALSE, message=FALSE}
missings<- sapply(insure_train_full,function(x) sum(is.na(x)))
kable(data.frame(missings), caption = "Missing Values")

```

\
\

We can try and impute values to AGE, YOJ, INCOME, HOME_VAL, CAR_AGE. Lets look at the distributions for each of the variable to determine the value to use to impute. 


```{r}

par(mfrow=c(2,3))
hist(insure_train_full$AGE)
hist(insure_train_full$YOJ)
hist(insure_train_full$INCOME)
hist(insure_train_full$HOME_VAL)
hist(insure_train_full$CAR_AGE)
```

\
\

Given that Age and YOJ look to be somewhat normally distributed, we can go ahead and use the mean to impute the missing values for these variables. For INCOME, HOME_VAL and CAR_AGE the median seems to be a better value to impute since there are strong right skews. We will carry out these transformation while data preparation.
\
\




###2.1.4 Outliers identification\ 



In this sub-section, we will look at the boxplots and determine the outliers in variables and decide on whether to act on the outliers. We will do the outliers only on the numeric variables. Below are the plots:

```{r, echo = FALSE, warning=FALSE, message=FALSE}

#
mdata<- select(insure_train_full, KIDSDRIV, AGE, BLUEBOOK, CAR_AGE, CLM_FREQ, HOME_VAL, HOMEKIDS, INCOME, MVR_PTS, OLDCLAIM, TIF, TRAVTIME, YOJ)
mdata2 <- melt(mdata)
# Output the boxplot
p <- ggplot(data = mdata2, aes(x=variable, y=value)) + 
  geom_boxplot() + ggtitle("Outliers Identification")
p + facet_wrap( ~ variable, scales="free", ncol=5)

```

From the "Outliers identification" plot above, we see that we have few outliers that we need to treat. We see that: KIDSDRIV, AGE, HOMEKIDS, MVR_PTS, OLDCLAIM, TIF, TRAVTIME, YOJ  need to be treated when we do the data preparation for modeling the TARGET_FLAG. 
\
\



###2.1.5 Analysis the link function \



In this section, we will investigate how our initial data aligns with a typical logistic model plot. 

Recall the Logistic Regression is part of a larger class of algorithms known as Generalized Linear Model (glm).  The fundamental equation of generalized linear model is:

$g(E(y)) = a+ Bx_1+B_2x_2+ B_3x_3+...$   

where, g() is the link function, E(y) is the expectation of target variable and $B_0 + B_1x_1 + B_2x_2+B_3x_3$ is the linear predictor ( $B_0,B_1,B_2, B_3$ to be predicted). The role of link function is to 'link' the expectation of y to linear predictor.

In logistic regression, we are only concerned about the probability of outcome dependent variable ( success or failure). As described above, g() is the link function. This function is established using two things: Probability of Success (p) and Probability of Failure (1-p).  p should meet following criteria:
It must always be positive (since p >= 0)
It must always be less than equals to 1 (since p <= 1).

Now let's investigate how our initial data model aligns with the above criteria. In other words, we will plot regression model plots for each variable and compare it to a typical logistic model plot:


```{r, echo = FALSE, warning=FALSE, message=FALSE}
par(mfrow=c(2,3))

# #fun1 <- function(a, y) cor(y, a , use = 'na.or.complete')
# #Correlation_TARGET_FLAG <- sapply(x, FUN = fun1, y=insure_train_full$TARGET_FLAG) 
# 
# show_chart_logi.hist <- function(a, y, ...) {
# #    xlabel <- unlist(str_split(deparse(substitute(a)), pattern = "\\$"))[2]
#     xlabel <- deparse(substitute(a))
#     message(xlabel)
#     logi.hist.plot(a,y,logi.mod = 1, type="hist", boxp=FALSE,col="gray", mainlabel = xlabel)
# }

x <- insure_train_full[,-2]
x <- x[complete.cases(x),]

# sapply(x, FUN = show_chart_logi.hist, y=x$TARGET_FLAG) 

logi.hist.plot(x$REVOKED_Yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'REVOKED_Yes')
logi.hist.plot(x$CAR_USE_Private,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_USE_Private')
logi.hist.plot(x$CAR_TYPE_SUV,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_SUV')
logi.hist.plot(x$PARENT1_Yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'PARENT1_Yes')
logi.hist.plot(x$KIDSDRIV,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'KIDSDRIV')
logi.hist.plot(x$CAR_AGE,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_AGE')
logi.hist.plot(x$JOB_Clerical,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Clerical')
logi.hist.plot(x$HOMEKIDS,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'HOMEKIDS')
logi.hist.plot(x$JOB_Doctor,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Doctor')
logi.hist.plot(x$CLM_FREQ,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CLM_FREQ')
logi.hist.plot(x$SEX_F,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'SEX_F')
logi.hist.plot(x$URBANICITY_Highly.Rural..Rural,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'URBANICITY_Highly.Rural..Rural')
logi.hist.plot(x$MVR_PTS,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'MVR_PTS')
logi.hist.plot(x$EDUCATION_Masters,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_Masters')
logi.hist.plot(x$CAR_TYPE_Van,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Van')
logi.hist.plot(x$CAR_TYPE_Minivan,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Minivan')
logi.hist.plot(x$YOJ,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'YOJ')
logi.hist.plot(x$TIF,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'TIF')
logi.hist.plot(x$MSTATUS_Yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'MSTATUS_Yes')
logi.hist.plot(x$RED_CAR_no,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'RED_CAR_no')
logi.hist.plot(x$JOB_Lawyer,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Lawyer')
logi.hist.plot(x$CAR_TYPE_Pickup,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Pickup')
logi.hist.plot(x$JOB_Student,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Student')
logi.hist.plot(x$OLDCLAIM,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'OLDCLAIM')
logi.hist.plot(x$INCOME,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'INCOME')
logi.hist.plot(x$EDUCATION_Bachelors,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_Bachelors')
logi.hist.plot(x$JOB_Manager,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Manager')
logi.hist.plot(x$EDUCATION_High.School,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_High.School')
logi.hist.plot(x$RED_CAR_yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'RED_CAR_yes')
logi.hist.plot(x$MSTATUS_No,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'MSTATUS_No')
logi.hist.plot(x$JOB_Home.Maker,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Home.Maker')
logi.hist.plot(x$EDUCATION_PhD,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_PhD')
logi.hist.plot(x$TRAVTIME,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'TRAVTIME')
logi.hist.plot(x$JOB_Professional,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Professional')
logi.hist.plot(x$URBANICITY_Highly.Urban..Urban,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'URBANICITY_Highly.Urban..Urban')
logi.hist.plot(x$SEX_M,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'SEX_M')
logi.hist.plot(x$JOB_Blue.Collar,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Blue.Collar')
logi.hist.plot(x$AGE,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'AGE')
logi.hist.plot(x$CAR_TYPE_Sports.Car,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Sports.Car')
logi.hist.plot(x$HOME_VAL,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'HOME_VAL')
logi.hist.plot(x$BLUEBOOK,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'BLUEBOOK')
logi.hist.plot(x$PARENT1_No,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'PARENT1_No')
logi.hist.plot(x$CAR_USE_Commercial,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_USE_Commercial')
logi.hist.plot(x$REVOKED_No,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'REVOKED_No')
logi.hist.plot(x$CAR_TYPE_Panel.Truck,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Panel.Truck')

```
\
\



####2.1.5.1 Interpretation 


\
\
You can see that the probability of crashing increases as we get closer to the "1" classification for the CAR_TYPE_Van, RED_CAR_no, JOB_Home.Maker, SEX_F, JOB_Clerical, CAR_TYPE_SUV, TRAVTIME, BLUEBOOK, CAR_TYPE_Pickup, CAR_TYPE_Sports.Car, JOB_Student, KIDSDRIV, JOB_Blue.Collar, HOMEKIDS, MSTATUS_No, EDUCATION_High.School, CAR_USE_Commercial, REVOKED_Yes, PARENT1_Yes, OLDCLAIM, CLM_FREQ, MVR_PTS, URBANICITY_Highly.Urban..Urban variables.
\
\

You can see that the probability of crashing decreases as we get closer to the "1" classification for the URBANICITY_Highly.Rural..Rural, PARENT1_No, REVOKED_No, HOME_VAL, CAR_USE_Private, CAR_TYPE_Minivan, MSTATUS_Yes, JOB_Manager, AGE, CAR_AGE, TIF, EDUCATION_Masters, YOJ, EDUCATION_PhD, JOB_Lawyer, JOB_Doctor, EDUCATION_Bachelors, JOB_Professional, INCOME, SEX_M, RED_CAR_yes, CAR_TYPE_Panel.Truck variables.  \
\
\
\
\
\



##2.2 Data Preparation\ 


\
Now that we have completed the data exploration / analysis, we will be transforming the data for use in analysis and modeling. \
\
\
We will be following the below steps as guidelines: \
- Outliers treatment \
- Missing values treatment \
- Adding New Variables \
\




###2.2.1 Outliers treatment\



In this sub-section, we will check different transformations for each of the variables - KIDSDRIV, AGE, HOMEKIDS, MVR_PTS, OLDCLAIM, TIF, TRAVTIME and YOJ - and create the appropriate outlier-handled / transformed variables.  
\
\
```{r, echo = FALSE, warning=FALSE, message=FALSE}
show_charts <- function(x, varlab, ...) {
    xlabel <- varlab
    xlab_log <- paste0(xlabel, '_log')
    xlab_sqrt <- paste0(xlabel, '_sqrt')
    xlab_sin <- paste0(xlabel, '_sin')
    xlab_inv <- paste0(xlabel, '_inv')
    
    mdata <- cbind(x, log(x), sqrt(x), sin(x), 1/x)
    colnames(mdata) <- c(xlabel, xlab_log, xlab_sqrt, xlab_sin, xlab_inv)
    mdata2 <- melt(mdata)
    mdata2 <- mdata2[, c(2:3)]
    names(mdata2) <- c("variable", "value")
    
    # Output the boxplot
    p <- ggplot(data = mdata2, aes(x=variable, y=value)) + geom_boxplot() + ggtitle("Outliers identification")
    p + facet_wrap( ~ variable, scales="free", ncol=5)
}

```
\
\




**Transformations for AGE**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
summary(insure_train_full$AGE)
show_charts(insure_train_full$AGE, 'AGE')
insure_train_full$AGE_sin <- sin(insure_train_full$AGE)
```
\
\
From the above charts we can see that a sin transformation works well for AGE. We will create this variable.
\
\


**Transformations for OLDCLAIM**


```{r, echo = FALSE, warning=FALSE, message=FALSE}

summary(insure_train_full$OLDCLAIM)
show_charts(insure_train_full$OLDCLAIM, 'OLDCLAIM')

insure_train_full$OLDCLAIM_log <- log(insure_train_full$OLDCLAIM)
insure_train_full$OLDCLAIM_sqrt <- sqrt(insure_train_full$OLDCLAIM)
insure_train_full$OLDCLAIM_inv <- 1/(insure_train_full$OLDCLAIM)
```
\
\
From the above charts we can see that a log, sqrt or an inverse transformation works better for OLDCLAIM. Hence, We will create these variables.
\
\


**Transformations for TIF**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
summary(insure_train_full$TIF)
show_charts(insure_train_full$TIF, 'TIF')

insure_train_full$TIF_log <- log(insure_train_full$TIF)
insure_train_full$TIF_sqrt <- sqrt(insure_train_full$TIF)
insure_train_full$TIF_sin <- sin(insure_train_full$TIF)
insure_train_full$TIF_inv <- 1 / insure_train_full$TIF
```
\
\

From the above charts we can see that a log, sqrt, sin or an inverse transformation works well for TIF. Hence, We will create these variables.
\
\



**Transformations for TRAVTIME**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
#TRAVTIME and YOJ

summary(insure_train_full$TRAVTIME)
show_charts(insure_train_full$TRAVTIME, 'TRAVTIME')

insure_train_full$TRAVTIME_sin <- sin(insure_train_full$TRAVTIME)

```
\
\
From the above charts we can see that a sin transformation works well for TRAVTIME. Hence, We will create this variable.
\
\



**Transformations for YOJ**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
summary(insure_train_full$YOJ)
show_charts(insure_train_full$YOJ, 'YOJ')
insure_train_full$YOJ_sin <- sin(insure_train_full$YOJ)
```
\
\
From the above charts we can see that a sin transformation works well for YOJ. This variable also have 0s as values. However, we are not doing any log or inverse transformations, so we can skip adding the constant and create a sin variable.
\
\



###2.2.2 Missing Values treatment\


\
\
As we have seen in the data exploration phase, we can do with removing the rows that contain missing values. We now do this removal of rows with missing information:
\
\

```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full <- insure_train_full[complete.cases(insure_train_full),]
#insure_train_crash <- insure_train_crash[complete.cases(insure_train_crash),]

```
\
\



###2.2.3 Adding New Variables\


\
\


In this section, we generate some additional variables that we feel will help the correlations. The following were some of the observations we made during the data exploration phase for TARGET_FLAG
\
\
CAR_TYPE - If you drive Minivans and Panel Trucks you have lesser chance of being in a crash as against Pickups, Sports, SUVs and Vans. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation. Accordingly, we will bin this variable as below:
\
\
CAR_TYPE_FLAG_BIN : \
\
- 1 : if CAR_TYPE is Minivans or Panel Trucks \
- 0 : if CAR_TYPE is Pickups, Sports, SUVs or Vans
\
\

```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$CAR_TYPE_FLAG_BIN <- ifelse(insure_train_full$CAR_TYPE_Minivan | insure_train_full$CAR_TYPE_Panel.Truck, 1, 0)

```
\
\
EDUCATION - If you have only a high school education then you are more likely to crash than if you have a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation:
\
\
EDUCATION_FLAG_BIN : \
\
- 0 : if EDUCATION is High School \
- 1 : if EDUCATION is Bachelors, Masters or Phd
\
\
```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$EDUCATION_FLAG_BIN <- ifelse(insure_train_full$EDUCATION_High.School, 0, 1)

```
\
\
JOB - If you are a Student, Homemaker, or in a Blue Collar or Clerical job, you are more likely to be in a crash against Doctor, Lawyer, Manager or professional.  Again binning this variable will strengthen the correlation:\
\
JOB_TYPE_FLAG_BIN : \
\
- 1 : if JOB_TYPE is Student, Homemaker, or in a Blue Collar or Clerical \
- 0 : if JOB_TYPE is Doctor, Lawyer, Manager, professional, Unknown
\
\
```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$JOB_TYPE_FLAG_BIN <- ifelse(insure_train_full$JOB_Student |  insure_train_full$JOB_Home.Maker | insure_train_full$JOB_Clerical | insure_train_full$JOB_Blue.Collar, 1, 0)

```
\
\


##2.3 Build Models



In this section, we will create 4 models. Aside from using original and transformed data, we will also using different methods and functions such as Linear Discriminant Analysis, step function, and logit function to enhance our models. 
\newline
\newline
Below is our model definition: \
-Model 1- This model will be created using all the variables in train data set with logit function GLM. \
-Model 2- This model will be created using all the variables; however using step function instead of GLM. \
-Model 3: this model will be created using original variables using Linear Discriminant Analysis function lda in ISLR  package.\
\
\


###2.3.1 Prepare TRAIN and VALID datasets


However, prior to that, we hold out a subset of data as a validation dataset to check model performance. This will be useful when we select a model.
\
\

```{r}

smp_size <- floor(0.80 * nrow(insure_train_full))

## set the seed to make your partition reproductible
set.seed(123)

train_index <- sample(seq_len(nrow(insure_train_full)), size = smp_size)

DS_TARGET_FLAG_TRAIN<- insure_train_full[train_index, ]
DS_TARGET_FLAG_VALID <- insure_train_full[-train_index, ]
```


###2.3.2 Model 1


In this model, we will be using all the given variables in train data set.  We will create model using logit function and we will highlight the summary of the model. \
 


```{r, echo = FALSE, warning=FALSE, message=FALSE}
model1 <- glm(TARGET_FLAG ~ ., data = DS_TARGET_FLAG_TRAIN, family = "binomial")
summary(model1)
```

##### Interpretation for model 1  \

$\newline$
(i) Based on the outcome, it can be seen that indus, chas, rm, age, black, and lstat are not statistically significant. 

(ii)As for the statistically significant variables, nox has the lowest p-value suggesting a strong association of the nox to the target variable. Other important variables are dis, rad, tax, ptratio, and medv. The AIC value for the model1 =168.71.

(iii) The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variables.

  a. For every one unit change in nox, the log odds of crime rate above median value increases by 53.41.\
  b. For a one unit increase in dis, the log odds of crime rate above median value increases by 0.80.\
  c. For a one unit increase in rad, the log odds of crime rate above median value increases by 0.72.\
  d. For a one unit increase in tax, the log odds of crime rate above median value increases by -0.007. Tax has a negative impact on crime rate.\
  e. For a one unit increase in ptratio, the log odds of crime rate above median value increases by 0.44.\
  f. For a one unit increase in medv , the log odds of crime rate above median value increases by 0.23.\
 
 (iv) No. of iterations are 9 before lowest value of AIC was derived for this model.

\
\


###2.3.3 Model 2


This model, we will be using original variables; however using step function (backward process) instead of GLM.


```{r step for model 1 creation,echo=FALSE,results="hide"}

stepmodel1<- step(model1, direction="backward")

```
\

```{r step for model 1,echo=FALSE}

summary(stepmodel1)

```

#####  Interpretation for model 2  \

$\newline$
(i)It can be seen that zn, age, and black are not statistically significant. 

(ii)As for the statistically significant variables, nox has the lowest p-value suggesting a strong association of the nox of the target variable. other important variables are dis, rad, tax, ptratio, medv, and lstat. 
The AIC value for the model1 =164.85.

(iii) The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variables.

  a. For every one unit change in nox, the log odds of crime rate above median value increases by 46.61.\
  b. For a one unit increase in dis, the log odds of crime rate above median value increases by 0.71.\
  c. For a one unit increase in rad, the log odds of crime rate above median value increases by 0.77.\
  d. For a one unit increase in tax, the log odds of crime rate above median value increases by -0.009.\
  e. For a one unit increase in ptratio, the log odds of crime rate above median value increases by 0.35.\
  f. For a one unit increase in medv , the log odds of crime rate above median value increases by 0.18\

(iv) there were 9 iterations in backward steps before final model was selected

\
\



###2.3.4 Model 3


In this model we will be using original variables; however the Linear Discriminant Analysis function lda in ISLR package.

```{r model with Linear Discriminant Analysis, eval=TRUE,echo=FALSE}


# model3=lda(target~.,data=city_crime_train)
# 
# model3


```

#####  Interpretation for model 5  \

$\newline$
(i)The Classification boundary equation for our model 5 is below: \

 $-0.004 * zn + 0.0281 * indus - 0.055 * chas + 7.910 * nox + 0.165 * rm + 0.013 * age + 0.084 * dis + 0.102 * rad - 0.001 * tax + 0.009 * ptratio  - 0.0009 * black + 0.024 * lstat + 0.042 * medv =0$


(ii) From summary table, we also have the prior probability of of success is 0.4731183 and failure is 0.5268817.\

(iii) Group means provides mean values for each variable with respect to target variable values 0 and 1.\

 (iv) This model has accuracy value 82.97 % which is less compare to the other models. LDA model assumes normality of the variables used in the model and theere are some variables which are not normally distributed and have outliers that is impacting the result out of this model.



##2.4 Select Models
###2.4.1 Model Evaluation Using TRAIN Data 
###2.4.2 Model Evaluation Using VALID Data
###2.4.3 Selection Summary
##2.5 Prediction Using Evaluation Data
###2.5.1 Tranformation of Evaluation Data 
###2.5.2 Predict Using Selected Model
###2.5.3 Model Output
###2.5.4 Inference



\newpage



#3 Linear Regression for TARGET_AMT\



In this section we will use Linear regression to model the TARGET_AMT. We will first start with the Data Exploration.\
\


##3.1 Data Exploration Analysis\


\

In this analysis, we will be using only those records where the TARGET-FLAG is 1. This indicates that the vehicle crashed. In such a scenario, we will be modeling the cost of repair using Linear Regression. 
\
\
First, lets create the required data set for the "Crashed" data from the existing "clean" full data and look at the structure of the resulting dataset.
\
\
We will remove from the new "crashed" dataset all those variables that were created specifically for predicting TARGET_FLAG. We willbe creating these variables separately for predicting TARGET_AMT.
\
\

```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_crash <- insure_train_full[insure_train_full$TARGET_FLAG==1,]
insure_train_crash <- select(insure_train_crash, -TARGET_FLAG)
insure_train_crash <- insure_train_crash[, c(1:46)]
str(insure_train_crash)
```
\
\
We notice that the dependent variable here is TARGET_AMT. Apart from the dependent variables, we have 45 independent or predictor variables.\
\
Also, since we created this dataset from the "Clean" full dataset, we already have taken care of missing values, Outliers etc.
\
\
However, we may need to look into the correlations again since we have a new target variable to correlate against.
\
\


###3.1.1 Data Summary and Correlation Analysis\


\
\
In this section, we will create summary data to better understand the relationship each of the variables have with our dependent variables using correlation, central tendency, and dispersion as shown below:  
\
\

```{r, echo = FALSE, warning=FALSE, message=FALSE, results='hide'}
ds_stats <- psych::describe(insure_train_crash, skew = TRUE, na.rm = TRUE)
#ds_stats
kable(ds_stats[1:7], caption= "Data Summary")
kable(ds_stats[8:13], caption= "Data Summary (Cont)")

fun1 <- function(a, y) cor(y, a , use = 'na.or.complete')
x<-insure_train_crash[,]
Correlation_TARGET_AMT <- sapply(x, FUN = fun1, y=insure_train_crash$TARGET_AMT) 
```
\
\
Now we will produce the correlation table between the independent variables and the dependent variable - TARGET_AMT  
\
\
```{r, echo = FALSE, warning=FALSE, message=FALSE}
Correlation_TARGET_AMT <- sort(Correlation_TARGET_AMT, decreasing = TRUE)
kable(data.frame(Correlation_TARGET_AMT), caption = "Correlation between TARGET_AMT and predictor variables")

```
\
\

The above table suggests that none of the variables seem to have a very strong correlation with TARGET_AMT. 
\
\
However, CAR_TYPE_Panel.Truck, JOB_Professional, MSTATUS_No, CAR_TYPE_Van, SEX_M, REVOKED_No, CAR_USE_Commercial, JOB_Blue.Collar, MVR_PTS, INCOME, YOJ, PARENT1_Yes, EDUCATION_Bachelors, AGE, EDUCATION_PhD, RED_CAR_yes, URBANICITY_Highly.Urban..Urban, JOB_Lawyer have a positive correlation. 
\
\
Similarly, CAR_TYPE_Minivan, URBANICITY_Highly.Rural..Rural, EDUCATION_Masters, TRAVTIME, CAR_TYPE_Sports.Car, HOMEKIDS, TIF, JOB_Doctor, KIDSDRIV, RED_CAR_no, EDUCATION_High.School, PARENT1_No, BLUEBOOK, HOME_VAL, CAR_TYPE_Pickup, JOB_Home.Maker, CAR_AGE, JOB_Clerical, JOB_Manager, OLDCLAIM, CAR_TYPE_SUV, CLM_FREQ, JOB_Student, CAR_USE_Private, REVOKED_Yes, SEX_F, MSTATUS_Yes have a negative correlation. 
\
\

Lets now see how values in some of the variable affects the correlation:

\

CAR_TYPE - If you drive Vans or Panel Trucks your cost of repair seems to increase as against Minivan, Pickup, Sports.Car, SUV. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation.
\
\
EDUCATION - If you have only a high school education then your cost of repair is less compared to a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation.
\
\
JOB - If you are a Lawyer, Professional or in a Blue Collar job, you spend more on repairs as compared to a Doctor, Manager, Home Maker, Student, or Clerical job.  Again binning this variable will strengthen the correlation.
\
\
We will carry out the above transformations in the Data Preparation phase.
\
\


###3.1.2 Missing Values\


\
\
To reiterate, since we started with a "clean" dataset, the missing values have already been handled. Lets take a quick look to confirm. 
\
\
```{r, echo = FALSE, warning=FALSE, message=FALSE}
missings<- sapply(insure_train_crash,function(x) sum(is.na(x)))
kable(data.frame(missings), caption = "Missing Values")

```
\
\
For the record, we lost about 17.6% of the data for the TARGET_AMT dataset when we started with the "clean" dataset.
\
\



###3.1.3 Outliers identification\ 



In this sub-section, we will look at the boxplots and determine the outliers in variables and decide on whether to act on the outliers.\
\
We will do the outliers only on the numeric variables.\
\
Below are the plots:
\
\
```{r, echo = FALSE, warning=FALSE, message=FALSE}

#
mdata<- select(insure_train_crash, KIDSDRIV, AGE, BLUEBOOK, CAR_AGE, CLM_FREQ, HOME_VAL, HOMEKIDS, INCOME, MVR_PTS, OLDCLAIM, TIF, TRAVTIME, YOJ)
mdata2 <- melt(mdata)
# Output the boxplot
p <- ggplot(data = mdata2, aes(x=variable, y=value)) + 
  geom_boxplot() + ggtitle("Outliers identification")
p + facet_wrap( ~ variable, scales="free", ncol=5)

```

From the "Outliers identification" plot above, we see that we have few outliers that we need to treat. \
\
We see that: KIDSDRIV, AGE, CAR_AGE, MVR_PTS, TIF, TRAVTIME, YOJ  need to be treated when we do the data preparation for modeling the TARGET_AMT. 
\
\
\



##3.2 Data Preparation\ 


\
Now that we have completed the data exploration / analysis, we will be transforming the data for use in analysis and modeling. \
\
\
We will be following the below steps as guidelines: \
- Outliers treatment \
- Missing values treatment \
- Adding New Variables \
\



###3.2.1 Outliers treatment\



In this sub-section, we will check different transformations for each of the variables - KIDSDRIV, AGE, CAR_AGE, MVR_PTS, TIF, TRAVTIME and YOJ - and create the appropriate outlier-handled / transformed variables.  
\
\
```{r, echo = FALSE, warning=FALSE, message=FALSE}
show_charts <- function(x, varlab, ...) {
    xlabel <- varlab
    xlab_log <- paste0(xlabel, '_log')
    xlab_sqrt <- paste0(xlabel, '_sqrt')
    xlab_sin <- paste0(xlabel, '_sin')
    xlab_inv <- paste0(xlabel, '_inv')
    
    mdata <- cbind(x, log(x), sqrt(x), sin(x), 1/x)
    colnames(mdata) <- c(xlabel, xlab_log, xlab_sqrt, xlab_sin, xlab_inv)
    mdata2 <- melt(mdata)
    mdata2 <- mdata2[, c(2:3)]
    names(mdata2) <- c("variable", "value")
    
    # Output the boxplot
    p <- ggplot(data = mdata2, aes(x=variable, y=value)) + geom_boxplot() + ggtitle("Outliers identification")
    p + facet_wrap( ~ variable, scales="free", ncol=5)
}

```
\
\


**Transformations for KIDSDRV**


```{r, echo = FALSE, warning=FALSE, message=FALSE}

#KIDSDRIV, AGE, CAR_AGE, MVR_PTS, TIF, TRAVTIME and YOJ

show_charts(insure_train_crash$KIDSDRIV, 'KIDSDRV')

insure_train_crash$KIDSDRIV_log <- log(insure_train_crash$KIDSDRIV)
insure_train_crash$KIDSDRIV_inv <- 1 / insure_train_crash$KIDSDRIV
```
\
\
From the above charts we can see that a log or an inverse transformation works well for KIDSDRV. Hence, We create both these variables.
\
\



**Transformations for AGE**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
#KIDSDRIV, AGE, CAR_AGE, MVR_PTS, TIF, TRAVTIME and YOJ
show_charts(insure_train_crash$AGE, 'AGE')
insure_train_crash$AGE_sin <- sin(insure_train_crash$AGE)
```
\
\
From the above charts we can see that a sin transformation works well for AGE. We will create this variable.
\
\



**Transformations for HOMEKIDS**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
#KIDSDRIV, AGE, CAR_AGE, MVR_PTS, TIF, TRAVTIME and YOJ
show_charts(insure_train_crash$CAR_AGE, 'CAR_AGE')
insure_train_crash$CAR_AGE_log <- log(insure_train_crash$CAR_AGE)
insure_train_crash$CAR_AGE_inv <- 1/(insure_train_crash$CAR_AGE)
```
\
\
From the above charts we can see that all the transformations address the outliers. However, the log and inverse transformation seem to do a better job of maintaining the central tendancy. So We just use the log and inverse transformation for CAR_AGE. 
\
\


**Transformations for MVR_PTS**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
show_charts(insure_train_crash$MVR_PTS, 'MVR_PTS')

insure_train_crash$MVR_PTS_log <- log(insure_train_crash$MVR_PTS)
insure_train_crash$MVR_PTS_sqrt <- sqrt(insure_train_crash$MVR_PTS)
insure_train_crash$MVR_PTS_sin <- sin(insure_train_crash$MVR_PTS)
```
\
\
From the above charts we can see that a log, sqrt or a sin transformation works better for MVR_PTS. Hence, We will create these variables.
\
\



**Transformations for TIF**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
show_charts(insure_train_crash$TIF, 'TIF')

insure_train_crash$TIF_log <- log(insure_train_crash$TIF)
insure_train_crash$TIF_sqrt <- sqrt(insure_train_crash$TIF)
insure_train_crash$TIF_sin <- sin(insure_train_crash$TIF)
insure_train_crash$TIF_inv <- 1 / insure_train_crash$TIF
```
\
\

From the above charts we can see that a log, sqrt, sin or an inverse transformation works well for TIF. Hence, We will create these variables.
\
\



**Transformations for TRAVTIME**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
#TRAVTIME and YOJ

show_charts(insure_train_crash$TRAVTIME, 'TRAVTIME')

insure_train_crash$TRAVTIME_sqrt <- sqrt(insure_train_crash$TRAVTIME)
insure_train_crash$TRAVTIME_sin <- sin(insure_train_crash$TRAVTIME)

```
\
\
From the above charts we can see that a sqrt and sin transformation works well for TRAVTIME. Hence, We will create these variables.
\
\



**Transformations for YOJ**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
show_charts(insure_train_crash$YOJ, 'YOJ')
insure_train_crash$YOJ_sin <- sin(insure_train_crash$YOJ)
```
\
\
From the above charts we can see that a sin transformation works well for YOJ. Hence, We will create this variable.
\
\



###3.2.2 Adding New Variables\


\
In this section, we generate some additional variables that we feel will help the correlations. The following were some of the observations we made during the data exploration phase for TARGET_AMT.
\

The following were some of the observations we made during the data exploration phase for TARGET_AMT

\
\

CAR_TYPE - If you drive Vans or Panel Trucks your cost of repair seems to increase as against Minivan, Pickup, Sports.Car, SUV. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation.

\
\
Accordingly, we will bin these variables as below:
\
CAR_TYPE_AMT_BIN : \
\
- 1 : if CAR_TYPE is Vans or Panel Trucks \
- 0 : if CAR_TYPE is Pickups, Sports, SUVs or Minivans
\
\

```{r}

insure_train_crash$CAR_TYPE_AMT_BIN <- ifelse(insure_train_crash$CAR_TYPE_Van | insure_train_crash$CAR_TYPE_Panel.Truck, 1, 0)

```

EDUCATION - If you have only a high school education then your cost of repair is less compared to a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation.
\
\

Accordingly, we will bin these variables as below:
\
EDUCATION_AMT_BIN : \
\
- 1 : if EDUCATION is High School \
- 0 : if EDUCATION is Bachelors, Masters or Phd
\
\


```{r}

insure_train_crash$EDUCATION_AMT_BIN <- ifelse(insure_train_crash$EDUCATION_High.School, 1, 0)

```

JOB - If you are a Lawyer, Professional or in a Blue Collar job, you spend more on repairs as compared to a Doctor, Manager, Home Maker, Student, or Clerical job.  Again binning this variable will strengthen the correlation.

\
\

Accordingly, we will bin these variables as below:
\
JOB_TYPE_AMT_BIN : \
\
- 1 : if JOB_TYPE is Lawyer, Professional or in a Blue Collar \
- 0 : if JOB_TYPE is Doctor, Manager, Home Maker, Student, or Clerical
\
\

```{r}

insure_train_crash$JOB_TYPE_AMT_BIN <- ifelse(insure_train_crash$JOB_Lawyer |  insure_train_crash$JOB_Professional | insure_train_crash$JOB_Blue.Collar, 1, 0)

```
\
\



##3.3 Build Models


Now that we have the dataset in a shape that can be modeled, we will go ahead and train the model for TARGET_AMT. We will train 2 models and select the best among these 2 models. The following will be the model specifications:
\
\

- Model1 - This will use all the variables available in the available "Train" dataset.\
\

- Model2 - We will do a backward stepwise on Model1 and save this as Model 2.\
\


We will then generate inferences from these models.\
\
\

###3.3.1 Prepare TRAIN and VALID datasets





###3.3.2 Model 1
####3.3.2.1 Overview
####3.3.2.2 Output
####3.3.2.3 Inference
###3.3.3 Model 2
####3.3.3.1 Overview
####3.3.3.2 Output
####3.3.3.3 Inference
##3.4 Select Models
###3.4.1 Model Evaluation Using TRAIN Data 
###3.4.2 Model Evaluation Using VALID Data
###3.4.3 Selection Summary
##3.5 Prediction Using Evaluation Data
###3.5.1 Tranformation of Evaluation Data 
###3.5.2 Independent Prediction Using Selected Model
###3.5.3 Dependent Prediction Using Selected Model
###3.5.4 Inference
#4 Conclusion
