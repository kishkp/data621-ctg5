---
title: "Home Work Assignment - 04"
author: "Critical Thinking Group 5"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

```{r, echo = FALSE, warning=FALSE, message=FALSE}
if (!require("ggplot2",character.only = TRUE)) (install.packages("ggplot2",dep=TRUE))
if (!require("MASS",character.only = TRUE)) (install.packages("MASS",dep=TRUE))
if (!require("knitr",character.only = TRUE)) (install.packages("knitr",dep=TRUE))
if (!require("xtable",character.only = TRUE)) (install.packages("xtable",dep=TRUE))
if (!require("dplyr",character.only = TRUE)) (install.packages("dplyr",dep=TRUE))
if (!require("psych",character.only = TRUE)) (install.packages("psych",dep=TRUE))
if (!require("stringr",character.only = TRUE)) (install.packages("stringr",dep=TRUE))
#if (!require("car",character.only = TRUE)) (install.packages("car",dep=TRUE))
if (!require("faraway",character.only = TRUE)) (install.packages("faraway",dep=TRUE))
if (!require("dummy",character.only = TRUE)) (install.packages("dummy",dep=TRUE))
if (!require("reshape2",character.only = TRUE)) (install.packages("reshape2",dep=TRUE))
if (!require("popbio",character.only = TRUE)) (install.packages("popbio",dep=TRUE))
if (!require("rpart",character.only = TRUE)) (install.packages("rpart",dep=TRUE))
if (!require("pROC",character.only = TRUE)) (install.packages("pROC",dep=TRUE))


library(ggplot2)
library(MASS)
library(knitr)
library(xtable)
library(dplyr)
library(psych)
library(stringr)
#library(car)
library(faraway)
library(dummy)
library(reshape2)
library(popbio)
library(rpart)
library(pROC)

```

newpage

#1 Overview 
The data set contains approximately 8161 records. Each record represents a customer profile at an auto insurance company. Each record has two response variables. 


The first response variable, TARGET_FLAG, is a 1 or a 0. A "1" means that the person was in a car crash. A zero means that the person was not in a car crash. 


The second response variable is TARGET_AMT. This is the amount spent on repairs if there was a crash. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.


We will be exploring, analyzing, and modeling the training data. Since there are 2 different predictions we have to work with, we will deal with each prediction independently. The following are the 2 predictions we will be modeling for:

1. TARGET_FLAG - This dependent variable tells whether there was a crash or not. This is a binary variable and as such we will be using a Logistic Regression Model to predict this.
2. TARGET_AMT - This dependent variable gives the amount / cost of repairs if there was a crash. This is a continuous variable and we will ve using a Linear Regression Model to predict this.


Each of the above models will be built and evaluated separately. In the first section of this document we will deal with the Logistic Model for TARGET_FLAG and in the second section we will deal with Linear Model for the TARGET_AMT


Out of the many models for each task, we will go ahead and shortlist one model that works the best. We will then use these models (one for each task) on the test / evaluation data.


To attain our objective, we will be follow the below steps for each modeling exercise:

1 -Data Exploration 
2 -Data Preparation 
3 -Build Models 
4 -Select Models 

**Model Selection Strategy:** As a strategy, we will split the train dataset into 2 parts - TRAIN and VALID. In the VALID dataset, we will hold out some values to validate how well the model is trained using the TRAIN dataset. We will then use the Model that performs the best on the EVALUATION data to give the required output. We will split the TRAIN / VALID data after the **Data Exploration / Preparation** before the **Build Models**.


**Please Note:** 


- There are some common clean-up and transformations that we will carry out initially that will serve all the models.

- While working on the Linear Models for the TARGET_AMT, we will be using only a subset of the data where the TARGET_FLAG = 1. This will give us all the records where there was a crash and subsequently a repair amount.

- While Predicting the TARGET_AMT with the given Evaluation dataset, We will take the output of the TARGET_FLAG predictions on the Evaluation dataset and use only those rows that were classified as a "Crash" and use it as the input to the TARGET_AMT prediction. So this is a two step prediction, one for the TARGET_FLAG and using the output to predict TARGET_AMT.




newpage



#2 Data Exploration and Cleanup / Common Transformations





In this section we go ahead and perform some common cleanup and create additional variables that will be used for modeling both the logistic as well as the linear regressions. We will explore and gain some insights into the dataset by pursuing the below high level steps and inquiries: 


- Variable Identification / Relationships
- Data Clean-up
- Common Transformations
- Create Missing Flags / Impute Missing Values


##2.1 Variable Identification


First let's display and examine the data dictionary or the data columns as shown in below table:

```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full <- read.csv("https://raw.githubusercontent.com/kishkp/data621-ctg5/master/HW4/insurance_training_data.csv")

kable(read.csv("https://raw.githubusercontent.com/kishkp/data621-ctg5/master/HW4/insurevars.csv"), caption = "Variable Description")

```

We notice that there are 2 dependent variables - TARGET_FLAG and TARGET_AMT. Apart from these 2 dependent variables, we have 23 independent or predictor variables.




##2.2 Data Cleanup


```{r, echo = FALSE, warning=FALSE, message=FALSE, results='asis'}
#str(insure_train_full)

levels(insure_train_full$MSTATUS)
levels(insure_train_full$SEX)
levels(insure_train_full$EDUCATION)
levels(insure_train_full$JOB)
levels(insure_train_full$CAR_TYPE)
levels(insure_train_full$URBANICITY)
levels(insure_train_full$REVOKED)

summary(insure_train_full)

```


From the output above we can make the following observations:

- some numeric variables like INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM have been converted to Factor variables. This needs to be set right.

- Some of the variables like MSTATUS, SEX, EDUCATION, JOB, CAR_TYPE, URBANICITY have some of the values encoded with "z_". Not that this will impact the analysis, but it will look a bit odd. So we will be fixing this.


- EDUCATION has 2 "High School" values - one starting with "<" and another starting with "z_". It is assumed that both these values are to be converted to "HIGH School".


- JOB has a "" value. This would indicate that the job is unknown or is not coded. Hence, we will replace this with "Unknown".


- There are records where CAR_AGE is negative or zero, which is improbable. Upon investigation, we find that there are 4 records that are affected. We will remove these records.



```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$INCOME <- as.numeric(str_replace_all(insure_train_full$INCOME, pattern =  "[$*,]", replacement = ""))
insure_train_full$HOME_VAL <- as.numeric(str_replace_all(insure_train_full$HOME_VAL, pattern =  "[$*,]", replacement = ""))
insure_train_full$BLUEBOOK <- as.numeric(str_replace_all(insure_train_full$BLUEBOOK, pattern =  "[$*,]", replacement = ""))
insure_train_full$OLDCLAIM <- as.numeric(str_replace_all(insure_train_full$OLDCLAIM, pattern =  "[$*,]", replacement = ""))

insure_train_full$MSTATUS <- as.factor(str_replace_all(insure_train_full$MSTATUS, "z_", ""))
insure_train_full$SEX <- as.factor(str_replace_all(insure_train_full$SEX, "z_", ""))
insure_train_full$EDUCATION <- as.factor(str_replace_all(insure_train_full$EDUCATION, "z_", ""))
insure_train_full$EDUCATION <- as.factor(str_replace_all(insure_train_full$EDUCATION, "<", ""))
insure_train_full$CAR_TYPE <- as.factor(str_replace_all(insure_train_full$CAR_TYPE, "z_", ""))
insure_train_full$URBANICITY <- as.factor(str_replace_all(insure_train_full$URBANICITY, "z_", ""))

insure_train_full$JOB <- as.character(insure_train_full$JOB)
insure_train_full$JOB[insure_train_full$JOB==""] <- "Unknown"
insure_train_full$JOB <- as.factor(str_replace_all(insure_train_full$JOB, "z_", ""))

insure_train_full <- insure_train_full[ -which( insure_train_full$CAR_AGE == -3 | insure_train_full$CAR_AGE == 0 ) , ]

```






##2.3 Common Transformations


- We will also create dummy variables for all the factors. 


- Please note that we will not be using INDEX variable as it serves as just an identifier for each row. And has no relationships to other variables. 


Making the above fixes to the data, we now have a "clean" dataset which can be explored further.



```{r}

# Create Dummy Variable for 2 factor variables 
insure_train_full$CAR_USE_Commercial <- ifelse(insure_train_full$CAR_USE=="Commercial", 1, 0)
insure_train_full$MSTATUS_Yes <- ifelse(insure_train_full$MSTATUS=="Yes", 1, 0)
insure_train_full$PARENT1_Yes <- ifelse(insure_train_full$PARENT1=="Yes", 1, 0)
insure_train_full$RED_CAR_yes <- ifelse(insure_train_full$RED_CAR=="yes", 1, 0)
insure_train_full$REVOKED_Yes <- ifelse(insure_train_full$REVOKED=="Yes", 1, 0)
insure_train_full$SEX_M <- ifelse(insure_train_full$SEX=="M", 1, 0)
insure_train_full$URBANICITY_Rural <- ifelse(insure_train_full$URBANICITY=="Highly Rural/ Rural", 1, 0)

# remove original variables
insure_train_full <- select(insure_train_full, -CAR_USE, -MSTATUS, -PARENT1, -RED_CAR, -REVOKED, -SEX, -URBANICITY)

insure_without_dummy <- insure_train_full

#- We will also create dummy variables for all the factors and drop the original variables. 
dummy_vars<-as.data.frame(sapply(dummy(insure_train_full), FUN = as.numeric))
dummy_vars <- dummy_vars-1

# remove original variables
insure_train_full <- select(insure_train_full, -EDUCATION, -JOB, -CAR_TYPE)

insure_train_full <- cbind(insure_train_full, dummy_vars)
insure_train_full <- select(insure_train_full, -INDEX)

```






##2.4 Create Missing Flags / Impute Missing Values






Based on the missing data from the below table, we can see that there are a few missing values for AGE, YOJ, INCOME, HOME_VAL, CAR_AGE variables.  We will create flags to indicate that there are missing values in some of the variables. 



```{r, echo = FALSE, warning=FALSE, message=FALSE}
missings<- sapply(insure_train_full,function(x) sum(is.na(x)))
kable(data.frame(missings), caption = "Missing Values")

```




We now impute values to AGE, YOJ, INCOME, HOME_VAL, CAR_AGE. However, while doing the impute, we will impute to a new variable so as not to impact the original variables. We will look at the distributions for each of the variable to determine the value to use to impute. Given that Age and YOJ look to be somewhat normally distributed, we can go ahead and use the mean to impute the missing values for these variables. For INCOME, HOME_VAL and CAR_AGE the median seems to be a better value to impute since there are strong right skews. We will carry out these transformation while data preparation.




```{r, echo = FALSE, warning=FALSE, message=FALSE}

par(mfrow=c(2,3))
hist(insure_train_full$AGE)
hist(insure_train_full$YOJ)
hist(insure_train_full$INCOME)
hist(insure_train_full$HOME_VAL)
hist(insure_train_full$CAR_AGE)
# Missing Flags

insure_train_full$YOJ_MISS  <- ifelse(is.na(insure_train_full$YOJ), 1, 0)
insure_train_full$INCOME_MISS  <- ifelse(is.na(insure_train_full$INCOME), 1, 0)
insure_train_full$HOME_VAL_MISS  <- ifelse(is.na(insure_train_full$HOME_VAL), 1, 0)
insure_train_full$CAR_AGE_MISS  <- ifelse(is.na(insure_train_full$CAR_AGE), 1, 0)

# Missing Impute

# insure_train_full$AGE_IMPUTE <- insure_train_full$AGE
# insure_train_full$AGE_IMPUTE[is.na(insure_train_full$AGE_IMPUTE)] <- mean(insure_train_full$AGE_IMPUTE, na.rm = T) 
# 
# insure_train_full$YOJ_IMPUTE <- insure_train_full$YOJ
# insure_train_full$YOJ_IMPUTE[is.na(insure_train_full$YOJ_IMPUTE)] <- mean(insure_train_full$YOJ_IMPUTE, na.rm = T) 
# 
# insure_train_full$INCOME_IMPUTE <- insure_train_full$INCOME
# insure_train_full$INCOME_IMPUTE[is.na(insure_train_full$INCOME_IMPUTE)] <- median(insure_train_full$INCOME_IMPUTE, na.rm = T) 
# 
# insure_train_full$HOME_VAL_IMPUTE <- insure_train_full$HOME_VAL
# insure_train_full$HOME_VAL_IMPUTE[is.na(insure_train_full$HOME_VAL_IMPUTE)] <- median(insure_train_full$HOME_VAL_IMPUTE, na.rm = T) 
# 
# insure_train_full$CAR_AGE_IMPUTE <- insure_train_full$CAR_AGE
# insure_train_full$CAR_AGE_IMPUTE[is.na(insure_train_full$CAR_AGE_IMPUTE)] <- median(insure_train_full$CAR_AGE_IMPUTE, na.rm = T) 

# Direct Impute

insure_train_full$AGE[is.na(insure_train_full$AGE)] <- mean(insure_train_full$AGE, na.rm = T) 
insure_train_full$YOJ[is.na(insure_train_full$YOJ)] <- mean(insure_train_full$YOJ, na.rm = T) 
insure_train_full$INCOME[is.na(insure_train_full$INCOME)] <- median(insure_train_full$INCOME, na.rm = T) 
insure_train_full$HOME_VAL[is.na(insure_train_full$HOME_VAL)] <- median(insure_train_full$HOME_VAL, na.rm = T) 
insure_train_full$CAR_AGE[is.na(insure_train_full$CAR_AGE)] <- median(insure_train_full$CAR_AGE, na.rm = T) 

# Save point for Original data set with dummies created
insure_orig <- insure_train_full

```



Now that we are done with the common clean-up and transformations, we can proceed to each specific model as below.


newpage

#3 Logistic Regression for TARGET_FLAG

In this section we will use Logistic regression to model the TARGET_FLAG. We will first start with the Data Exploration.





##3.1 Data Summary and Correlation Analysis



###3.1.1 Data Summary



In this section, we will create summary data to better understand the relationship each of the variables have with our dependent variables using correlation, central tendency, and dispersion as shown below:  



```{r, echo = FALSE, warning=FALSE, message=FALSE, results='hide'}

ds_stats <- psych::describe(insure_train_full, skew = TRUE, na.rm = TRUE)
#ds_stats
kable(ds_stats[1:7], caption= "Data Summary")
kable(ds_stats[8:13], caption= "Data Summary (Cont)")

fun1 <- function(a, y) cor(y, a , use = 'na.or.complete')
Correlation_TARGET_FLAG <- sapply(insure_train_full, FUN = fun1, y=insure_train_full$TARGET_FLAG) 
```




###3.1.2 Correlations



Now we will produce the correlation table between the independent variables and the dependent variable - TARGET_FLAG  



```{r, echo = FALSE, warning=FALSE, message=FALSE}
Correlation_TARGET_FLAG <- sort(Correlation_TARGET_FLAG, decreasing = TRUE)
kable(data.frame(Correlation_TARGET_FLAG), caption = "Correlation between TARGET_FLAG and predictor variables")

```



The above table suggests that none of the variables seem to have a very strong correlation with TARGET_FLAG. However, CAR_TYPE_Van, RED_CAR_no, JOB_Home.Maker, SEX_F, JOB_Clerical, CAR_TYPE_SUV, TRAVTIME, CAR_TYPE_Pickup, CAR_TYPE_Sports.Car, JOB_Student, JOB_Blue.Collar, KIDSDRIV, HOMEKIDS, MSTATUS_No, OLDCLAIM, EDUCATION_High.School, CAR_USE_Commercial, REVOKED_Yes, PARENT1_Yes, CLM_FREQ, MVR_PTS, URBANICITY_Highly.Urban..Urban have a positive correlation. 


Similarly, URBANICITY_Highly.Rural..Rural, HOME_VAL, PARENT1_No, REVOKED_No, CAR_USE_Private, INCOME, CAR_TYPE_Minivan, MSTATUS_Yes, JOB_Manager, BLUEBOOK, AGE, CAR_AGE, TIF, EDUCATION_Masters, YOJ, EDUCATION_PhD, JOB_Lawyer, JOB_Doctor, EDUCATION_Bachelors, JOB_Professional, SEX_M, RED_CAR_yes, CAR_TYPE_Panel.Truck have a negative correlation. 


Lets now see how values in some of the variable affects the correlation:


CAR_TYPE - If you drive Minivans and Panel Trucks you have lesser chance of being in a crash as against Pickups, Sports, SUVs and Vans. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation.


EDUCATION - If you have only a high school education then you are more likely to crash than if you have a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation.


JOB - If you are a Student, Homemaker, or in a Blue Collar or Clerical job, you are more likely to be in a crash against Doctor, Lawyer, Manager, professional or Unknown job.  Again binning this variable will strengthen the correlation.




###3.1.3 Binning of Variables





Lets have a look at the following numeric variables to see how they are distributed vis-a-vis TARGET_FLAG: INCOME, YOJ, HOME_VAL, OLDCLAIM, CLM_FREQ, MVR_PTS, CAR_AGE, AGE, BLUEBOOK, TIF, TRAVTIME. The goal here is to see if we can bin these variables into zero and non-zero bin values and check the correlations. While doing that we will also see how the variables are distributed vis-a-vis TARGET_FLAG.




```{r}

show_hist <- function(var) {
    
    col_x <- which(colnames(insure_train_full)==var)
    h0 <- select(insure_train_full[insure_train_full$TARGET_FLAG==1,], col_x)
    h1 <- select(insure_train_full[insure_train_full$TARGET_FLAG==0,], col_x)

    min_x <- min(select(insure_train_full, col_x), na.rm = TRUE) 
    max_x <- max(select(insure_train_full, col_x), na.rm = TRUE) 
    by_x <- (max_x - min_x) / 20
    
        
    hist(h0[,1], breaks = 20, col=rgb(1,0,0,0.5), main="Overlapping Histogram", xlab = var, xaxt = "n") 
    axis(1, at = seq(min_x, max_x, by = by_x), las=2)

    hist(h1[,1], breaks = 20, col=rgb(0,0,1,0.5), add=T) # 
#    axis(1, at = seq(min_x, max_x, by = by_x), las=2)

    box()
}

check_bins <- function(var, thresholds) {

    col_x <- which(colnames(insure_train_full)==var)
    old_x <- select(insure_train_full, col_x)
    cor_old <- cor(old_x, insure_train_full$TARGET_FLAG,use = 'na.or.complete')
    ds <- data.frame("Item" = "Original", "Correlation"= round(cor_old, 5))


    
    for(i in 1:length(thresholds)) {
        New_x <- ifelse(select(insure_train_full, col_x)<=thresholds[i],0,1)
        cor_new <- cor(New_x, insure_train_full$TARGET_FLAG,use = 'na.or.complete')
        ds_1 <- data.frame("Item" = as.character(thresholds[i]), "Correlation"= round(cor_new, 5))
        ds <- rbind(ds, ds_1)
    }
    return (ds)
}

show_hist("INCOME")
check_bins("INCOME", c(0, 20000, 90000, 130000))

show_hist("YOJ")
check_bins("YOJ", c(0, 4, 8, 15))

show_hist("HOME_VAL")
check_bins("HOME_VAL", c(0, 20000, 90000, 130000))

show_hist("OLDCLAIM")
check_bins("OLDCLAIM", c(0, 5000, 10000, 15000, 20000, 40000))

show_hist("CLM_FREQ")
check_bins("CLM_FREQ", c(0, 1, 2, 3, 4))

table(insure_train_full$MVR_PTS)
show_hist("MVR_PTS")
check_bins("MVR_PTS", c(0:12))

#table(insure_train_full$CAR_AGE)
show_hist("CAR_AGE")
#check_bins("CAR_AGE", c(1:27))

#table(insure_train_full$AGE)
show_hist("AGE")
#check_bins("AGE", c(16:80))

#table(insure_train_full$BLUEBOOK)
show_hist("BLUEBOOK")
#check_bins("BLUEBOOK", c(11000, 41000, 41050, 57500, 58000))

table(insure_train_full$TIF)
show_hist("TIF")
check_bins("TIF", c(1, 4, 6, 10, 24))

table(insure_train_full$TRAVTIME)
show_hist("TRAVTIME")
check_bins("TRAVTIME", c(21, 59, 120))



```




From the outputs above, we can come to the following conclusions:



- INCOME - Binning this variable seems to make a difference in the correlation. We will go ahead and create a binned variable for this at zero value.

- YOJ - Binning this variable seems to make a difference in the correlation. We will go ahead and create a binned variable for this.

- HOME_VAL - Binning this variable seems to make a difference in the correlation. We will go ahead and create a binned variable for this.

- OLDCLAIM- There is a huge difference in the coorrelation when we transform this vatiable. Binning this variable seems like a good idea.

- CLM_FREQ - Binning this variable seems to make a difference in the correlation. We will go ahead and create a binned variable for this.

- MVR_PTS - Binning this variable seems to make a difference in the correlation. We will go ahead and create a binned variable for this.

- CAR_AGE - There are quite a few records with a 1 year car age. We will use this bound to generate a binned variable as well as retain the original varible as is. 

- AGE - There is no specific pattern that emerges. We will retain this variable as is.

-BLUEBOOK - There is no specific pattern that emerges. We will retain the variable as is.

- TIF - Looking at the plots, values and the correlations with TARGET_FLAG, we can conclude that this is not a good variable for binning. We will retain this variable as is.

- TRAVTIME - from the plot, we can see that there is a clear pattern around the value - 20. We will go ahead and create a binned variable for this.





We will carry out the above transformations in the Data Preparation phase.





###3.1.4 Outliers identification 



In this sub-section, we will look at the boxplots and determine the outliers in variables and decide on whether to act on the outliers. We will do the outliers only on some of the currency and few other variables. Below are the plots:

```{r, echo = FALSE, warning=FALSE, message=FALSE}

#
mdata<- select(insure_train_full, AGE, BLUEBOOK, TIF)
mdata2 <- melt(mdata)
# Output the boxplot
p <- ggplot(data = mdata2, aes(x=variable, y=value)) + 
  geom_boxplot() + ggtitle("Outliers Identification")
p + facet_wrap( ~ variable, scales="free", ncol=5)

```

From the "Outliers identification" plot above, we see that we have few outliers that we need to treat. We will treat the outliers in this variable when we do the data preparation for modeling the TARGET_FLAG. 





###3.1.5 Analysis of the link function 



In this section, we will investigate how our initial data aligns with a typical logistic model plot. 

Recall the Logistic Regression is part of a larger class of algorithms known as Generalized Linear Model (glm).  The fundamental equation of generalized linear model is:

$g(E(y)) = a+ Bx_1+B_2x_2+ B_3x_3+...$   

where, g() is the link function, E(y) is the expectation of target variable and $B_0 + B_1x_1 + B_2x_2+B_3x_3$ is the linear predictor ( $B_0,B_1,B_2, B_3$ to be predicted). The role of link function is to 'link' the expectation of y to linear predictor.

In logistic regression, we are only concerned about the probability of outcome dependent variable ( success or failure). As described above, g() is the link function. This function is established using two things: Probability of Success (p) and Probability of Failure (1-p).  p should meet following criteria:
It must always be positive (since p >= 0)
It must always be less than equals to 1 (since p <= 1).

Now let's investigate how our initial data model aligns with the above criteria. In other words, we will plot regression model plots for each variable and compare it to a typical logistic model plot:


```{r, echo = FALSE, warning=FALSE, message=FALSE}
par(mfrow=c(2,3))


x <- select(insure_train_full, -TARGET_AMT)
x <- x[complete.cases(x),]

# sapply(x, FUN = show_chart_logi.hist, y=x$TARGET_FLAG) 

logi.hist.plot(x$REVOKED_Yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'REVOKED_Yes')
logi.hist.plot(x$CAR_USE_Commercial,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_USE_Commercial')
logi.hist.plot(x$CAR_TYPE_SUV,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_SUV')
logi.hist.plot(x$PARENT1_Yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'PARENT1_Yes')
logi.hist.plot(x$KIDSDRIV,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'KIDSDRIV')
logi.hist.plot(x$CAR_AGE,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_AGE')
logi.hist.plot(x$JOB_Clerical,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Clerical')
logi.hist.plot(x$HOMEKIDS,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'HOMEKIDS')
logi.hist.plot(x$JOB_Doctor,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Doctor')
logi.hist.plot(x$CLM_FREQ,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CLM_FREQ')
logi.hist.plot(x$SEX_M,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'SEX_F')
logi.hist.plot(x$MVR_PTS,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'MVR_PTS')
logi.hist.plot(x$EDUCATION_Masters,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_Masters')
logi.hist.plot(x$CAR_TYPE_Van,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Van')
logi.hist.plot(x$CAR_TYPE_Minivan,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Minivan')
logi.hist.plot(x$YOJ,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'YOJ')
logi.hist.plot(x$TIF,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'TIF')
logi.hist.plot(x$MSTATUS_Yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'MSTATUS_Yes')
logi.hist.plot(x$RED_CAR_yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'RED_CAR_no')
logi.hist.plot(x$JOB_Lawyer,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Lawyer')
logi.hist.plot(x$CAR_TYPE_Pickup,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Pickup')
logi.hist.plot(x$JOB_Student,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Student')
logi.hist.plot(x$OLDCLAIM,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'OLDCLAIM')
logi.hist.plot(x$INCOME,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'INCOME')
logi.hist.plot(x$EDUCATION_Bachelors,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_Bachelors')
logi.hist.plot(x$JOB_Manager,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Manager')
logi.hist.plot(x$EDUCATION_High.School,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_High.School')
logi.hist.plot(x$JOB_Home.Maker,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Home.Maker')
logi.hist.plot(x$EDUCATION_PhD,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'EDUCATION_PhD')
logi.hist.plot(x$TRAVTIME,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'TRAVTIME')
logi.hist.plot(x$JOB_Professional,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Professional')
logi.hist.plot(x$URBANICITY_Rural,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'URBANICITY_Highly.Urban..Urban')
logi.hist.plot(x$JOB_Blue.Collar,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'JOB_Blue.Collar')
logi.hist.plot(x$AGE,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'AGE')
logi.hist.plot(x$CAR_TYPE_Sports.Car,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Sports.Car')
logi.hist.plot(x$HOME_VAL,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'HOME_VAL')
logi.hist.plot(x$BLUEBOOK,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'BLUEBOOK')
logi.hist.plot(x$PARENT1_Yes,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'PARENT1_No')
logi.hist.plot(x$CAR_TYPE_Panel.Truck,x$TARGET_FLAG,logi.mod = 1, type='hist', boxp=FALSE,col='gray', mainlabel = 'CAR_TYPE_Panel.Truck')

```





####3.1.5.1 Interpretation 




You can see that the probability of crashing increases as we get closer to the "1" classification for the CAR_TYPE_Van, RED_CAR_no, JOB_Home.Maker, SEX_F, JOB_Clerical, CAR_TYPE_SUV, TRAVTIME, BLUEBOOK, CAR_TYPE_Pickup, CAR_TYPE_Sports.Car, JOB_Student, KIDSDRIV, JOB_Blue.Collar, HOMEKIDS, MSTATUS_No, EDUCATION_High.School, CAR_USE_Commercial, REVOKED_Yes, PARENT1_Yes, OLDCLAIM, CLM_FREQ, MVR_PTS, URBANICITY_Highly.Urban..Urban variables.



You can see that the probability of crashing decreases as we get closer to the "1" classification for the HOME_VAL, CAR_TYPE_Minivan, MSTATUS_Yes, JOB_Manager, AGE, CAR_AGE, TIF, EDUCATION_Masters, YOJ, EDUCATION_PhD, JOB_Lawyer, JOB_Doctor, EDUCATION_Bachelors, JOB_Professional, INCOME, SEX_M, RED_CAR_yes, CAR_TYPE_Panel.Truck variables.  








##3.2 Data Preparation 



Now that we have completed the data exploration / analysis, we will be transforming the data for use in analysis and modeling. 


We will be following the below steps as guidelines: 
- Outliers treatment 
- Adding New Variables 




###3.2.1 Outliers treatment



In this sub-section, we will check different transformations for AGE, BLUEBOOK and TIF to create the appropriate outlier-handled / transformed variables.  


```{r, echo = FALSE, warning=FALSE, message=FALSE}
show_charts <- function(x, varlab, ...) {
    xlabel <- varlab
    xlab_log <- paste0(xlabel, '_log')
    xlab_sqrt <- paste0(xlabel, '_sqrt')
    xlab_sin <- paste0(xlabel, '_sin')
    xlab_inv <- paste0(xlabel, '_inv')
    
    mdata <- cbind(x, log(x), sqrt(x), sin(x), 1/x)
    colnames(mdata) <- c(xlabel, xlab_log, xlab_sqrt, xlab_sin, xlab_inv)
    mdata2 <- melt(mdata)
    mdata2 <- mdata2[, c(2:3)]
    names(mdata2) <- c("variable", "value")
    
    # Output the boxplot
    p <- ggplot(data = mdata2, aes(x=variable, y=value)) + geom_boxplot() + ggtitle("Outliers identification")
    p + facet_wrap( ~ variable, scales="free", ncol=5)
}

```






**Transformations for TIF**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
summary(insure_train_full$TIF)
show_charts(insure_train_full$TIF, 'TIF')
insure_train_full$TIF_sin <- sin(insure_train_full$TIF)
```



From the above charts we can see that a log, sqrt, sin or an inverse transformation works well for TIF. However, the sin transformation seems to be better distributed. Hence, We will create this variable.



**Transformations for BLUEBOOK**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
summary(insure_train_full$BLUEBOOK)
show_charts(insure_train_full$BLUEBOOK, 'BLUEBOOK')
insure_train_full$BLUEBOOK_sin <- sin(insure_train_full$BLUEBOOK)
```



From the above charts we can see that a sin transformation works well. Hence, We will create this variable.



**Transformations for AGE**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
summary(insure_train_full$AGE)
show_charts(insure_train_full$AGE, 'AGE')
insure_train_full$AGE_sin <- sin(insure_train_full$AGE)
```



From the above charts we can see that a sin works well for AGE.Hence, We will create this variable.






###3.2.2 Adding New Variables




In this section, we generate some additional variables that we feel will help the correlations. The following were some of the observations we made during the data exploration phase for TARGET_FLAG


CAR_TYPE - If you drive Minivans and Panel Trucks you have lesser chance of being in a crash as against Pickups, Sports, SUVs and Vans. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation. Accordingly, we will bin this variable as below:


CAR_TYPE_FLAG_BIN : 

- 1 : if CAR_TYPE is Minivans or Panel Trucks 
- 0 : if CAR_TYPE is Pickups, Sports, SUVs or Vans



```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$CAR_TYPE_FLAG_BIN <- ifelse(insure_train_full$CAR_TYPE_Minivan | insure_train_full$CAR_TYPE_Panel.Truck, 1, 0)

```


EDUCATION - If you have only a high school education then you are more likely to crash than if you have a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation:


EDUCATION_FLAG_BIN : 

- 0 : if EDUCATION is High School 
- 1 : if EDUCATION is Bachelors, Masters or Phd


```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$EDUCATION_FLAG_BIN <- ifelse(insure_train_full$EDUCATION_High.School, 0, 1)

```


JOB - If you are a Student, Homemaker, or in a Blue Collar or Clerical job, you are more likely to be in a crash against Doctor, Lawyer, Manager or professional.  Again binning this variable will strengthen the correlation:

JOB_TYPE_FLAG_BIN : 

- 1 : if JOB_TYPE is Student, Homemaker, or in a Blue Collar or Clerical 
- 0 : if JOB_TYPE is Doctor, Lawyer, Manager, professional, Unknown


```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$JOB_TYPE_FLAG_BIN <- ifelse(insure_train_full$JOB_Student |  insure_train_full$JOB_Home.Maker | insure_train_full$JOB_Clerical | insure_train_full$JOB_Blue.Collar, 1, 0)

```



- INCOME - Binning this variable seems to make a difference in the correlation. We will go ahead and create a binned variable for this at zero value.



INCOME_FLAG_BIN : 

- 1 : if INCOME <= 0 
- 0 : if INCOME > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$INCOME_FLAG_BIN <- ifelse(insure_train_full$INCOME <=0, 1, 0)

```



- YOJ - Binning this variable seems to make a difference in the correlation. We will go ahead and create a binned variable for this.


YOJ_FLAG_BIN : 

- 1 : if YOJ <= 0 
- 0 : if YOJ > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$YOJ_FLAG_BIN <- ifelse(insure_train_full$YOJ <=0, 1, 0)

```




- HOME_VAL - Binning this variable seems to make a difference in the correlation. We will go ahead and create a binned variable for this.


HOME_VAL_FLAG_BIN : 

- 1 : if HOME_VAL <= 0 
- 0 : if HOME_VAL > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$HOME_VAL_FLAG_BIN <- ifelse(insure_train_full$HOME_VAL <=0, 1, 0)

```



- OLDCLAIM- There is a huge difference in the coorrelation when we transform this vatiable. Binning this variable seems like a good idea.


OLDCLAIM_FLAG_BIN : 

- 1 : if OLDCLAIM <= 0 
- 0 : if OLDCLAIM > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}
insure_train_full$OLDCLAIM_FLAG_BIN <- ifelse(insure_train_full$OLDCLAIM <=0, 1, 0)
```



- CLM_FREQ - Binning this variable seems to make a difference in the correlation. We will go ahead and create a binned variable for this.


CLM_FREQ_FLAG_BIN : 

- 1 : if CLM_FREQ <= 0 
- 0 : if CLM_FREQ > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}
insure_train_full$CLM_FREQ_FLAG_BIN <- ifelse(insure_train_full$CLM_FREQ <=0, 1, 0)
```



- MVR_PTS - Binning this variable seems to make a difference in the correlation. We will go ahead and create a binned variable for this.


MVR_PTS_FLAG_BIN : 

- 1 : if MVR_PTS <= 0 
- 0 : if MVR_PTS > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}
insure_train_full$MVR_PTS_FLAG_BIN <- ifelse(insure_train_full$MVR_PTS <=0, 1, 0)
```



- CAR_AGE - There are quite a few records with a 1 year car age. We will use this bound to generate a binned variable as well as retain the original varible as is. 


CAR_AGE_FLAG_BIN : 

- 1 : if CAR_AGE <= 1 
- 0 : if CAR_AGE > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$CAR_AGE_FLAG_BIN <- ifelse(insure_train_full$CAR_AGE <=1, 1, 0)

```



- AGE - There is no specific pattern that emerges. We will retain this variable as is.



- BLUEBOOK - There is no specific pattern that emerges. We will retain this variable as is.




- TRAVTIME - from the plot, we can see that there is a clear pattern around the value - 20. We will go ahead and create a binned variable for this.


TRAVTIME_FLAG_BIN : 

- 1 : if TRAVTIME <= 20 
- 0 : if TRAVTIME > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_full$TRAVTIME_FLAG_BIN <- ifelse(insure_train_full$TRAVTIME <=20, 1, 0)


```




###3.2.3 Additional Binned Variables


After having prepared the data, we will go ahead and drop some of the variables.

```{r}

#write.csv(insure_train_full, file = "D:/CUNY/Courses/Business Analytics and Data Mining/Assignments/data621-ctg5/HW4/final.csv")

#DS_TARGET_FLAG <- insure_train_full
DS_TARGET_FLAG <- select(insure_train_full, -TARGET_AMT, -JOB_Blue.Collar, -JOB_Clerical, -JOB_Doctor, -JOB_Home.Maker, -JOB_Lawyer, -JOB_Manager, -JOB_Professional, -JOB_Student, -JOB_Unknown, -CAR_TYPE_Minivan, -CAR_TYPE_Panel.Truck, -CAR_TYPE_Pickup, -CAR_TYPE_Sports.Car, -CAR_TYPE_SUV, -CAR_TYPE_Van, -EDUCATION_Bachelors, -EDUCATION_High.School, -EDUCATION_Masters, -EDUCATION_PhD) 
                         
# New Additional Variables.
#-AGE, -AGE_IMPUTE, -BLUEBOOK,-CAR_AGE_IMPUTE, -CAR_AGE_MISS, -CLM_FREQ, -HOME_VAL, -HOME_VAL_IMPUTE, -HOME_VAL_MISS, -INCOME, -INCOME_IMPUTE, -INCOME_MISS, -JOB, -MVR_PTS, -OLDCLAIM, , -TIF, -TRAVTIME, -YOJ, -YOJ_IMPUTE,-YOJ_MISS

str(DS_TARGET_FLAG)

```






##3.3 Build Models



In this section, we will create 3 models. Aside from using original and transformed data, we will also using different methods and functions such as Linear Discriminant Analysis, step function, and logit function to enhance our models. 
newline
newline
Below is our model definition: 
-Model 1- This model will be created using all the variables in train data set with logit function GLM. 
-Model 2: This model step function will be used to enhance the model 1. 
-Model 3- This model will be created using calssification and regression tree. 





###3.3.1 Prepare TRAIN and VALID datasets


However, prior to that, we hold out a subset of data as a validation dataset to check model performance. This will be useful when we select a model.



```{r}

smp_size <- floor(0.80 * nrow(DS_TARGET_FLAG))

## set the seed to make your partition reproductible
set.seed(123)

train_index <- sample(seq_len(nrow(DS_TARGET_FLAG)), size = smp_size)

DS_TARGET_FLAG_TRAIN<- DS_TARGET_FLAG[train_index, ]
DS_TARGET_FLAG_VALID <- DS_TARGET_FLAG[-train_index, ]
```


###3.3.2 Model 1 and enahncement of Model 1 with step function (Model 2)


In this model, we will be using all the given variables in train data set.  We will create model using logit function. We will then step thru the model to remove unnecessary variables and generate the refined model. We will highlight the summary of the refined model. 



```{r model1, echo = FALSE, warning=FALSE, message=FALSE}

TF_Model1 <- glm(TARGET_FLAG ~ ., data = na.omit(DS_TARGET_FLAG_TRAIN), family = "binomial")
TF_Model1_ref<- TF_Model1
#TF_Model1_ref<- step(TF_Model1, direction="backward")
summary(TF_Model1_ref)

```

##### Interpretation for the  TF_Model1 and TF_Model1_ref  

$newline$

TF_Model1: 

From model 1 summary we can find following important points-

(i) Variable URBANICITY_Rural has most significant association with lowest p value. negative value of log odd function indicates that chances of accidents ae higher in Urbancity areas compare to rural area. 

(ii) For MSTATUS_Yes variable log odd is negative which indicates married people tend to drive slowly and have less number of accidents. 

(iii) Sex variable has no significant association which means driving patters does not depend on men and women.  

(iv) variable REVOKED_Yes has strong association which indicates if person's license has been revoked in last 7 years then chane of end up in accidents are much higher with log odds value of 0.809090. 

(v) If person has a claim in last 5 years then chances of more claims are higher. Variable OLDCLAIM_FLAG_BI indicates that with negative log odds value(1 is here no claim  -0.559409). 

(vi) AIC value of the model is AIC: 6078.7 and number of iteration was 5. 


TF_Model1_ref:  

Model 1 has AIC value 6078.7 and enhaned model TF_Model1_ref has AIC value 6064.9 slightly better compared to model1. We will look into more details on model1_ref below-

(i) Based on the outcome from model_ref, it can be seen that following variables KIDSDRIV,PARENT1_Yes, MSTATUS_Yes,CAR_USE_Commercial, REVOKED_Yes, TIF_sin , CAR_TYPE_FLAG_BIN , EDUCATION_FLAG_BIN, JOB_TYPE_FLAG_BIN, INCOME_FLAG_BIN , HOME_VAL_FLAG_BIN,OLDCLAIM_FLAG_BIN,MVR_PTS_FLAG_BIN,TRAVTIME_FLAG_BIN , URBANICITY_Rural are only statistically significant. Most of the variables are having similar association as above model 1. 

(ii)As for the statistically significant variables, URBANICITY_Rural has the lowest p-value suggesting a strong association of the URBANICITY_Rural to the target variable.Implication is also same negative value indicate lower chances of accidents in rural areas.

(iii) One interesting outcome is when childrens are dirivng your car then more chances of accidents with log odd value of 0.41327 for variable KIDSDRIV. 

  (iv) For variable CAR_TYPE_FLAG_BIN there is high negative corelation is there with log odds value of -0.65867 that means Mnivan and Panel truck has higher chance of getting into an accident. 
  
  (v) Variable EDUCATION_FLAG_BIN has negative log odds value of -0.46755 indicaating that people with higher education above high school has less chance of an accident comared to the other group. 
  
 (iv) No. of iterations are 5 before lowest value of AIC was derived for this model. 






###3.3.3 Model 3


In this model, we will be using original variables; however we use the CART (Classification and Regression Trees) algorithm to train the model. We will then Prune the tree and have a look at the summary of this pruned model.


```{r, echo = FALSE, warning=FALSE, message=FALSE}

 # grow tree 
TF_Model2 <- rpart(TARGET_FLAG~., data=DS_TARGET_FLAG_TRAIN, method = "class")
plotcp(TF_Model2)

# plot tree 
# plot(model2, uniform=TRUE, main="Classification Tree for TARGET_FLAG")
# text(model2, use.n=TRUE, all=TRUE, cex=.8)

# create attractive postscript plot of tree 
#post(fit, file = "c:/tree.ps", title = "Classification Tree for Kyphosis")

TF_Model2_ref <- prune(TF_Model2, cp = TF_Model2$cptable[which.min(TF_Model2$cptable[,"xerror"]),"CP"])


printcp(TF_Model2_ref) # display the results 

# plot the pruned tree 
par(mfrow=c(1,1))
plot(TF_Model2_ref, uniform=TRUE, main="Pruned Classification Tree for TARGET_FLAG")
text(TF_Model2_ref, use.n=TRUE, all=TRUE, cex=.8)
#plotcp(TF_Model2_ref)

#post(pfit, file = "c:/ptree.ps",    title = "Pruned Classification Tree for Kyphosis") 
```

#####  Interpretation for Model 3

$newline$

Following analysis can be drawn from this model:
(i)The following variables have been used for classification - OLD_CLAIM, JOB_TYPE_FLAG_BIN, URBANICITY_Rural, KIDSDRIV, MVR_PTS, REVOKED_Yes.


(ii)lowest Cp value and Xerror occured on split 7. 


(iii) OLDCLAIM_FLAG_BIN is the first variable used to split the classification based on its value 0 and 1. When there is claim (1 in abobe variable) branch is further split to other branches by variable JOB_TYPE_FLAG_BIN (based on value 0 and 1). Based on value of JOB_TYPE_FLAG_BIN (0 and 1) there is two different routes in calssification. one Split (774/323) is based on variable KIDSDRIV and the other one (738/675) is based on URBANICITY_Rural variable. Using the above variable total 7 splits have been performed for classification. 



##3.4 Model Evaluation Using VALID Data


Lets go ahead and apply the above models to the VALID dataset that we had held out. Below is the table of predictions for each of the models:




###3.4.1 Evaluation of Model 1



```{r ,echo = FALSE, warning=FALSE, message=FALSE}


#Following function Eval() will be used to calculate various metrics related to the model like Accuracy, Sensitivity, #Precision , Specificity, and F1 score

Eval<-function(x){
    TP<-x$Freq[x$metrics=="TRUE_1"]
    FP<-x$Freq[x$metrics=="FALSE_1"]
    TN<-x$Freq[x$metrics=="FALSE_0"]
    FN<-x$Freq[x$metrics=="TRUE_0"]
    Accuracy <-(TP+TN)/(TP+TN+FP+FN)
    Error_Rate<-(FP+FN)/(TP+TN+FP+FN)
    Precision<-TP/(TP+FP)
    sensitivity<-TP/(TP+FN)
    specificity<-TN/(TN+FP)
    F1_Score=2*Precision*sensitivity/(sensitivity+specificity)
    eval_result<-data.frame(Accuracy=c(0),Error_Rate=c(0),Precision=c(0),sensitivity=c(0),specificity=c(0),F1_Score=c(0))
    
    eval_result[1,1]<-Accuracy
    eval_result[1,2]<-Error_Rate
    eval_result[1,3]<- Precision
    eval_result[1,4]<-sensitivity
    eval_result[1,5]<-specificity
    eval_result[1,6]<-F1_Score
    eval_result
}

model_comparison<-data.frame(Accuracy=c(0),Error_Rate=c(0),Precision=c(0),sensitivity=c(0),specificity=c(0),F1_Score=c(0), AUC=c(0))

#confusion matrix

DS_TARGET_FLAG_VALID$M1_TARGET_FLAG <- predict(TF_Model1, newdata=DS_TARGET_FLAG_VALID, type="response")
df_pre_train1<-as.data.frame(table(DS_TARGET_FLAG_VALID$M1_TARGET_FLAG>0.5,DS_TARGET_FLAG_VALID$TARGET_FLAG))
df_pre_train1$metrics <- paste(df_pre_train1$Var1,df_pre_train1$Var2, sep = '_') 
model_comparison[1,]<-Eval(df_pre_train1)
model_comparison[1,c("AUC")]<-c(auc(DS_TARGET_FLAG_VALID$TARGET_FLAG, DS_TARGET_FLAG_VALID$M1_TARGET_FLAG))
kable(model_comparison[1,],row.names = TRUE, caption = " Model 1 evaluation KPIs")

```


Model 1 has good accuracy value close to 78.3%. sensitivity value is lower than the specificity value.



###3.4.2 Evaluation of Model 2



```{r ,echo = FALSE, warning=FALSE, message=FALSE}

DS_TARGET_FLAG_VALID$M2_TARGET_FLAG <- predict(TF_Model1_ref,newdata=DS_TARGET_FLAG_VALID)
df_pre_train1<-as.data.frame(table(DS_TARGET_FLAG_VALID$M2_TARGET_FLAG>0.5,DS_TARGET_FLAG_VALID$TARGET_FLAG))
df_pre_train1$metrics <- paste(df_pre_train1$Var1,df_pre_train1$Var2, sep = '_') 
model_comparison[2,]<-Eval(df_pre_train1)
model_comparison[2,c("AUC")]<-c(auc(DS_TARGET_FLAG_VALID$TARGET_FLAG, DS_TARGET_FLAG_VALID$M2_TARGET_FLAG))
kable(model_comparison[2,],row.names = TRUE, caption = " Model 2 evaluation KPIs")

```


Model 2 has good accuracy value close to 77.3% and very close to model1. sensitivity value is lower than the specificity value.


###3.4.3 Evaluation of Model 3



```{r ,echo = FALSE, warning=FALSE, message=FALSE}

#ds <- select(DS_TARGET_FLAG_VALID, -AGE, -AGE_IMPUTE, -BLUEBOOK, -CAR_AGE_IMPUTE, -CAR_AGE_MISS, -CLM_FREQ, -HOME_VAL, -HOME_VAL_IMPUTE, -HOME_VAL_MISS, -INCOME, -INCOME_IMPUTE, -INCOME_MISS, -MVR_PTS, -OLDCLAIM, -TIF, -TRAVTIME, -YOJ, -YOJ_IMPUTE,-YOJ_MISS)

DS_TARGET_FLAG_VALID$M2_TARGET_FLAG <- predict(TF_Model2_ref,newdata=DS_TARGET_FLAG_VALID)[,2]
df_pre_train1<-as.data.frame(table(DS_TARGET_FLAG_VALID$M2_TARGET_FLAG>0.5,DS_TARGET_FLAG_VALID$TARGET_FLAG))
df_pre_train1$metrics <- paste(df_pre_train1$Var1,df_pre_train1$Var2, sep = '_') 
model_comparison[3,]<-Eval(df_pre_train1)
model_comparison[3,c("AUC")]<-c(auc(DS_TARGET_FLAG_VALID$TARGET_FLAG, DS_TARGET_FLAG_VALID$M2_TARGET_FLAG))
kable(model_comparison,row.names = TRUE, caption = " Model 3 evaluation KPIs")
```

This model has accuracy value of 75.4%. AUC for this model is 67.4 % and less compared to the other two models. 

##3.5 Final Logistic Model Selection Summary



Following is the comparison of various metrics for above 3 models

```{r ,echo = FALSE, warning=FALSE, message=FALSE}

model_comparison$Model_No<-c(1:3)
kable(model_comparison[,c("Model_No","Accuracy","Error_Rate","AUC","Precision","sensitivity","specificity","F1_Score")],caption = "Model Performance Metrics Comparison")

```

From the comparison table, we see that Model 1 is quite superior from the accuracy and AUC perspective.

The AUC provides the best score on probability of correctly identifying the patterns at various cut off values. The Accuracy, on the other hand, is calculated as specific cut off value.  For this assignment we will go with cut off value of 0.5 and choose the Model 1 based on Accuracy value for further prediction on evaluation data set.


###3.5.1 Detailed Inference for Final Model

The following analysis will be carried out on the final model: 

 (i) Relevant variables in the model
(ii) Estimate confidence interval for coefficient
(iii) odds ratios and 95% CI
(iv) AUC curve
 (v) Distribution of prediction 
 
 
 
###3.5.2 Most important variables in the model



```{r best model analysis 1,echo=FALSE}

summary(TF_Model1)

```

Following are the most relevant variables for the model: CAR_USE_Commercial, REVOKED_Yes, URBANICITY_Rural, CAR_TYPE_FLAG_BIN,TRAVTIME, OLDCLAIM_FLAG_BIN, MVR_PTS, TIF_SIN, KIDSDRIV, PARENT1_Yes, EDUCATION_FLAG_BIN, MSTATUS_Yes, BLUEBOOK, OLDCLAIM, JOB_TYPE_FLAG_BIN, HOME_VAL, INCOME_FLAG_BIN. 


we can write the equation of the Model 1 as:

$log(y) = -0.4015 + 0.3431 * KIDSDRIV -0.000001027 * HOME_VAL + 0.01557 * TRAVTIME - 0.00001858 * BLUEBOOK - 0.05103 * TIF - 0.00001873 * OLDCLAIM + 0.1043 * MVR_PTS + 0.7632 * CAR_USE_Commercial - 0.4066 * MSTATUS_Yes + 0.5246 * PARENT1_Yes + 1.025 * REVOKED_Yes - 2.221 * URBANICITY_Rural - 0.5662 * CAR_TYPE_FLAG_BIN - 0.3996 * EDUCATION_FLAG_BIN + 0.3584 * JOB_TYPE_FLAG_BIN + 0.311 * INCOME_FLAG_BIN - 0.627 * OLDCLAIM_FLAG_BIN$



###3.5.3 Analysis of odds ratios of variables 95% CI



```{r best model analysis 2,echo=FALSE}

exp(cbind(OR = coef(TF_Model1), confint.default(TF_Model1)))

```

The following points can be made for the important variables in the model: 

In keeping all other variables same, the odds of an accident increases as follow: 1.8449962 for per unit change in CAR_USE_Commercial, 2.2458626 per unit change in REVOKED_Yes, 0.1104633 for per unit change in URBANICITY_Rural, etc. Any value which is less than 1, it means that there is less chance of an event with the per unit increase of the variable. 


###3.5.4 ROC curve for the selected model



```{r,echo=FALSE}
#AUC

myRoc <- roc(DS_TARGET_FLAG_VALID$TARGET_FLAG~DS_TARGET_FLAG_VALID$M1_TARGET_FLAG, DS_TARGET_FLAG_VALID)
plot(myRoc, main="ROC Curve for Classification data") 

# pred <- prediction(TF_Model1_ref, DS_TARGET_FLAG_VALID$TARGET_FLAG)
# perf <- performance(pred, measure = "tpr", x.measure = "fpr")
# 
# auc <- performance(pred, measure = "auc")
# auc <- auc@y.values[[1]]
# 
# roc.data <- data.frame(fpr=unlist(perf@x.values),
#                        tpr=unlist(perf@y.values),
#                        model="GLM")
# ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
#     geom_ribbon(alpha=0.2) +
#     geom_line(aes(y=tpr)) +
#     ggtitle(paste0("ROC Curve w/ AUC=", auc))

```


###3.5.5 Distribution of the Predictions



```{r distribution of prediction, echo=FALSE}
plot_pred_type_distribution <- function(df, threshold) {
  v <- rep(NA, nrow(df))
  v <- ifelse(DS_TARGET_FLAG_VALID$M1_TARGET_FLAG >= threshold & DS_TARGET_FLAG_VALID$TARGET_FLAG == 1, "TP", v)
  v <- ifelse(DS_TARGET_FLAG_VALID$M1_TARGET_FLAG >= threshold & DS_TARGET_FLAG_VALID$TARGET_FLAG == 0, "FP", v)
  v <- ifelse(DS_TARGET_FLAG_VALID$M1_TARGET_FLAG < threshold & DS_TARGET_FLAG_VALID$TARGET_FLAG == 1, "FN", v)
  v <- ifelse(DS_TARGET_FLAG_VALID$M1_TARGET_FLAG < threshold & DS_TARGET_FLAG_VALID$TARGET_FLAG == 0, "TN", v)
  
  DS_TARGET_FLAG_VALID$pred_type <- v
  
  ggplot(data=DS_TARGET_FLAG_VALID, aes(x=TARGET_FLAG, y=M1_TARGET_FLAG)) + 
    geom_violin(fill=rgb(1,1,1,alpha=0.6), color=NA) + 
    geom_jitter(aes(color=pred_type), alpha=0.6) +
    geom_hline(yintercept=threshold, color="red", alpha=0.6) +
    scale_color_discrete(name = "type") +
    labs(title=sprintf("Threshold at %.2f", threshold))
}

DS_TARGET_FLAG_VALID$M1_TARGET_FLAG <- predict(TF_Model1_ref, newdata=DS_TARGET_FLAG_VALID, type="response")

plot_pred_type_distribution (DS_TARGET_FLAG_VALID,0.5)
```

Considering the target has value 1 (accident occurs) and 0 when no accident, then the above plot illustrates the tradeoff of choosing a reasonable threshold. In other words, if the threshold is increased, the number of false positive (FP) results is lowered; while the number of false negative (FN) results increases.
 


\newpage




#4 Linear Regression for TARGET_AMT



In this section we will use Linear regression to model the TARGET_AMT. We will first start with the Data Exploration. We will be using only those records where the TARGET_FLAG is 1. This indicates that the vehicle crashed. In such a scenario, we will be modeling the cost of repair using Linear Regression. First, lets create the required data set for the "Crashed" data from the existing "clean" full data and look at the structure of the resulting dataset. We will remove from the new "crashed" dataset all those variables that were created specifically for predicting TARGET_FLAG. We will be creating these variables separately for predicting TARGET_AMT.





```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_crash <- insure_without_dummy[insure_without_dummy$TARGET_FLAG==1,]

dummy_vars<-as.data.frame(sapply(dummy(insure_train_crash), FUN = as.numeric))
dummy_vars <- dummy_vars-1

insure_train_crash <- cbind(insure_train_crash, dummy_vars)



#write.csv(insure_train_crash, file = "D:/CUNY/Courses/Business Analytics and Data Mining/Assignments/data621-ctg5/HW4/final.csv")


#insure_train_crash <- select(insure_train_crash, -CAR_AGE_FLAG_BIN, -CAR_TYPE_FLAG_BIN, -CLM_FREQ_FLAG_BIN, -EDUCATION_FLAG_BIN, -HOME_VAL_FLAG_BIN, -INCOME_FLAG_BIN, -JOB_TYPE_FLAG_BIN, -MVR_PTS_FLAG_BIN, -OLDCLAIM_FLAG_BIN, -TARGET_FLAG, -TIF_sin, -YOJ_FLAG_BIN)

#str(insure_train_crash)
```


We notice that the dependent variable here is TARGET_AMT. Apart from the dependent variables, we have 49 independent or predictor variables.

Also, since we created this dataset from the "Clean" full dataset, we already have taken care of the missing values.


However, we may need to look into the outliers and correlations again since we have a new target variable to correlate against.




##4.1 Data Summary and Correlation Analysis


###4.1.1 Data Summary




In this section, we will create summary data to better understand the relationship each of the variables have with our dependent variables using correlation, central tendency, and dispersion as shown below:  



```{r, echo = FALSE, warning=FALSE, message=FALSE, results='hide'}

ds_stats <- psych::describe(insure_train_crash, skew = TRUE, na.rm = TRUE)
#ds_stats
kable(ds_stats[1:7], caption= "Data Summary")
kable(ds_stats[8:13], caption= "Data Summary (Cont)")

fun1 <- function(a, y) cor(y, a , use = 'na.or.complete')
x<-select(insure_train_crash, -EDUCATION, -JOB, -CAR_TYPE)
Correlation_TARGET_AMT <- sapply(x, FUN = fun1, y=insure_train_crash$TARGET_AMT) 
```




###4.1.2 Correlations


Now we will produce the correlation table between the independent variables and the dependent variable - TARGET_AMT  


```{r, echo = FALSE, warning=FALSE, message=FALSE}
Correlation_TARGET_AMT <- sort(Correlation_TARGET_AMT, decreasing = TRUE)
kable(data.frame(Correlation_TARGET_AMT), caption = "Correlation between TARGET_AMT and predictor variables")

```



The above table suggests that none of the variables seem to have a very strong correlation with TARGET_AMT. 


However, BLUEBOOK, CAR_TYPE_Panel.Truck, SEX_M, CAR_TYPE_Van, CAR_USE_Commercial, INCOME, INCOME_IMPUTE, JOB_Professional, JOB_Unknown, MVR_PTS, YOJ, YOJ_IMPUTE, HOME_VAL_IMPUTE, EDUCATION_PhD, HOME_VAL, AGE, AGE_IMPUTE, RED_CAR_yes, PARENT1_Yes, YOJ_MISS, HOME_VAL_MISS, JOB_Blue.Collar, EDUCATION_Masters, EDUCATION_Bachelors, JOB_Lawyer, TRAVTIME, CLM_FREQ, HOMEKIDS have a positive correlation. 


Similarly, KIDSDRIV, TRAVTIME_FLAG_BIN, INCOME_MISS, URBANICITY_Rural, OLDCLAIM, CAR_TYPE_Minivan, TIF, CAR_AGE_MISS, JOB_Doctor, CAR_AGE, CAR_AGE_IMPUTE, JOB_Clerical, CAR_TYPE_Sports.Car, CAR_TYPE_Pickup, JOB_Manager, JOB_Home.Maker, JOB_Student, MSTATUS_Yes, EDUCATION_High.School, REVOKED_Yes, CAR_TYPE_SUV
 have a negative correlation. 



Lets now see how values in some of the variable affects the correlation:



CAR_TYPE - If you drive Vans or Panel Trucks your cost of repair seems to increase as against Minivan, Pickup, Sports.Car, SUV. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation.


EDUCATION - If you have only a high school education then your cost of repair is less compared to a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation.


JOB - If you are a Lawyer, Professional, in a Blue Collar job or the job is unknown, you spend more on repairs as compared to a Doctor, Manager, Home Maker, Student, or Clerical job.  Again binning this variable will strengthen the correlation.




###4.1.3 Binning of Variables


Lets have a look at the following numeric variables that have 0 as one of their values: INCOME, YOJ, HOME_VAL, OLDCLAIM, CLM_FREQ, MVR_PTS, CAR_AGE, AGE, BLUEBOOK, TIF, TRAVTIME. The goal here is to see if we can bin these variables into zero and non-zero bin values and check the correlations. While doing that we will also see how the variables are distributed vis-a-vis TARGET_AMT.




```{r, echo = FALSE, warning=FALSE, message=FALSE}

check_bins <- function(var, thresholds) {
    col_x <- which(colnames(insure_train_crash)==var)
    old_x <- select(insure_train_crash, col_x)
    cor_old <- cor(old_x, insure_train_crash$TARGET_AMT,use = 'na.or.complete')
    ds <- data.frame("Item" = "Original", "Correlation"= round(cor_old, 5))

    old_tresh <- 0
    for(i in 1:length(thresholds)) {
        New_x <- ifelse((select(insure_train_crash, col_x) >= old_tresh & select(insure_train_crash, col_x)<=thresholds[i]),0,1)
        
        cor_new <- cor(New_x, insure_train_crash$TARGET_AMT,use = 'na.or.complete')
        
        ds_1 <- data.frame("Item" = as.character(thresholds[i]), "Correlation"= round(cor_new, 5))
        
        ds <- rbind(ds, ds_1)
        old_tresh <- thresholds[i]
    }
    
    return (ds)
}

par(mfrow=c(2,2))
plot(insure_train_crash$INCOME, insure_train_crash$TARGET_AMT, xlab = "INCOME", ylab = "TARGET_AMT")
#check_bins("INCOME", c(0, 50000, 125000, 200000))

plot(insure_train_crash$YOJ, insure_train_crash$TARGET_AMT, xlab = "YOJ", ylab = "TARGET_AMT")
#check_bins("YOJ", c(0:19))

plot(insure_train_crash$HOME_VAL, insure_train_crash$TARGET_AMT, ylab = "TARGET_AMT", xlab = "HOME_VAL")
#check_bins("HOME_VAL", c(seq(0, 600000, 10000)))

plot(insure_train_crash$OLDCLAIM, insure_train_crash$TARGET_AMT, ylab = "TARGET_AMT", xlab = "OLDCLAIM")
#hist(insure_train_crash$OLDCLAIM, breaks=50)
#check_bins("OLDCLAIM", c(seq(0, 50000, 1000)))

#show_hist("CLM_FREQ")
plot(insure_train_crash$CLM_FREQ, insure_train_crash$TARGET_AMT, ylab = "TARGET_AMT", xlab = "CLM_FREQ")
#check_bins("CLM_FREQ", c(0, 1, 2, 3, 4))

#table(insure_train_full$MVR_PTS)
plot(insure_train_crash$MVR_PTS, insure_train_crash$TARGET_AMT, ylab = "TARGET_AMT", xlab = "MVR_PTS")
#check_bins("MVR_PTS", c(0:12))

#table(insure_train_full$CAR_AGE)
#show_hist("CAR_AGE")
plot(insure_train_crash$CAR_AGE, insure_train_crash$TARGET_AMT, ylab = "TARGET_AMT", xlab = "CAR_AGE")
#check_bins("CAR_AGE", c(1:27))

#table(insure_train_full$AGE)
plot(insure_train_crash$AGE, insure_train_crash$TARGET_AMT, ylab = "TARGET_AMT", xlab = "AGE")
#show_hist("AGE")
#check_bins("AGE", c(16:80))

#table(insure_train_full$BLUEBOOK)
plot(insure_train_crash$BLUEBOOK, insure_train_crash$TARGET_AMT, ylab = "TARGET_AMT", xlab = "BLUEBOOK")
#show_hist("BLUEBOOK")
#check_bins("BLUEBOOK", c(5000, 10000, 20000, 30000, 45000, 57500, 58000))

# table(insure_train_full$TIF)
# show_hist("TIF")
plot(insure_train_crash$TIF, insure_train_crash$TARGET_AMT, ylab = "TARGET_AMT", xlab = "TIF")
# check_bins("TIF", c(1, 4, 6, 10, 24))

#table(insure_train_full$TRAVTIME)
plot(insure_train_crash$TRAVTIME, insure_train_crash$TARGET_AMT, ylab = "TARGET_AMT", xlab = "TRAVTIME")
#show_hist("TRAVTIME")
#check_bins("TRAVTIME", c(21, 59, 120))

```




From the outputs above, we can come to the following conclusions:




- INCOME - From the plot we can see that there is a marked difference in the chart at around 125000. We will use this value to bin this variable. 

- YOJ - We can see that from 7 - 17 years, there is a visible change in the TARGET_AMT. We will use this  bound to create the binned variable.
 
- HOME_VAL - We see from the plot 3 distinct segments - Between 0-10000, 60000-400000 and the rest. We will use these values to create 2 bins.

- OLDCLAIM- We can visualize 3 clusters in the data - 0-2000, 2000-10000, > 10000,  We will use these values to create 2 bins.

- CLM_FREQ - Values less than 4 seem to have a positive correlation. We will use this value for binning. 

- MVR_PTS - We can see from the plot that after 2, the TARGET_AMT starts decreasing. We will use this value for binning. 

- CAR_AGE - There are quite a few records with a 1 year car age. We will use this bound to generate a binned variable as well as retain the original varible as is. 

- AGE - There is no specific pattern that emerges in AGE. We will retain the variable as is.

- BLUEBOOK - There is no specific pattern that emerges. We will retain the variable as is.

- TIF - Looking at the plot we can conclude that this is not a good variable for binning. We will retain this variable as is.

- TRAVTIME - from the plot, we can see that there is a clear pattern around the value - 20. We will go ahead and create a binned variable for this.



We will carry out the above transformations in the Data Preparation phase.




###4.1.4 Outliers identification 



In this sub-section, we will look at the boxplots and determine the outliers in variables and decide on whether to act on the outliers.

We will do the outliers only on the numeric variables: AGE, BLUEBOOK and TIF. The other variables will be binned and would not beed outlier handling.


Below are the plots:


```{r, echo = FALSE, warning=FALSE, message=FALSE}

# AGE, BLUEBOOK, CAR_AGE, CLM_FREQ, HOME_VAL, HOMEKIDS, INCOME, KIDSDRIV, MVR_PTS, OLDCLAIM, TIF, TRAVTIME, YOJ

mdata<- select(insure_train_crash, AGE, BLUEBOOK, TIF)
mdata2 <- melt(mdata)
# Output the boxplot
p <- ggplot(data = mdata2, aes(x=variable, y=value)) + 
  geom_boxplot() + ggtitle("Outliers Identification")
p + facet_wrap( ~ variable, scales="free", ncol=5)

```

From the "Outliers identification" plot above, we see that we have few outliers that we need to treat. 

We see thatall the 3 variables need to be treated when we do the data preparation for modeling the TARGET_AMT. 






##4.2 Data Preparation 



Now that we have completed the data exploration / analysis, we will be transforming the data for use in analysis and modeling. 


We will be following the below steps as guidelines: 
- Outliers treatment 
- Adding New Variables 




###4.2.1 Outliers treatment


In this sub-section, we will check different transformations for each of the variables - AGE, BLUEBOOK, TIF - and create the appropriate outlier-handled / transformed variables.  


```{r, echo = FALSE, warning=FALSE, message=FALSE}
show_charts <- function(x, varlab, ...) {
    xlabel <- varlab
    xlab_log <- paste0(xlabel, '_log')
    xlab_sqrt <- paste0(xlabel, '_sqrt')
    xlab_sin <- paste0(xlabel, '_sin')
    xlab_inv <- paste0(xlabel, '_inv')
    
    mdata <- cbind(x, log(x), sqrt(x), sin(x), 1/x)
    colnames(mdata) <- c(xlabel, xlab_log, xlab_sqrt, xlab_sin, xlab_inv)
    mdata2 <- melt(mdata)
    mdata2 <- mdata2[, c(2:3)]
    names(mdata2) <- c("variable", "value")
    
    # Output the boxplot
    p <- ggplot(data = mdata2, aes(x=variable, y=value)) + geom_boxplot() + ggtitle("Outliers Treatment")
    p + facet_wrap( ~ variable, scales="free", ncol=5)
}

```





**Transformations for AGE**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
#KIDSDRIV, AGE, CAR_AGE, MVR_PTS, TIF, TRAVTIME and YOJ
show_charts(insure_train_crash$AGE, 'AGE')
insure_train_crash$AGE_sin <- sin(insure_train_crash$AGE)
```


From the above charts we can see that a sin transformation works well for AGE. We will create this variable.




**Transformations for TIF**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
show_charts(insure_train_crash$TIF, 'TIF')
insure_train_crash$TIF_sin <- sin(insure_train_crash$TIF)
```



From the above charts we can see that a log, sqrt, sin or an inverse transformation works well for TIF. However, a sin transformation seems to be more appropriate as it is well centered. Hence, We will create these variables.



**Transformations for BLUEBOOK**


```{r, echo = FALSE, warning=FALSE, message=FALSE}
show_charts(insure_train_crash$BLUEBOOK, 'BLUEBOOK')
insure_train_crash$BLUEBOOK_sin <- sin(insure_train_crash$BLUEBOOK)
```



From the above charts we can see that a sin transformation works well for BLUEBOOK. We will create these variables.




###4.2.2 Adding New Variables



In this section, we generate some additional variables that we feel will help the correlations. The following were some of the observations we made during the data exploration phase for TARGET_AMT.


The following were some of the observations we made during the data exploration phase for TARGET_AMT




CAR_TYPE - If you drive Vans or Panel Trucks your cost of repair seems to increase as against Minivan, Pickup, Sports.Car, SUV. Since the distiction is clear, we believe that binning this variable accordingly will help strengthen the correlation. Accordingly, we will bin these variables as below:

CAR_TYPE_AMT_BIN : 

- 1 : if CAR_TYPE is Vans or Panel Trucks 
- 0 : if CAR_TYPE is Pickups, Sports, SUVs or Minivans



```{r, echo = FALSE, warning=FALSE, message=FALSE}
insure_train_crash$CAR_TYPE_AMT_BIN <- ifelse(insure_train_crash$CAR_TYPE_Van | insure_train_crash$CAR_TYPE_Panel.Truck, 1, 0)

```

EDUCATION - If you have only a high school education then your cost of repair is less compared to a Bachelors, Masters or a Phd. Again binning this variable will strengthen the correlation. Accordingly, we will bin these variables as below:

EDUCATION_AMT_BIN : 

- 1 : if EDUCATION is High School 
- 0 : if EDUCATION is Bachelors, Masters or Phd




```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_crash$EDUCATION_AMT_BIN <- ifelse(insure_train_crash$EDUCATION_High.School, 1, 0)

```

JOB - If you are a Lawyer, Professional, in a Blue Collar job or the job is unknown, you spend more on repairs as compared to a Doctor, Manager, Home Maker, Student, or Clerical job.  Again binning this variable will strengthen the correlation. Accordingly, we will bin these variables as below:

JOB_TYPE_AMT_BIN : 

- 1 : if JOB_TYPE is Lawyer, Professional, Unknown or in a Blue Collar 
- 0 : if JOB_TYPE is Doctor, Manager, Home Maker, Student, or Clerical



```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_crash$JOB_TYPE_AMT_BIN <- ifelse(insure_train_crash$JOB_Lawyer |  insure_train_crash$JOB_Professional | insure_train_crash$JOB_Blue.Collar | insure_train_crash$JOB_Unknown, 1, 0)

```


INCOME - From the plot we can see that there is a marked difference in the chart at around 125000. We will use this value to bin this variable. 



INCOME_AMT_BIN : 

- 1 : if INCOME <= 125000 
- 0 : if INCOME > 125000


```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_crash$INCOME_AMT_BIN <- ifelse(insure_train_crash$INCOME <=125000, 1, 0)

```



- YOJ - We can see that from 7 - 17 years, there is a visible change in the TARGET_AMT. We will use this  bound to create the binned variable.


YOJ_AMT_BIN : 

- 1 : if YOJ >=7 and YOJ<= 17 
- 0 : ELSE 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_crash$YOJ_AMT_BIN <- ifelse((insure_train_crash$YOJ>=7 & insure_train_crash$YOJ<=17), 1, 0)

```




- HOME_VAL - We see from the plot 3 distinct segments - Between 0-10000, 60000-400000 and the rest. We will use these values to create 2 bins.


HOME_VAL_AMT_0_10K_BIN : 

- 1 : if HOME_VAL >=0 and HOME_VAL<= 10000 
- 0 : ELSE 0


HOME_VAL_AMT_60K_400K_BIN : 

- 1 : if HOME_VAL >=60000 and HOME_VAL<= 400000 
- 0 : ELSE 0




```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_crash$HOME_VAL_AMT_0_10K_BIN <- ifelse((insure_train_crash$HOME_VAL>=0 & insure_train_crash$HOME_VAL<=10000), 1, 0)

insure_train_crash$HOME_VAL_AMT_60K_400K_BIN <- ifelse((insure_train_crash$HOME_VAL>=60000 & insure_train_crash$HOME_VAL<=400000), 1, 0)

```



OLDCLAIM- We can visualize 3 clusters in the data - 0-2000, 2000-10000, > 10000,  We will use these values to create 2 bins.


OLDCLAIM_AMT_0_2K_BIN : 

- 1 : if OLDCLAIM >=0 and OLDCLAIM<= 2000 
- 0 : ELSE 0


OLDCLAIM_AMT_2K_10K_BIN : 

- 1 : if OLDCLAIM >=2000 and OLDCLAIM<= 10000 
- 0 : ELSE 0




```{r, echo = FALSE, warning=FALSE, message=FALSE}

insure_train_crash$OLDCLAIM_AMT_0_2K_BIN <- ifelse((insure_train_crash$OLDCLAIM>=0 & insure_train_crash$OLDCLAIM<=2000), 1, 0)

insure_train_crash$OLDCLAIM_AMT_2K_10K_BIN <- ifelse((insure_train_crash$OLDCLAIM>=2001 & insure_train_crash$OLDCLAIM<=10000), 1, 0)

```




- CLM_FREQ - Values less than 4 seem to have a positive correlation. We will use this value for binning. 


CLM_FREQ_AMT_BIN : 

- 1 : if CLM_FREQ < 4 
- 0 : if CLM_FREQ >= 4


```{r, echo = FALSE, warning=FALSE, message=FALSE}
insure_train_crash$CLM_FREQ_AMT_BIN <- ifelse(insure_train_crash$CLM_FREQ <4, 1, 0)
```



- MVR_PTS - We can see from the plot that after 2, the TARGET_AMT starts decreasing. We will use this value for binning. 


MVR_PTS_AMT_BIN : 

- 1 : if MVR_PTS <=2 
- 0 : if MVR_PTS > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}
insure_train_crash$MVR_PTS_AMT_BIN <- ifelse(insure_train_crash$MVR_PTS <=2, 1, 0)
```



- CAR_AGE - There are quite a few records with a 1 year car age. We will use this bound to generate a binned variable as well as retain the original varible as is. 


CAR_AGE_AMT_BIN : 

- 1 : if CAR_AGE <= 1 
- 0 : if CAR_AGE > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}
insure_train_crash$CAR_AGE_AMT_BIN <- ifelse(insure_train_crash$CAR_AGE <=1, 1, 0)
```



- TRAVTIME - from the plot, we can see that there is a clear pattern around the value - 20. We will go ahead and create a binned variable for this.


TRAVTIME_AMT_BIN : 

- 1 : if TRAVTIME <= 20 
- 0 : if TRAVTIME > 0


```{r, echo = FALSE, warning=FALSE, message=FALSE}
insure_train_crash$TRAVTIME_AMT_BIN <- ifelse(insure_train_crash$TRAVTIME <=20, 1, 0)


insure_train_crash$KIDSDRIV_AMT_BIN_0  <- ifelse(insure_train_crash$KIDSDRIV <=0, 1, 0)
insure_train_crash$KIDSDRIV_AMT_BIN_1  <- ifelse(insure_train_crash$KIDSDRIV <=1, 1, 0)

insure_train_crash$HOMEKIDS_AMT_BIN_0  <- ifelse(insure_train_crash$HOMEKIDS <=0, 1, 0)
insure_train_crash$HOMEKIDS_AMT_BIN_3  <- ifelse(insure_train_crash$HOMEKIDS <=3, 1, 0)

insure_train_crash$YOJ_AMT_BIN_0_AND_9To14  <- ifelse((insure_train_crash$YOJ ==0 | (insure_train_crash$YOJ>=9 & insure_train_crash$YOJ>=14)), 1, 0)

insure_train_crash$INCOME_AMT_BIN_MISS_0  <- ifelse((is.na(insure_train_crash$INCOME) |  insure_train_crash$INCOME<=0), 1, 0)

insure_train_crash$HOME_VAL_AMT_BIN_MISS_0  <- ifelse((is.na(insure_train_crash$HOME_VAL) |  insure_train_crash$HOME_VAL<=0), 1, 0)

insure_train_crash$EDUCATION_AMT_BIN_HS  <- ifelse(insure_train_crash$EDUCATION_High.School==1, 1, 0)
insure_train_crash$EDUCATION_AMT_BIN_HS_B  <- ifelse((insure_train_crash$EDUCATION_Bachelors |  insure_train_crash$EDUCATION_High.School), 1, 0)

insure_train_crash$JOB_AMT_BIN_CPSB  <- ifelse((insure_train_crash$JOB_Clerical |  insure_train_crash$JOB_Blue.Collar | insure_train_crash$JOB_Professional |  insure_train_crash$JOB_Student), 1, 0)

insure_train_crash$TIF_AMT_BIN_6  <- ifelse(insure_train_crash$TIF <=6, 1, 0)

insure_train_crash$CAR_TYPE_AMT_BIN_V_PT_MV  <- ifelse((insure_train_crash$CAR_TYPE_Van |  insure_train_crash$CAR_TYPE_Panel.Truck | insure_train_crash$CAR_TYPE_Minivan), 1, 0)

insure_train_crash$OLDCLAIM_AMT_BIN_MISS_0  <- ifelse((is.na(insure_train_crash$OLDCLAIM) |  insure_train_crash$OLDCLAIM<=0), 1, 0)

insure_train_crash$CLM_FREQ_AMT_BIN_0  <- ifelse(insure_train_crash$CLM_FREQ<=0, 1, 0)
insure_train_crash$CLM_FREQ_AMT_BIN_3  <- ifelse(insure_train_crash$CLM_FREQ<=3, 1, 0)

insure_train_crash$MVR_PTS_AMT_BIN_0  <- ifelse(insure_train_crash$MVR_PTS<=0, 1, 0)
insure_train_crash$MVR_PTS_AMT_BIN_5  <- ifelse(insure_train_crash$MVR_PTS<=5, 1, 0)

#write.csv(ds, file = "D:/CUNY/Courses/Business Analytics and Data Mining/Assignments/data621-ctg5/HW4/ds.csv")

insure_train_crash <- select(insure_train_crash, -TARGET_FLAG, -INDEX)

DS_TARGET_AMT <- insure_train_crash
    
#DS_TARGET_AMT <- select(insure_train_crash, -AGE, -BLUEBOOK, -CAR_AGE, -CAR_TYPE_Minivan, -CAR_TYPE_Panel.Truck, -CAR_TYPE_Pickup, -CAR_TYPE_Sports.Car, -CAR_TYPE_SUV, -CAR_TYPE_Van, -CLM_FREQ, -EDUCATION_Bachelors, -EDUCATION_High.School, -EDUCATION_Masters, -EDUCATION_PhD, -HOME_VAL, -INCOME, -JOB_Blue.Collar, -JOB_Clerical, -JOB_Doctor, -JOB_Home.Maker, -JOB_Lawyer, -JOB_Manager, -JOB_Professional, -JOB_Student, -JOB_Unknown, -MVR_PTS, -OLDCLAIM, -TIF, -TRAVTIME, -YOJ)

DS_TARGET_AMT <- select(insure_train_crash, -EDUCATION, -JOB, -CAR_TYPE, -EDUCATION_AMT_BIN_HS_B, -EDUCATION_AMT_BIN_HS, -EDUCATION_AMT_BIN, -JOB_AMT_BIN_CPSB, -HOME_VAL_AMT_BIN_MISS_0, -CLM_FREQ_AMT_BIN_3, -CLM_FREQ_AMT_BIN_0, -EDUCATION_PhD, -JOB_Unknown, -CAR_TYPE_Van, -CAR_TYPE_AMT_BIN, -JOB_TYPE_AMT_BIN, -CAR_TYPE_AMT_BIN_V_PT_MV)

```




##4.3 Build Models


Now that we have the dataset in a shape that can be modeled, we will go ahead and train the model for TARGET_AMT. We will train 2 models and select the best among these 2 models. The following will be the model specifications:



- Model1 (All Variables in Linear Dataset) - This will use the standard lm for building the model. We will use all available variables.  



- Model2 -  (A few selected variables) - This will use the standard lm for building the model. However, we will use only a few selected variables that seemed to have a good correlation with TARGET_AMT.   




###4.3.1 Model 1



In this model, we will be using the standard lm modeling technique. We will use the entire set of variables from the Linear dataset.  

```{r, echo = FALSE, warning=FALSE, message=FALSE}

#origvars <- c(3, 9, 14, 44, 35, 36, 37, 38, 39, 40, 15, 12, 22, 23, 24, 25, 7, 43, 4, 6, 42, 26, 27, 28, 29, 30, 31, 32, 33, 34, 2, 16, 13, 11, 17, 18, 19, 20, 1, 10, 8, 21, 5, 41)

#DS_TARGET_AMT <- select(DS_TARGET_AMT, )
# TA_Model1 <- lm(TARGET_AMT~.-EDUCATION-JOB-CAR_TYPE-EDUCATION_AMT_BIN_HS_B-EDUCATION_AMT_BIN_HS-EDUCATION_AMT_BIN-JOB_AMT_BIN_CPSB-HOME_VAL_AMT_BIN_MISS_0-CLM_FREQ_AMT_BIN_3-CLM_FREQ_AMT_BIN_0-EDUCATION_PhD-JOB_Unknown-CAR_TYPE_Van-CAR_TYPE_AMT_BIN-JOB_TYPE_AMT_BIN-CAR_TYPE_AMT_BIN_V_PT_MV,  data=na.omit(DS_TARGET_AMT))

TA_Model1 <- lm(TARGET_AMT~., data=na.omit(DS_TARGET_AMT))
summary(TA_Model1)

# TA_Model1_ref<-step(TA_Model1,direction="backward",test="F")
# summary(TA_Model1_ref)

```



**Interpretation of the Model**

Based on the Model output, below are the characteristics of the refined model :

- The Residual standard error is 7577
- Multiple R-squared: 0.05812
- Adjusted R-squared: 0.02428
- F-statistic: 1.717 on 59 and 1642 DF
- p-value: < 0.0006848




###4.3.2 Model 2


In this model, we will be using the standard lm modeling technique. We will use only those variables that seemed to have a good correlation with TARGET_AMT.  

```{r, echo = FALSE, warning=FALSE, message=FALSE}

DS_SELECTED_VARS <- select(DS_TARGET_AMT, TARGET_AMT, AGE, BLUEBOOK, CAR_AGE, CAR_AGE_AMT_BIN, CAR_USE_Commercial, CLM_FREQ, CLM_FREQ_AMT_BIN, HOME_VAL, HOME_VAL_AMT_0_10K_BIN, HOME_VAL_AMT_60K_400K_BIN, HOMEKIDS, HOMEKIDS_AMT_BIN_0, HOMEKIDS_AMT_BIN_3, INCOME, INCOME_AMT_BIN, KIDSDRIV, KIDSDRIV_AMT_BIN_0, KIDSDRIV_AMT_BIN_1, MSTATUS_Yes, MVR_PTS, MVR_PTS_AMT_BIN, MVR_PTS_AMT_BIN_0, MVR_PTS_AMT_BIN_5, OLDCLAIM, OLDCLAIM_AMT_0_2K_BIN, OLDCLAIM_AMT_2K_10K_BIN, PARENT1_Yes, RED_CAR_yes, REVOKED_Yes, SEX_M, TIF, TIF_AMT_BIN_6, TRAVTIME, TRAVTIME_AMT_BIN, URBANICITY_Rural, YOJ, YOJ_AMT_BIN, YOJ_AMT_BIN_0_AND_9To14, EDUCATION_Masters, EDUCATION_High.School, JOB_Clerical, JOB_Doctor, JOB_Home.Maker, JOB_Lawyer, JOB_Professional, JOB_Manager, JOB_Student, CAR_TYPE_Panel.Truck, CAR_TYPE_Pickup, CAR_TYPE_Sports.Car, CAR_TYPE_SUV)

TA_Model2<- lm(TARGET_AMT~., data=DS_SELECTED_VARS)
summary(TA_Model2)

# #DS_TARGET_AMT_TRAIN_ORIG <- select(DS_TARGET_AMT_TRAIN, origvars)
# TA_Model2 <- lm(TARGET_AMT~., data=na.omit(DS_TARGET_AMT))
# summary(TA_Model2)

# TA_Model2_ref<-step(TA_Model2,direction="backward",test="F")
# summary(TA_Model2_ref)


# TA_model2 <- lm(TARGET_AMT~.-EDUCATION_PhD-JOB_Unknown-CAR_TYPE_Van-CAR_TYPE_AMT_BIN-EDUCATION_AMT_BIN-JOB_TYPE_AMT_BIN-TRAVTIME_AMT_BIN-HOME_VAL_AMT_BIN_MISS_0-EDUCATION_AMT_BIN_HS-EDUCATION_AMT_BIN_HS_B-JOB_AMT_BIN_CPSB-CLM_FREQ_AMT_BIN_0-CLM_FREQ_AMT_BIN_3-CAR_TYPE_AMT_BIN_V_PT_MV, data=DS_TARGET_AMT)
# 
# 
# TA_model2 <- lm(TARGET_AMT~BLUEBOOK+CAR_AGE+REVOKED_Yes+SEX_M+EDUCATION_Bachelors+EDUCATION_High.School+HOME_VAL_AMT_60K_400K_BIN+CAR_AGE_AMT_BIN+MVR_PTS+MSTATUS_Yes, data=DS_TARGET_AMT)  

```




**Interpretation of the Model**

Based on the backward stepwise selection, below are the characteristics of the refined model :

- The Residual standard error is 7577
- Multiple R-squared: 0.05333
- Adjusted R-squared: 0.02407
- F-statistic: 1.823 on 51 and 1650 DF
- p-value: < 0.0004045




##4.4 Final Linear Model Selection Summary



Based on the Models above, it is clear that Model 1 performs slightly better than Model 2. We will now use this model to validate further. We will create plots to validate the assumption of Linear Regression:



###Normality check of residual values:\


```{r, echo = FALSE, warning=FALSE, message=FALSE}

#  Analysis of plot on residuals to verify normal distribution of residuals

library(MASS)
sresid <- studres(TA_Model1) 
hist(sresid, freq=FALSE, 
     main="Distribution of Residuals")
xfit<-seq(min(sresid),max(sresid),length=40) 
yfit<-dnorm(xfit) 
lines(xfit, yfit)
```

Based on the normality plot it appears that residual distribution is nearly normal. This indicates the mean of the difference between our predictions. 


```{r, echo = FALSE, warning=FALSE, message=FALSE}

step4.res <- resid(TA_Model1)
score<-predict(TA_Model1,type="response")

plot(score, step4.res,  ylab="Residuals", xlab="Predicted Cost of Repair", main="Residual vs Predicted values") 
abline(0, 0) 
```

Distribution of residual values are random around base line and do not show any pattern around base line.

###Evaluate homoscedasticity:

```{r, echo = FALSE, warning=FALSE, message=FALSE,eval=FALSE}

library(car)

ncvTest(TA_Model1)
# plot studentized residuals vs. fitted values 
spreadLevelPlot(TA_Model1)



```

The test confirms the non-constant error variance test. It also has a p-value higher than a significance level of 0.05. 

###Analysis of collinearity: 
```{r, echo = FALSE, warning=FALSE, message=FALSE}

library(faraway)

# Evaluate Collinearity of the variables in model "step4" vif(step4) # variance inflation factors 
#kable(sqrt(vif(TA_Model1)), caption = 'Analysis of collinearity')

sqrt(vif(TA_Model1))

```

Variables have been tested with variance inflation factors (VIF). If any variable has value which is greater than 3 then the highest value variable been removed from model and model performance has been evaluated. Following are the out comes from this assessment steps-

```{r, echo = FALSE, warning=FALSE, message=FALSE}
TA_Model1 <- lm(TARGET_AMT~.-KIDSDRIV, data=na.omit(DS_TARGET_AMT))
#summary(TA_Model1)
```

Pass 1- Based on that variance inflation factors (VIF) following variable "KIDSDRIV" has highest value > 3 and is removed from model, and model is evaluated without that variable. Adjusted R^2 value has changed to 0.02487 due to removal of this variable. Hence this variable is not adding lot of value to the model and can be removed.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
TA_Model1 <- lm(TARGET_AMT~.-KIDSDRIV, data=na.omit(DS_TARGET_AMT))
summary(TA_Model1)
```

Pass 2- Based on that variance inflation factors (VIF) following variable "HOME_VAL_AMT_0_10K_BIN" has highest value < 3 and is removed from model, and model is evaluated without that variable. Adjusted R^2 values changed to 0.02461. Hence this variable is not adding lot of value to the model and can be removed. We stopped at this point as further removal of variables led to a rapid detoriation of Adjusted R^2.  

```{r, echo = FALSE, warning=FALSE, message=FALSE}

# Adjusted R2 = 0.02461
TA_Model1 <- lm(TARGET_AMT~.-KIDSDRIV-HOME_VAL_AMT_0_10K_BIN, data=na.omit(DS_TARGET_AMT))
summary(TA_Model1)

# # Adjusted R2 = 0.01871
# TA_Model1 <- lm(TARGET_AMT~.-KIDSDRIV-HOME_VAL_AMT_0_10K_BIN-OLDCLAIM_AMT_2K_10K_BIN-EDUCATION_High.School-MVR_PTS-INCOME_AMT_BIN_MISS_0, data=na.omit(DS_TARGET_AMT))
# summary(TA_Model1)


```


Final model was derived after number of iterations of variable eliminations were carried out. VIF values in the final model among variables < 3. In this scenario a model with slightly less performance was selected to avoid collinearity effect among variables and reduced complexity. 
\





#5 Prediction Using Evaluation Data



Now that we have selected the final models for both the TARGET_FLAG and the TARGET_AMT, we will go ahead and use these models to predict the results for the evaluation dataset. After transforming the data to meet the needs of the trained models, we will apply the models in 2 steps. 

 
Step 1 - Here we use the transformed evaluation dataset to predict for the TARGET_FLAG using the requisite predictors.

 
Step 2 - Once we have the prediction for the TARGET_FLAG, we will filter this data for only those rows that were predicted for a CRASH. We then use this smaller dataset to precict for the TARGET_AMT.


##5.1 Tranformation of Evaluation Data 

First we need to transform the evaluation dataset to account for all the predictors that were used in both the models.



```{r, echo = FALSE, warning=FALSE, message=FALSE, results='asis'}

eval_ds <- read.csv("https://raw.githubusercontent.com/kishkp/data621-ctg5/master/HW4/insurance-evaluation-data.csv")

eval_ds$INCOME <- as.numeric(str_replace_all(eval_ds$INCOME, pattern =  "[$*,]", replacement = ""))
eval_ds$HOME_VAL <- as.numeric(str_replace_all(eval_ds$HOME_VAL, pattern =  "[$*,]", replacement = ""))
eval_ds$BLUEBOOK <- as.numeric(str_replace_all(eval_ds$BLUEBOOK, pattern =  "[$*,]", replacement = ""))
eval_ds$OLDCLAIM <- as.numeric(str_replace_all(eval_ds$OLDCLAIM, pattern =  "[$*,]", replacement = ""))

eval_ds$MSTATUS <- as.factor(str_replace_all(eval_ds$MSTATUS, "z_", ""))
eval_ds$SEX <- as.factor(str_replace_all(eval_ds$SEX, "z_", ""))
eval_ds$EDUCATION <- as.factor(str_replace_all(eval_ds$EDUCATION, "z_", ""))
eval_ds$EDUCATION <- as.factor(str_replace_all(eval_ds$EDUCATION, "<", ""))
eval_ds$CAR_TYPE <- as.factor(str_replace_all(eval_ds$CAR_TYPE, "z_", ""))
eval_ds$URBANICITY <- as.factor(str_replace_all(eval_ds$URBANICITY, "z_", ""))

eval_ds$JOB <- as.character(eval_ds$JOB)
eval_ds$JOB[eval_ds$JOB==""] <- "Unknown"
eval_ds$JOB <- as.factor(str_replace_all(eval_ds$JOB, "z_", ""))

eval_ds <- eval_ds[ -which( eval_ds$CAR_AGE == -3 | eval_ds$CAR_AGE == 0 ) , ]

# Create Dummy Variable for 2 factor variables 
eval_ds$CAR_USE_Commercial <- ifelse(eval_ds$CAR_USE=="Commercial", 1, 0)
eval_ds$MSTATUS_Yes <- ifelse(eval_ds$MSTATUS=="Yes", 1, 0)
eval_ds$PARENT1_Yes <- ifelse(eval_ds$PARENT1=="Yes", 1, 0)
eval_ds$RED_CAR_yes <- ifelse(eval_ds$RED_CAR=="yes", 1, 0)
eval_ds$REVOKED_Yes <- ifelse(eval_ds$REVOKED=="Yes", 1, 0)
eval_ds$SEX_M <- ifelse(eval_ds$SEX=="M", 1, 0)
eval_ds$URBANICITY_Rural <- ifelse(eval_ds$URBANICITY=="Highly Rural/ Rural", 1, 0)

# remove original variables
eval_ds <- select(eval_ds, -CAR_USE, -MSTATUS, -PARENT1, -RED_CAR, -REVOKED, -SEX, -URBANICITY)

#- We will also create dummy variables for all the factors and drop the original variables. 
dummy_vars<-as.data.frame(sapply(dummy(eval_ds), FUN = as.numeric))
dummy_vars <- dummy_vars-1

# remove original variables
#eval_ds <- select(eval_ds, -EDUCATION, -JOB, -CAR_TYPE)

eval_ds <- cbind(eval_ds, dummy_vars)

eval_ds$YOJ_MISS  <- ifelse(is.na(eval_ds$YOJ), 1, 0)
eval_ds$INCOME_MISS  <- ifelse(is.na(eval_ds$INCOME), 1, 0)
eval_ds$HOME_VAL_MISS  <- ifelse(is.na(eval_ds$HOME_VAL), 1, 0)
eval_ds$CAR_AGE_MISS  <- ifelse(is.na(eval_ds$CAR_AGE), 1, 0)

# Direct Impute

insure_train_full$AGE[is.na(insure_train_full$AGE)] <- mean(insure_train_full$AGE, na.rm = T) 
insure_train_full$YOJ[is.na(insure_train_full$YOJ)] <- mean(insure_train_full$YOJ, na.rm = T) 
insure_train_full$INCOME[is.na(insure_train_full$INCOME)] <- median(insure_train_full$INCOME, na.rm = T) 
insure_train_full$HOME_VAL[is.na(insure_train_full$HOME_VAL)] <- median(insure_train_full$HOME_VAL, na.rm = T) 
insure_train_full$CAR_AGE[is.na(insure_train_full$CAR_AGE)] <- median(insure_train_full$CAR_AGE, na.rm = T) 

eval_ds$TIF_sin <- sin(eval_ds$TIF)
eval_ds$BLUEBOOK_sin <- sin(eval_ds$BLUEBOOK)
eval_ds$AGE_sin <- sin(eval_ds$AGE)

eval_ds$CAR_TYPE_FLAG_BIN <- ifelse(eval_ds$CAR_TYPE_Minivan | eval_ds$CAR_TYPE_Panel.Truck, 1, 0)

eval_ds$EDUCATION_FLAG_BIN <- ifelse(eval_ds$EDUCATION_High.School, 0, 1)

eval_ds$JOB_TYPE_FLAG_BIN <- ifelse(eval_ds$JOB_Student |  eval_ds$JOB_Home.Maker | eval_ds$JOB_Clerical | eval_ds$JOB_Blue.Collar, 1, 0)


eval_ds$INCOME_FLAG_BIN <- ifelse(eval_ds$INCOME <=0, 1, 0)

eval_ds$YOJ_FLAG_BIN <- ifelse(eval_ds$YOJ <=0, 1, 0)

eval_ds$HOME_VAL_FLAG_BIN <- ifelse(eval_ds$HOME_VAL <=0, 1, 0)

eval_ds$OLDCLAIM_FLAG_BIN <- ifelse(eval_ds$OLDCLAIM <=0, 1, 0)

eval_ds$CLM_FREQ_FLAG_BIN <- ifelse(eval_ds$CLM_FREQ <=0, 1, 0)


eval_ds$MVR_PTS_FLAG_BIN <- ifelse(eval_ds$MVR_PTS <=0, 1, 0)

eval_ds$CAR_AGE_FLAG_BIN <- ifelse(eval_ds$CAR_AGE <=1, 1, 0)

eval_ds$TRAVTIME_FLAG_BIN <- ifelse(eval_ds$TRAVTIME <=20, 1, 0)

new_ds_full <- eval_ds

#eval_ds <- select(eval_ds, -JOB_Blue.Collar, -JOB_Clerical, -JOB_Doctor, -JOB_Home.Maker, -JOB_Lawyer, -JOB_Manager, -JOB_Professional, -JOB_Student, -JOB_Unknown, -CAR_TYPE_Minivan, -CAR_TYPE_Panel.Truck, -CAR_TYPE_Pickup, -CAR_TYPE_Sports.Car, -CAR_TYPE_SUV, -CAR_TYPE_Van, -EDUCATION_Bachelors, -EDUCATION_High.School, -EDUCATION_Masters, -EDUCATION_PhD) 

## Create Variables for Linear Regression

eval_ds$AGE_sin <- sin(eval_ds$AGE)

eval_ds$TIF_sin <- sin(eval_ds$TIF)


eval_ds$BLUEBOOK_sin <- sin(eval_ds$BLUEBOOK)


eval_ds$CAR_TYPE_AMT_BIN <- ifelse(eval_ds$CAR_TYPE_Van | eval_ds$CAR_TYPE_Panel.Truck, 1, 0)

eval_ds$EDUCATION_AMT_BIN <- ifelse(eval_ds$EDUCATION_High.School, 1, 0)

eval_ds$JOB_TYPE_AMT_BIN <- ifelse(eval_ds$JOB_Lawyer |  eval_ds$JOB_Professional | eval_ds$JOB_Blue.Collar | eval_ds$JOB_Unknown, 1, 0)

eval_ds$INCOME_AMT_BIN <- ifelse(eval_ds$INCOME <=125000, 1, 0)

eval_ds$YOJ_AMT_BIN <- ifelse((eval_ds$YOJ>=7 & eval_ds$YOJ<=17), 1, 0)

eval_ds$HOME_VAL_AMT_0_10K_BIN <- ifelse((eval_ds$HOME_VAL>=0 & eval_ds$HOME_VAL<=10000), 1, 0)

eval_ds$HOME_VAL_AMT_60K_400K_BIN <- ifelse((eval_ds$HOME_VAL>=60000 & eval_ds$HOME_VAL<=400000), 1, 0)


eval_ds$OLDCLAIM_AMT_0_2K_BIN <- ifelse((eval_ds$OLDCLAIM>=0 & eval_ds$OLDCLAIM<=2000), 1, 0)

eval_ds$OLDCLAIM_AMT_2K_10K_BIN <- ifelse((eval_ds$OLDCLAIM>=2001 & eval_ds$OLDCLAIM<=10000), 1, 0)


eval_ds$CLM_FREQ_AMT_BIN <- ifelse(eval_ds$CLM_FREQ <4, 1, 0)


eval_ds$MVR_PTS_AMT_BIN <- ifelse(eval_ds$MVR_PTS <=2, 1, 0)


eval_ds$CAR_AGE_AMT_BIN <- ifelse(eval_ds$CAR_AGE <=1, 1, 0)

eval_ds$TRAVTIME_AMT_BIN <- ifelse(eval_ds$TRAVTIME <=20, 1, 0)

eval_ds$KIDSDRIV_AMT_BIN_0  <- ifelse(eval_ds$KIDSDRIV <=0, 1, 0)
eval_ds$KIDSDRIV_AMT_BIN_1  <- ifelse(eval_ds$KIDSDRIV <=1, 1, 0)

eval_ds$HOMEKIDS_AMT_BIN_0  <- ifelse(eval_ds$HOMEKIDS <=0, 1, 0)
eval_ds$HOMEKIDS_AMT_BIN_3  <- ifelse(eval_ds$HOMEKIDS <=3, 1, 0)

eval_ds$YOJ_AMT_BIN_0_AND_9To14  <- ifelse((eval_ds$YOJ ==0 | (eval_ds$YOJ>=9 & eval_ds$YOJ>=14)), 1, 0)

eval_ds$INCOME_AMT_BIN_MISS_0  <- ifelse((is.na(eval_ds$INCOME) |  eval_ds$INCOME<=0), 1, 0)

eval_ds$HOME_VAL_AMT_BIN_MISS_0  <- ifelse((is.na(eval_ds$HOME_VAL) |  eval_ds$HOME_VAL<=0), 1, 0)

eval_ds$EDUCATION_AMT_BIN_HS  <- ifelse(eval_ds$EDUCATION_High.School==1, 1, 0)
eval_ds$EDUCATION_AMT_BIN_HS_B  <- ifelse((eval_ds$EDUCATION_Bachelors |  eval_ds$EDUCATION_High.School), 1, 0)

eval_ds$JOB_AMT_BIN_CPSB  <- ifelse((eval_ds$JOB_Clerical |  eval_ds$JOB_Blue.Collar | eval_ds$JOB_Professional |  eval_ds$JOB_Student), 1, 0)

eval_ds$TIF_AMT_BIN_6  <- ifelse(eval_ds$TIF <=6, 1, 0)

eval_ds$CAR_TYPE_AMT_BIN_V_PT_MV  <- ifelse((eval_ds$CAR_TYPE_Van |  eval_ds$CAR_TYPE_Panel.Truck | eval_ds$CAR_TYPE_Minivan), 1, 0)

eval_ds$OLDCLAIM_AMT_BIN_MISS_0  <- ifelse((is.na(eval_ds$OLDCLAIM) |  eval_ds$OLDCLAIM<=0), 1, 0)

eval_ds$CLM_FREQ_AMT_BIN_0  <- ifelse(eval_ds$CLM_FREQ<=0, 1, 0)
eval_ds$CLM_FREQ_AMT_BIN_3  <- ifelse(eval_ds$CLM_FREQ<=3, 1, 0)

eval_ds$MVR_PTS_AMT_BIN_0  <- ifelse(eval_ds$MVR_PTS<=0, 1, 0)
eval_ds$MVR_PTS_AMT_BIN_5  <- ifelse(eval_ds$MVR_PTS<=5, 1, 0)







```

##5.2 Model Output for Logistic Regression


We now apply the final Logistic regression model that was trained for predicting the TARGET_FLAG. Below is a table of predictions.


###Count for Crash / No Crash  


```{r, echo = FALSE, warning=FALSE, message=FALSE, results='asis'}

eval_ds$TARGET_FLAG_prob <- unlist(data.frame(predict(TF_Model1, type ="response", newdata=eval_ds)))
eval_ds$TARGET_FLAG<-ifelse(eval_ds$TARGET_FLAG_prob>0.5,1,0)

# eval_ds$TARGET_FLAG <- ifelse(eval_ds$TARGET_FLAG_prob>0.5, 1, 0)
x <- as.data.frame(table(eval_ds$TARGET_FLAG))

names(x) <- c("Crash Predicted?", "Counts")
# x[1,1] <- FALSE
# x[2,1] <- TRUE

kable(x, caption="Predicted Crash Counts")

```





```{r, echo = FALSE, warning=FALSE, message=FALSE, results='asis'}

###Top 10 Records by Index  
#kable(head(eval_ds)[,c(1,2,94)], caption="Logistic Regression Results")


# eval_ds$TARGET_FLAG <- ifelse(eval_ds$TARGET_FLAG_prob>0.5, 1, 0)
#table(eval_ds$TARGET_FLAG)
#kable(eval_ds[eval_ds$TARGET_FLAG_prob="NA",c(1,61,62)], caption="Outcome on evaluation data set")

```


##5.3 Model Output for Linear Regression


Next we filter for the "predicted" crashes and we apply the final linear model to this smaller dataset to predict the TARGET_AMT. Below are the results for ONLY the "Crashed" records.


###Top 10 Records by TARGET_AMT for TARGET_FLAG = 1 


```{r, echo = FALSE, warning=FALSE, message=FALSE, results='asis'}

eval_ds$TARGET_AMT <- 0

eval_ds_TA <- filter(eval_ds, TARGET_FLAG == 1)
eval_ds_TA$TARGET_FLAG<-as.numeric(eval_ds_TA$TARGET_FLAG)

eval_ds_TA$TARGET_AMT <- predict(TA_Model1, newdata=eval_ds_TA)

x<-arrange(eval_ds_TA, (TARGET_AMT))
x<-x[-c(1:2),]
kable(x[1:10,c(1,2,94,3)], caption="Linear Regression Results")


```

##5.4 Conclusion

```{r , echo=FALSE,message=FALSE}


Class_Expense<-lm(TARGET_AMT~TARGET_FLAG_prob,data=eval_ds_TA)

plot(eval_ds_TA$TARGET_FLAG_prob,eval_ds_TA$TARGET_AMT)
abline(Class_Expense,col="red")



```

Outcome from regression model and outcome from linear model was plotted in the chart above.It can be seen from the chart above that probability associated with classification and predicted amount from linear model 
does not show any specific patterns. From insurance business standpoint cases where probability of incident  and repair expense amount is high will be the focus area top right side corner of the chart.\


