---
title: "Business Analytics - Final Project"
author:
- Critical Thinking Group 5
- Arindam Barman
- Mohamed Elmoudni
- Shazia Khan
- Kishore Prasad
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

# 1 Abstract:


# 2 Key words:

# 3 Introduction:

The data set contains approximately 41188 obs. of 21 variables. \

This dataset is based on "Bank Marketing" UCI dataset (please check the description at: http://archive.ics.uci.edu/ml/datasets/Bank+Marketing).
   The data is enriched by the addition of five new social and economic features/attributes (national wide indicators from a ~10M population country), published by the Banco de Portugal and publicly available at: https://www.bportugal.pt/estatisticasweb.\
   
The binary classification goal is to predict if the client will subscribe a bank term deposit (variable y).

This dependent variable tells whether the client will subscribe a bank term deposit or not. This is a binary variable and as such we will be using a Logistic Regression Model.

# 4 Literature review:

# 5 Methodology:

## 5.1 Data Exploration 

In section we will explore and gain some insights into the dataset by pursuing the below high level steps and inquiries: \
-Variable identification \
-Understanding predictor variables relationship with response variable
-Missing values and Unique Values \


##5.1.2	Variable identification 

First let's display and examine the data dictionary or the data columns as shown in table 1

```{r, echo = FALSE, warning=FALSE, message=FALSE}
if (!require("ggplot2",character.only = TRUE)) (install.packages("ggplot2",dep=TRUE))
if (!require("MASS",character.only = TRUE)) (install.packages("MASS",dep=TRUE))
if (!require("knitr",character.only = TRUE)) (install.packages("knitr",dep=TRUE))
if (!require("xtable",character.only = TRUE)) (install.packages("xtable",dep=TRUE))
if (!require("dplyr",character.only = TRUE)) (install.packages("dplyr",dep=TRUE))
if (!require("psych",character.only = TRUE)) (install.packages("psych",dep=TRUE))
if (!require("stringr",character.only = TRUE)) (install.packages("stringr",dep=TRUE))
#if (!require("car",character.only = TRUE)) (install.packages("car",dep=TRUE))
if (!require("faraway",character.only = TRUE)) (install.packages("faraway",dep=TRUE))
if (!require("popbio",character.only = TRUE)) (install.packages("popbio",dep=TRUE))
if (!require("gdata",character.only = TRUE)) (install.packages("gdata",dep=TRUE))
if (!require("reshape",character.only = TRUE)) (install.packages("reshape",dep=TRUE))


#install.packages("fancyvrb")

library(ggplot2)
library(MASS)
library(knitr)
library(xtable)
library(dplyr)
library(psych)
library(stringr)
#library(car)
library(faraway)
library(aod)
library(Rcpp)
library(leaps)
library(ISLR)
library(AUC)
library(ROCR)
library(Amelia)
library(popbio)
library(gdata)
library(reshape)
library(gridExtra)
library(rpart)
library(pROC)
library(pscl)
library(randomForest)

bank_train <- read.table(
  "https://raw.githubusercontent.com/kishkp/data621-ctg5/master/Final%20Project/bank-additional-full.csv",
           sep = ";",
           header = TRUE)

bank_test <-read.table(
  "https://raw.githubusercontent.com/kishkp/data621-ctg5/master/Final%20Project/bank-additional.csv",
           sep = ";",
           header = TRUE)

```

We notice that the variables are numerical, categorical and binary. The responce variable y is binary.

Based on the original dataset, our predictor input has 21 variables. And our response variable is 1 variable called y. 

Binomial Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more metric (interval or ratio scale) independent variables.


##5.1.3 Preliminary Data Analysis

##5.1.4 Analysis of Predictor variable 

```{r, echo = FALSE, warning=FALSE, message=FALSE}

variable_analysis<- read.csv(
  "https://raw.githubusercontent.com/kishkp/data621-ctg5/master/Final%20Project/Variable%20Analysis.csv")
kable(variable_analysis, caption = "Variable Analysis") 

```


##5.1.4 Missing values 

We see that there are no missing values in our dataset as shown in table 2 and graph format.
The unique values are given in the table 

```{r, echo = FALSE, warning=FALSE, message=FALSE}



```

##5.1.5 Proportion of Response Variables 

```{r, echo = FALSE, warning=FALSE, message=FALSE}



```



#5.2	Data Preparation

-Convert Binary to 0 and 1\
-Create dummy variables\
-Data Summary Analysis \
-Correlation of Variables with y\


##5.2.1 Convert Binary yes and no to 0 and 1

Now in order to prepare the data for modeling, we need to update Yes = 1 and No = 0. \

```{r, echo = FALSE, warning=FALSE, message=FALSE}

bank_train2<-bank_train

#update response variable to binary values of 0 and 1
#levels(bank_train2$y)
levels(bank_train2$y) <- c(0, 1)
bank_train2$y <- as.numeric(levels(bank_train2$y))[bank_train2$y]
#str(bank_train2)

```

##5.2.2 Create dummy variables

Now we need to create dummy variables to find out the relationship between y variables and dependent variables, for all categorical variables.\
```{r, echo = FALSE, warning=FALSE, message=FALSE}

# age is numeric 

#create dummy variables for job values
for(level in unique(bank_train2$job)){
  bank_train2[paste("job", level, sep = "_")] <- ifelse(bank_train2$job == level, 1, 0)
}
#Delete original catagorical variable
bank_train2$job <- NULL
#head(bank_train2)

#create dummy variables for marital values
#levels(bank_train2$marital)
for(level in unique(bank_train2$marital)){
  bank_train2[paste("marital", level, sep = "_")] <- ifelse(bank_train2$marital == level, 1, 0)
}
#Delete original catagorical variable
bank_train2$marital <- NULL
#head(bank_train2)

#--------------------------------------------------------------------------
#education dummy variables primary, secondary tertiary, unknown and illiterate

#education_None
bank_train2$education_illiterate <- as.numeric(ifelse(bank_train2$education == "illiterate", 1, 0))
#table(bank_train2$education_Illiterate)

#education_Unknown
bank_train2$education_unknown <-as.numeric(ifelse(bank_train2$education == "unknown", 1, 0))
#table(bank_train2$education_Unknown)

#education_Primary
bank_train2$education_primary <- as.numeric(ifelse(bank_train2$education == "basic.4y" 
                                        | bank_train2$education == "basic.6y", 1, 0))
#table(bank_train2$education_Primary)

#education_Secondary
bank_train2$education_secondary <- as.numeric(ifelse(bank_train2$education == "basic.9y" 
                                        | bank_train2$education == "high.school", 1, 0))
#table(bank_train2$education_Secondary)

#education_Tertiary
bank_train2$education_tertiary <- as.numeric(ifelse(bank_train2$education == "professional.course" 
                                        | bank_train2$education == "university.degree", 1, 0))
#table(bank_train2$education_Tertiary)

#Delete original catagorical variable
bank_train2$education <- NULL


# contact has 2 levels - 1 variable is required
#levels(bank_train2$default)
for(level in unique(bank_train2$default)){
  bank_train2[paste("default", level, sep = "_")] <- ifelse(bank_train2$default == level, 1, 0)
}
#Delete original catagorical variable
bank_train2$default <- NULL


#levels(bank_train2$housing)
for(level in unique(bank_train2$housing)){
  bank_train2[paste("housing", level, sep = "_")] <- ifelse(bank_train2$housing == level, 1, 0)
}
#Delete original catagorical variable
bank_train2$housing <- NULL

#levels(bank_train2$loan)
for(level in unique(bank_train2$loan)){
  bank_train2[paste("loan", level, sep = "_")] <- ifelse(bank_train2$loan == level, 1, 0)
}
#Delete original catagorical variable
bank_train2$loan <- NULL

# contact has 2 levels - 1 variable is required
#levels(bank_train2$contact)
for(level in unique(bank_train2$contact)){
  bank_train2[paste("contact", level, sep = "_")] <- ifelse(bank_train2$contact == level, 1, 0)
}
#Delete original catagorical variable
bank_train2$contact <- NULL

#levels(bank_train2$month)
for(level in unique(bank_train2$month)){
  bank_train2[paste("month", level, sep = "_")] <- ifelse(bank_train2$month == level, 1, 0)
}
#Delete original catagorical variable
bank_train2$month <- NULL

#levels(bank_train2$day_of_week)
for(level in unique(bank_train2$day_of_week)){
  bank_train2[paste("day_of_week", level, sep = "_")] <- ifelse(bank_train2$day_of_week == level, 1, 0)
}
#Delete original catagorical variable
bank_train2$day_of_week <- NULL

#duration is numeric
#campaign is numeric

#pdays is numeric
#dummy variable for pdays -previous contact yes or no ; 1 or 0  when 999
bank_train2$previous_contact <- as.numeric(ifelse(bank_train2$pdays == 999, 0, 1))

#previous is numeric

#levels(bank_train2$poutcome)
for(level in unique(bank_train2$poutcome)){
  bank_train2[paste("poutcome", level, sep = "_")] <- ifelse(bank_train2$poutcome == level, 1, 0)
}
#Delete original catagorical variable
bank_train2$poutcome <- NULL

# emp.var.rate is numeric
# cons.price.idx is numeric
# cons.conf.idx  is numeric
# euribor3m is numeric
# nr.employed is numeric


```

##5.2.3 Data Summary with Dummy variables


##5.2.4 Correlation between Response and Predictor of Variables
Now we will produce the correlation table between the independent variables and the dependent variable



##2.5 Outliers Handling

```{r, echo = FALSE, warning=FALSE, message=FALSE}



```

##5.2.6 Analysis the link function for given variables \

In this section, we will investigate how our initial data aligns with a typical logistic model plot.

Recall the Logistic Regression is part of a larger class of algorithms known as Generalized Linear Model (glm). The fundamental equation of generalized linear model is:

$g(E(y)) = a+ Bx_1+B_2x_2+ B_3x_3+...$

where, g() is the link function, E(y) is the expectation of target variable and $B_0 + B_1x_1 + B_2x_2+B_3x_3$ is the linear predictor ( $B_0,B_1,B_2, B_3$ to be predicted). The role of link function is to 'link' the expectation of y to linear predictor.

In logistic regression, we are only concerned about the probability of outcome dependent variable ( success or failure). As described above, g() is the link function. This function is established using two things: Probability of Success (p) and Probability of Failure (1-p). p should meet following criteria: It must always be positive (since p >= 0) It must always be less than equals to 1 (since p <= 1).

Now let's investigate how our initial data model aligns with the above criteria. In other words, we will plot regression model plots for each variable and compare it to a typical logistic model plot:


The main objective in the transformations is to achieve linear relationships with the dependent variable (or, really, with its logit).

```{r, echo = FALSE, warning=FALSE, message=FALSE}


 
```

#3 Experimentation and Results: 

#3.1 Build Models

In this section, we will create 3 models. Aside from using original and transformed data, we will be using different modeling methods and functions such as Classification and Regression tree, Random tree and Generalized logistics model to predict outcome. 

Below is our model definition: 

-Model 1- This model will be created using all the variables in train data set with logit function GLM. 

-Model 2: This model calssification and Regression tree function.  

-Model 3- This model will be created using Random Forests model. 



Taking the treated data and splitting into 80/20 to train model and validate the data.One data set (80%) population will be used for training the model. Rest 20% will used be used for evaluation of the model.

```{r, echo = FALSE, warning=FALSE, message=FALSE}

smp_size <- floor(0.80 * nrow(bank_train2))

## set the seed to make your partition reproductible
set.seed(123)

train_index <- sample(seq_len(nrow(bank_train2)), size = smp_size)

DS_TARGET_FLAG_TRAIN<- bank_train2[train_index, ]
DS_TARGET_FLAG_VALID <- bank_train2[-train_index, ]

# remove"-" from variable to avoid any issues in running the model
colnames(DS_TARGET_FLAG_TRAIN)[15]<-c("job_blue_collar")
colnames(DS_TARGET_FLAG_TRAIN)[20]<-c("job_self_employed")
colnames(DS_TARGET_FLAG_VALID)[15]<-c("job_blue_collar")
colnames(DS_TARGET_FLAG_VALID)[20]<-c("job_self_employed")

```


## 3.1.1 Model 1

\
```{r, echo = FALSE, warning=FALSE, message=FALSE,eval=TRUE,results="hide"}


DS_TARGET_FLAG_TRAIN$y<-as.factor(DS_TARGET_FLAG_TRAIN$y)
model1 <- glm(y ~.,family=binomial,data=DS_TARGET_FLAG_TRAIN)
summary(model1)

#anova(model1, test="Chisq")
model1_update<-glm(y ~.-job_student-marital_unknown-education_tertiary-education_tertiary-default_yes-housing_unknown-loan_yes-loan_unknown-contact_cellular-month_sep-day_of_week_fri-poutcome_success,family=binomial,data=DS_TARGET_FLAG_TRAIN)

exp(coef(model1_update))


```


### Analysis of model 1 & model1_update:\

1.Using GLM function model 1 has been developed by using all the available variables. model1_update is an enhancement of model1 by using only those predictor having association with response variable and not showing up as NA values in model outcome. in model outcome AIC values for both the models are same.\

2. There are total 10 iterations which have been performed in the model before final selection of variables were made.\

3. Table below provide details on significance of the variables and its odd ratio.\


```{r,echo = FALSE, warning=FALSE, message=FALSE}


model1_var <-read.table(
  "https://raw.githubusercontent.com/kishkp/data621-ctg5/master/Final%20Project/Model1_var.csv",
           sep = ",",
           header = TRUE)
kable(model1_var, caption = "Signifcant Variables model 1") 

```


## 3.1.2 Model 2

```{r,echo = FALSE, warning=FALSE, message=FALSE }

model2 <- rpart(y~., data=DS_TARGET_FLAG_TRAIN, method = "class")


model2_update <- prune(model2, cp = model2$cptable[which.min(model2$cptable[,"xerror"]),"CP"])

plotcp(model2_update)

#printcp(model2_update) # display the results 


```


### Analysis of model 2 & model2_update:\

1. model 2 has been built using the CRT (classification and Regression Tree).  Based on available data model 2 has been constructed wwit CRT and then using prune function model2_update has been created by pruning the model 2. \

2. Above chart shows the size of the tree and optimal cp values.

3. There are 6 splits in total from this model.\

4. Three key variables are used in this model2_update are - duration,nr.employed ,previous_contact. \

5. Classification tree is given below in the diagram- \

```{r, echo = FALSE, warning=FALSE, message=FALSE}
              
# plot the pruned tree 
par(mfrow=c(1,1))
plot(model2_update, uniform=TRUE, main="Pruned Classification Tree for TARGET_FLAG")
text(model2_update, use.n=TRUE, all=TRUE, cex=.8)


```

## 3.1.3 Model 3


```{r,echo = FALSE, warning=FALSE, message=FALSE }

# Random Forest prediction of Kyphosis data

model3 <- randomForest(as.factor(y) ~ .,data=DS_TARGET_FLAG_TRAIN,importance=TRUE, ntree=50)
# print(model3) # view results 


#plot model 3

layout(matrix(c(1,2),nrow=1),
width=c(4,1)) 
par(mar=c(5,4,4,0)) #No margin on the right side
plot(model3, log="y")
par(mar=c(5,0,4,2)) #No margin on the left side
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("top", colnames(model3$err.rate),col=1:3,cex=0.8,fill=1:4)

```

### Analysis of model 3 :\

1.In Random Forests many classification trees are formed for classification. Each tree gives a classification, each tree is voted for performance for that classification. The forest chooses the classification having the most votes (over all the trees in the forest). Chart above shows how classification erro rate and  change with the increase in number of trees.\

2. The model 3 created here by using RandomForest having tree size of 50 as the data set having records of only around 8000 records.\

3. No of variables tried at each split is 7 and having error rate of 8.92%.\

4. Below chart provides importance of various variables in the model.\


```{r,echo = FALSE, warning=FALSE, message=FALSE}

# display importance of variables
varImpPlot(model3)

```



#4 Evaluate Models


```{r ,echo = FALSE, warning=FALSE, message=FALSE}

#Following function Eval() will be used to calculate various metrics related to the model like Accuracy, Sensitivity, #Precision , Specificity, and F1 score

Eval<-function(x){
    TP<-x$Freq[x$metrics=="TRUE_1"]
    FP<-x$Freq[x$metrics=="FALSE_1"]
    TN<-x$Freq[x$metrics=="FALSE_0"]
    FN<-x$Freq[x$metrics=="TRUE_0"]
    Accuracy <-(TP+TN)/(TP+TN+FP+FN)
    Error_Rate<-(FP+FN)/(TP+TN+FP+FN)
    Precision<-TP/(TP+FP)
    sensitivity<-TP/(TP+FN)
    specificity<-TN/(TN+FP)
    F1_Score=2*Precision*sensitivity/(sensitivity+specificity)
    eval_result<-data.frame(Accuracy=c(0),Error_Rate=c(0),Precision=c(0),sensitivity=c(0),specificity=c(0),F1_Score=c(0))
    
    eval_result[1,1]<-Accuracy
    eval_result[1,2]<-Error_Rate
    eval_result[1,3]<- Precision
    eval_result[1,4]<-sensitivity
    eval_result[1,5]<-specificity
    eval_result[1,6]<-F1_Score
    eval_result
}

model_comparison<-data.frame(Accuracy=c(0),Error_Rate=c(0),Precision=c(0),sensitivity=c(0),specificity=c(0),F1_Score=c(0), AUC=c(0))

```


## 4.1 Model 1 Evaluation



```{r, echo = FALSE, warning=FALSE, message=FALSE,eval=TRUE}

#the McFadden R^2 index can be used to assess the model fit.
#pR2(model1)

# Predict result from the model 1 with probability

DS_TARGET_FLAG_VALID$TARGET_FLAG1 <- predict(model1_update,newdata=DS_TARGET_FLAG_VALID,type='response')



#confusion matrix model 1


df_pre_train1<-as.data.frame(table(DS_TARGET_FLAG_VALID$TARGET_FLAG1>0.5,DS_TARGET_FLAG_VALID$y))

df_pre_train1$metrics <- paste(df_pre_train1$Var1,df_pre_train1$Var2, sep = '_') 

model_comparison[1,]<-Eval(df_pre_train1)


# model_comparison[1,c("AUC")]<-c(auc(DS_TARGET_FLAG_VALID$y,DS_TARGET_FLAG_VALID$TARGET_FLAG1))

kable(model_comparison[1,],row.names = TRUE, caption = " Model 1 evaluation KPIs")


```

### Model 1 Evaluation:

1. Significance of McFadden R^2??

2. From above table it can be seen model1_update has a very high accuracy rate of 91.07% when model was evaluated using the validation data set.\

```{r,echo = FALSE, warning=FALSE, message=FALSE,eval=TRUE}

# sensivity vs Specificity
results1<-ifelse(DS_TARGET_FLAG_VALID$TARGET_FLAG1>0.5,1,0)
pr <- prediction(results1, DS_TARGET_FLAG_VALID$y)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

```

3. Model has a very high rate of sensitivity till 0.4 and low sensitivity. After that it has very high rate of specificity which suggested that there are some records which are easy to classify but as the number grows then it becomes difficult to classify those records. \



## 4.2 Model 2 Evaluation


```{r,echo = FALSE, warning=FALSE, message=FALSE,eval=TRUE }

DS_TARGET_FLAG_VALID$TARGET_FLAG2<- predict(model2_update,newdata=DS_TARGET_FLAG_VALID)[,2]


#confusion matrix model 2

df_pre_train1<-as.data.frame(table(DS_TARGET_FLAG_VALID$TARGET_FLAG2>0.5,DS_TARGET_FLAG_VALID$y))

df_pre_train1$metrics <- paste(df_pre_train1$Var1,df_pre_train1$Var2, sep = '_') 

model_comparison[2,]<-Eval(df_pre_train1)

model_comparison[2,c("AUC")]<-c(auc(DS_TARGET_FLAG_VALID$y, DS_TARGET_FLAG_VALID$TARGET_FLAG2))

kable(model_comparison[2,],row.names = TRUE, caption = " Model 1 evaluation KPIs")

```

1. It can be seen from the table above, this model 2 has also very high accuracy rate of 91.05% which is very good.\ 


```{r, echo = FALSE, warning=FALSE, message=FALSE,eval=TRUE}
# Specificity vs Sensivity

results2 <- predict(model2_update,newdata=DS_TARGET_FLAG_VALID,type="prob")[,2]

pr <- prediction(results2, DS_TARGET_FLAG_VALID$y)

prf <- performance(pr, measure = "tpr", x.measure = "fpr")

plot(prf)

```

2. From the chart above it can be seen that model has very very high rate true positive values which suggests initial stages of accurate classification and gradually rate of specificty increases takes over the increase of sensitivity. That means to achive higher accuracy model mmisclassification increases.\





## 4.3 Model 3 Evaluation

\
```{r,echo = FALSE, warning=FALSE, message=FALSE,eval=TRUE}

# results from model 3

DS_TARGET_FLAG_VALID$TARGET_FLAG3<-as.numeric(predict(model3,newdata=DS_TARGET_FLAG_VALID,type='response'))

DS_TARGET_FLAG_VALID$TARGET_FLAG3<-ifelse(DS_TARGET_FLAG_VALID$TARGET_FLAG3==1,0,1)


#confusion matrix model 3

df_pre_train1<-as.data.frame(table(DS_TARGET_FLAG_VALID$TARGET_FLAG3==1,DS_TARGET_FLAG_VALID$y))
df_pre_train1$metrics <- paste(df_pre_train1$Var1,df_pre_train1$Var2, sep = '_') 
model_comparison[3,]<-Eval(df_pre_train1)
model_comparison[3,c("AUC")]<-c(auc(DS_TARGET_FLAG_VALID$y, DS_TARGET_FLAG_VALID$TARGET_FLAG2))
kable(model_comparison[3,],row.names = TRUE, caption = " Model 1 evaluation KPIs")

```

1. The model created using Randomforest has accuracy of 90.94% which is good but compare to other two models this value is less than what we have. 


```{r, echo = FALSE, warning=FALSE, message=FALSE,eval=TRUE}

# specificty vs sensitivity

results3 <- ifelse(as.numeric(predict(model3,newdata=DS_TARGET_FLAG_VALID,type='response'))==1,0,1)

pr <- prediction(DS_TARGET_FLAG_VALID$TARGET_FLAG3, DS_TARGET_FLAG_VALID$y)

prf <- performance(pr, measure = "tpr", x.measure = "fpr")

plot(prf)


```

2. As shown in the chart above this model also shows the similar kind of trend in  classification of data in earlier stages with very stiff line till true positive rate of 0.4 annd then sharp increase in false positive rate.


# 5 Discussion and Conclusions:

#5.1 Select Models

```{r,echo = FALSE, warning=FALSE, message=FALSE,eval=TRUE,eval=FALSE}

# Area under curve model 1
myRoc1 <- roc(DS_TARGET_FLAG_VALID$y~DS_TARGET_FLAG_VALID$TARGET_FLAG1) 

# Area under curve model 2
myRoc2 <- roc(DS_TARGET_FLAG_VALID$y~DS_TARGET_FLAG_VALID$results2)


# Area under curve model 3

myRoc3<- roc(DS_TARGET_FLAG_VALID$y~DS_TARGET_FLAG_VALID$TARGET_FLAG3,DS_TARGET_FLAG_VALID) 


par(mfrow=c(1,3))

plot(myRoc1, main="ROC Curve for Classification data") 
plot(myRoc2, main="ROC Curve for Classification data") 
plot(myRoc3, main="ROC Curve for Classification data")
```



#5.2 Conclusion

# References:

# 6 Appendix



#6.1  Data Analysis details

##6.1.1  Variable Description

```{r, echo = FALSE, warning=FALSE, message=FALSE}

variables<- read.csv(
  "https://raw.githubusercontent.com/kishkp/data621-ctg5/master/Final%20Project/Variable%20Description.csv")
kable(variables, caption = "Variable Description") 

```
\

##6.1.2  Predictor and Response variable Association
```{r, echo = FALSE, warning=FALSE, message=FALSE}


#round(prop.table(table(bank_train$y, bank_train$age),2)*100,2)  #student, retired
ggplot(bank_train, aes(age)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$job),2)*100,2)  #student, retired
ggplot(bank_train, aes(job)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$marital),2)*100,2) #unknown
ggplot(bank_train, aes(marital)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$education),2)*100,2) #illeterate
ggplot(bank_train, aes(education)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$default),2)*100,2) #no
ggplot(bank_train, aes(default)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$housing),2)*100,2) #no
ggplot(bank_train, aes(housing)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$loan),2)*100,2) #no
ggplot(bank_train, aes(loan)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$contact),2)*100,2) #cellular
ggplot(bank_train, aes(contact)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$month),2)*100,2) #march, dec, sep, oct
ggplot(bank_train, aes(month)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$day_of_week),2)*100,2) #Thursday
ggplot(bank_train, aes(day_of_week)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$duration),2)*100,2) #increases with every contact upto 5
ggplot(bank_train, aes(duration)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))+ xlim(0, 2200)

#round(prop.table(table(bank_train$y, bank_train$campaign),2)*100,2) #increases with every contact upto 5
ggplot(bank_train, aes(campaign)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$pdays),2)*100,2) #increases with every contact upto 5
ggplot(bank_train, aes(pdays)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#round(prop.table(table(bank_train$y, bank_train$previous),2)*100,2) #increases with every contact upto 5
ggplot(bank_train, aes(previous)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

  #round(prop.table(table(bank_train$y, bank_train$poutcome),2)*100,2) #success
ggplot(bank_train, aes(poutcome)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

 #round(prop.table(table(bank_train$y, bank_train$emp.var.rate),2)*100,2) #success
ggplot(bank_train, aes(emp.var.rate)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

 #round(prop.table(table(bank_train$y, bank_train$cons.price.idx),2)*100,2) #success
ggplot(bank_train, aes(cons.price.idx)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

 #round(prop.table(table(bank_train$y, bank_train$cons.conf.idx),2)*100,2) #success
ggplot(bank_train, aes(cons.conf.idx)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

 #round(prop.table(table(bank_train$y, bank_train$euribor3m),2)*100,2) #success
ggplot(bank_train, aes(euribor3m)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

 #round(prop.table(table(bank_train$y, bank_train$nr.employed),2)*100,2) #success
ggplot(bank_train, aes(nr.employed)) + geom_bar(aes(fill = y), position = "fill")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))



```

##6.1.3  Unique Value & Missing value

We see that there are no missing values in our dataset as shown in table 2 and graph format.
The unique values are given in the table 

```{r, echo = FALSE, warning=FALSE, message=FALSE}

par(mfrow=c(1,1))
#finding missing values
missings<- data.frame(sapply(bank_train,function(x) sum(is.na(x))))
names(missings)[1]<- paste("Missing Values")
kable(missings, caption = "Missing Values")

# missing values graph
#missmap(bank_train, main = "Missing values vs observed")

### finding unique values
uniques<- data.frame(sapply(bank_train, function(x) length(unique(x))))
names(uniques)[1]<- paste("Unique Values")
kable(uniques, caption = "Unique Values")

```


##6.1.4 Data Summary post conversion 

```{r, echo = FALSE, warning=FALSE, message=FALSE}

#str(bank_train2)

ds_stats <- psych::describe(bank_train2, skew = TRUE, na.rm = TRUE)
#ds_stats
kable(ds_stats[1:7], caption= "Data Summary")
kable(ds_stats[8:13], caption= "Data Summary (Cont)")

#head(bank_train2)

fun1 <- function(a, y) cor(y, a)
x<-bank_train2[,]
Correlation <- sapply(x, FUN = fun1, y=bank_train2$y) 


Correlation <- sort(Correlation, decreasing = TRUE)
#head(Correlation)
kable(Correlation, caption = "Variable Correlation")

#str(bank_train2)
#str(bank_train)
#summary(bank_train2)

```

##6.1.5 Outliers Analysis

```{r, echo = FALSE, warning=FALSE, message=FALSE}

mdata<- select(bank_train2,  age, previous, duration, campaign, emp.var.rate, cons.price.idx, euribor3m,nr.employed)
mdata2 <- melt(mdata)

# Output the boxplot
p <- ggplot(data = mdata2, aes(x=variable, y=value)) + 
  geom_boxplot() + ggtitle("Outliers Identification")
p + facet_wrap( ~ variable, scales="free", ncol=4)
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}

bank_train2 <- bank_train2 %>%
  select(-y, everything())

fun1 <- function(a, y) cor(y, a)
x<-bank_train2[,]
Correlation <- sapply(x, FUN = fun1, y=bank_train2$y) 


Correlation <- sort(Correlation, decreasing = TRUE)

vars <- names(Correlation)


par(mfrow=c(2,4))
for (i in 2:ncol(bank_train2)) {
  hist(bank_train2[,vars[i]], main = vars[i], xlab = "")
}

```

##6.1.6 Analysis of link functions for given variables

```{r, echo = FALSE, warning=FALSE, message=FALSE}

#move y to the last column


par(mfrow=c(2,4))
#Show in the order of Correlation
p = list()
#for (i in 2:ncol(bank_train2)) p[[i]] <- qplot(bank_train2[,i], xlab=names(bank_train2)[[i]])
for (i in 2:ncol(bank_train2)) {
  p[[i]] <- logi.hist.plot(bank_train2[,vars[i]],bank_train2$y,logi.mod = 1, type='hist', boxp=FALSE,col='gray', 
                           mainlabel = vars[i])
}
#do.call(grid.arrange, p)
#plot(p)
#plot(p[[1]], p[[2]])
#plot (p$your.x.coordinate, p$your.y.coordinate)
#head(bank_train2)

 
```
